[{"content":"<h2 id=\"-pattern-time-series\">Pattern: Time series</h2><p>The <code>APPEND</code> command can be used to create a very compact representation of a\nlist of fixed-size samples, usually referred as <em>time series</em>.\nEvery time a new sample arrives we can store it using the command</p><p>Accessing individual elements in the time series is not hard:</p><ul>\n<li><code>STRLEN</code> can be used in order to obtain the number of samples.</li>\n<li><code>GETRANGE</code> allows for random access of elements.\nIf our time series have associated time information we can easily implement\na binary search to get range combining <code>GETRANGE</code> with the Lua scripting\nengine available in Redis 2.6.</li>\n<li><code>SETRANGE</code> can be used to overwrite an existing time series.</li>\n</ul><p>The limitation of this pattern is that we are forced into an append-only mode\nof operation, there is no way to cut the time series to a given size easily\nbecause Redis currently lacks a command able to trim string objects.\nHowever the space efficiency of time series stored in this way is remarkable.</p><p>Hint: it is possible to switch to a different key based on the current Unix\ntime, in this way it is possible to have just a relatively small amount of\nsamples per key, to avoid dealing with very big keys, and to make this pattern\nmore friendly to be distributed across many Redis instances.</p><p>An example sampling the temperature of a sensor using fixed-size strings (using\na binary format is better in real implementations).</p>","link":"/alpha/commands/append.html","spaLink":"#/alpha/commands/append","title":"PATTERN: TIME SERIES"},{"content":"<h2 id=\"-pattern-real-time-metrics-using-bitmaps\">Pattern: real-time metrics using bitmaps</h2><p>Bitmaps are a very space-efficient representation of certain kinds of\ninformation.\nOne example is a Web application that needs the history of user visits, so that\nfor instance it is possible to determine what users are good targets of beta\nfeatures.</p><p>Using the <code>SETBIT</code> command this is trivial to accomplish, identifying every day\nwith a small progressive integer.\nFor instance day 0 is the first day the application was put online, day 1 the\nnext day, and so forth.</p><p>Every time a user performs a page view, the application can register that in\nthe current day the user visited the web site using the <code>SETBIT</code> command setting\nthe bit corresponding to the current day.</p><p>Later it will be trivial to know the number of single days the user visited the\nweb site simply calling the <code>BITCOUNT</code> command against the bitmap.</p><p>A similar pattern where user IDs are used instead of days is described\nin the article called “<a href=\"http://blog.getspool.com/2011/11/29/fast-easy-realtime-metrics-using-redis-bitmaps\">Fast easy realtime metrics using Redis\nbitmaps</a>“.</p>","link":"/alpha/commands/bitcount.html","spaLink":"#/alpha/commands/bitcount","title":"PATTERN: REAL-TIME METRICS USING BITMAPS"},{"content":"<h2 id=\"-performance-considerations\">Performance considerations</h2><p>In the above example of counting days, even after 10 years the application is\nonline we still have just <code>365*10</code> bits of data per user, that is just 456 bytes\nper user.\nWith this amount of data <code>BITCOUNT</code> is still as fast as any other O(1) Redis\ncommand like <code>GET</code> or <code>INCR</code>.</p><p>When the bitmap is big, there are two alternatives:</p><ul>\n<li>Taking a separated key that is incremented every time the bitmap is modified.\nThis can be very efficient and atomic using a small Redis Lua script.</li>\n<li>Running the bitmap incrementally using the <code>BITCOUNT</code> <em>start</em> and <em>end</em>\noptional parameters, accumulating the results client-side, and optionally\ncaching the result into a key.</li>\n</ul>","link":"/alpha/commands/bitcount.html","spaLink":"#/alpha/commands/bitcount","title":"PERFORMANCE CONSIDERATIONS"},{"content":"<h2 id=\"-handling-of-strings-with-different-lengths\">Handling of strings with different lengths</h2><p>When an operation is performed between strings having different lengths, all the\nstrings shorter than the longest string in the set are treated as if they were\nzero-padded up to the length of the longest string.</p><p>The same holds true for non-existent keys, that are considered as a stream of\nzero bytes up to the length of the longest string.</p><p>@return</p><p>@integer-reply</p><p>The size of the string stored in the destination key, that is equal to the\nsize of the longest input string.</p><p>@examples</p>","link":"/alpha/commands/bitop.html","spaLink":"#/alpha/commands/bitop","title":"HANDLING OF STRINGS WITH DIFFERENT LENGTHS"},{"content":"<h2 id=\"-pattern-real-time-metrics-using-bitmaps\">Pattern: real time metrics using bitmaps</h2><p><code>BITOP</code> is a good complement to the pattern documented in the <code>BITCOUNT</code> command\ndocumentation.\nDifferent bitmaps can be combined in order to obtain a target bitmap where\nthe population counting operation is performed.</p><p>See the article called “<a href=\"http://blog.getspool.com/2011/11/29/fast-easy-realtime-metrics-using-redis-bitmaps\">Fast easy realtime metrics using Redis\nbitmaps</a>“ for a interesting use cases.</p>","link":"/alpha/commands/bitop.html","spaLink":"#/alpha/commands/bitop","title":"PATTERN: REAL TIME METRICS USING BITMAPS"},{"content":"<h2 id=\"-performance-considerations\">Performance considerations</h2><p><code>BITOP</code> is a potentially slow command as it runs in O(N) time.\nCare should be taken when running it against long input strings.</p><p>For real-time metrics and statistics involving large inputs a good approach is\nto use a slave (with read-only option disabled) where the bit-wise\noperations are performed to avoid blocking the master instance.</p>","link":"/alpha/commands/bitop.html","spaLink":"#/alpha/commands/bitop","title":"PERFORMANCE CONSIDERATIONS"},{"content":"<h2 id=\"-non-blocking-behavior\">Non-blocking behavior</h2><p>When <code>BLPOP</code> is called, if at least one of the specified keys contains a\nnon-empty list, an element is popped from the head of the list and returned to\nthe caller together with the <code>key</code> it was popped from.</p><p>Keys are checked in the order that they are given.\nLet’s say that the key <code>list1</code> doesn’t exist and <code>list2</code> and <code>list3</code> hold\nnon-empty lists.\nConsider the following command:</p><p><code>BLPOP</code> guarantees to return an element from the list stored at <code>list2</code> (since\nit is the first non empty list when checking <code>list1</code>, <code>list2</code> and <code>list3</code> in\nthat order).</p>","link":"/alpha/commands/blpop.html","spaLink":"#/alpha/commands/blpop","title":"NON-BLOCKING BEHAVIOR"},{"content":"<h2 id=\"-blocking-behavior\">Blocking behavior</h2><p>If none of the specified keys exist, <code>BLPOP</code> blocks the connection until another\nclient performs an <code>LPUSH</code> or <code>RPUSH</code> operation against one of the keys.</p><p>Once new data is present on one of the lists, the client returns with the name\nof the key unblocking it and the popped value.</p><p>When <code>BLPOP</code> causes a client to block and a non-zero timeout is specified,\nthe client will unblock returning a <code>nil</code> multi-bulk value when the specified\ntimeout has expired without a push operation against at least one of the\nspecified keys.</p><p><strong>The timeout argument is interpreted as an integer value specifying the maximum number of seconds to block</strong>. A timeout of zero can be used to block indefinitely.</p>","link":"/alpha/commands/blpop.html","spaLink":"#/alpha/commands/blpop","title":"BLOCKING BEHAVIOR"},{"content":"<h2 id=\"-what-key-is-served-first-what-client-what-element-priority-ordering-details\">What key is served first? What client? What element? Priority ordering details.</h2><ul>\n<li>If the client tries to blocks for multiple keys, but at least one key contains elements, the returned key / element pair is the first key from left to right that has one or more elements. In this case the client is not blocked. So for instance <code>BLPOP key1 key2 key3 key4 0</code>, assuming that both <code>key2</code> and <code>key4</code> are non-empty, will always return an element from <code>key2</code>.</li>\n<li>If multiple clients are blocked for the same key, the first client to be served is the one that was waiting for more time (the first that blocked for the key). Once a client is unblocked it does not retain any priority, when it blocks again with the next call to <code>BLPOP</code> it will be served accordingly to the number of clients already blocked for the same key, that will all be served before it (from the first to the last that blocked).</li>\n<li>When a client is blocking for multiple keys at the same time, and elements are available at the same time in multiple keys (because of a transaction or a Lua script added elements to multiple lists), the client will be unblocked using the first key that received a push operation (assuming it has enough elements to serve our client, as there may be other clients as well waiting for this key). Basically after the execution of every command Redis will run a list of all the keys that received data AND that have at least a client blocked. The list is ordered by new element arrival time, from the first key that received data to the last. For every key processed, Redis will serve all the clients waiting for that key in a FIFO fashion, as long as there are elements in this key. When the key is empty or there are no longer clients waiting for this key, the next key that received new data in the previous command / transaction / script is processed, and so forth.</li>\n</ul>","link":"/alpha/commands/blpop.html","spaLink":"#/alpha/commands/blpop","title":"WHAT KEY IS SERVED FIRST? WHAT CLIENT? WHAT ELEMENT? PRIORITY ORDERING DETAILS."},{"content":"<h2 id=\"-behavior-of-blpop-when-multiple-elements-are-pushed-inside-a-list\">Behavior of <code>!BLPOP</code> when multiple elements are pushed inside a list.</h2><p>There are times when a list can receive multiple elements in the context of the same conceptual command:</p><ul>\n<li>Variadic push operations such as <code>LPUSH mylist a b c</code>.</li>\n<li>After an <code>EXEC</code> of a <code>MULTI</code> block with multiple push operations against the same list.</li>\n<li>Executing a Lua Script with Redis 2.6 or newer.</li>\n</ul><p>When multiple elements are pushed inside a list where there are clients blocking, the behavior is different for Redis 2.4 and Redis 2.6 or newer.</p><p>For Redis 2.6 what happens is that the command performing multiple pushes is executed, and <em>only after</em> the execution of the command the blocked clients are served. Consider this sequence of commands.</p><p>If the above condition happens using a Redis 2.6 server or greater, Client <strong>A</strong> will be served with the <code>c</code> element, because after the <code>LPUSH</code> command the list contains <code>c,b,a</code>, so taking an element from the left means to return <code>c</code>.</p><p>Instead Redis 2.4 works in a different way: clients are served <em>in the context</em> of the push operation, so as long as <code>LPUSH foo a b c</code> starts pushing the first element to the list, it will be delivered to the Client <strong>A</strong>, that will receive <code>a</code> (the first element pushed).</p><p>The behavior of Redis 2.4 creates a lot of problems when replicating or persisting data into the AOF file, so the much more generic and semantically simpler behavior was introduced into Redis 2.6 to prevent problems.</p><p>Note that for the same reason a Lua script or a <code>MULTI/EXEC</code> block may push elements into a list and afterward <strong>delete the list</strong>. In this case the blocked clients will not be served at all and will continue to be blocked as long as no data is present on the list after the execution of a single command, transaction, or script.</p>","link":"/alpha/commands/blpop.html","spaLink":"#/alpha/commands/blpop","title":"BEHAVIOR OF !BLPOP WHEN MULTIPLE ELEMENTS ARE PUSHED INSIDE A LIST."},{"content":"<h2 id=\"-blpop-inside-a-multi-exec-transaction\"><code>!BLPOP</code> inside a <code>!MULTI</code> / <code>!EXEC</code> transaction</h2><p><code>BLPOP</code> can be used with pipelining (sending multiple commands and\nreading the replies in batch), however this setup makes sense almost solely\nwhen it is the last command of the pipeline.</p><p>Using <code>BLPOP</code> inside a <code>MULTI</code> / <code>EXEC</code> block does not make a lot of sense\nas it would require blocking the entire server in order to execute the block\natomically, which in turn does not allow other clients to perform a push\noperation. For this reason the behavior of <code>BLPOP</code> inside <code>MULTI</code> / <code>EXEC</code> when the list is empty is to return a <code>nil</code> multi-bulk reply, which is the same\nthing that happens when the timeout is reached.</p><p>If you like science fiction, think of time flowing at infinite speed inside a\n<code>MULTI</code> / <code>EXEC</code> block…</p><p>@return</p><p>@array-reply: specifically:</p><ul>\n<li>A <code>nil</code> multi-bulk when no element could be popped and the timeout expired.</li>\n<li>A two-element multi-bulk with the first element being the name of the key\nwhere an element was popped and the second element being the value of the\npopped element.</li>\n</ul><p>@examples</p>","link":"/alpha/commands/blpop.html","spaLink":"#/alpha/commands/blpop","title":"!BLPOP INSIDE A !MULTI / !EXEC TRANSACTION"},{"content":"<h2 id=\"-reliable-queues\">Reliable queues</h2><p>When <code>BLPOP</code> returns an element to the client, it also removes the element from the list. This means that the element only exists in the context of the client: if the client crashes while processing the returned element, it is lost forever.</p><p>This can be a problem with some application where we want a more reliable messaging system. When this is the case, please check the <code>BRPOPLPUSH</code> command, that is a variant of <code>BLPOP</code> that adds the returned element to a target list before returning it to the client.</p>","link":"/alpha/commands/blpop.html","spaLink":"#/alpha/commands/blpop","title":"RELIABLE QUEUES"},{"content":"<h2 id=\"-pattern-event-notification\">Pattern: Event notification</h2><p>Using blocking list operations it is possible to mount different blocking\nprimitives.\nFor instance for some application you may need to block waiting for elements\ninto a Redis Set, so that as far as a new element is added to the Set, it is\npossible to retrieve it without resort to polling.\nThis would require a blocking version of <code>SPOP</code> that is not available, but using\nblocking list operations we can easily accomplish this task.</p><p>The consumer will do:</p><p>While in the producer side we’ll use simply:</p>","link":"/alpha/commands/blpop.html","spaLink":"#/alpha/commands/blpop","title":"PATTERN: EVENT NOTIFICATION"},{"content":"<h2 id=\"-pattern-reliable-queue\">Pattern: Reliable queue</h2><p>Please see the pattern description in the <code>RPOPLPUSH</code> documentation.</p>","link":"/alpha/commands/brpoplpush.html","spaLink":"#/alpha/commands/brpoplpush","title":"PATTERN: RELIABLE QUEUE"},{"content":"<h2 id=\"-pattern-circular-list\">Pattern: Circular list</h2><p>Please see the pattern description in the <code>RPOPLPUSH</code> documentation.</p>","link":"/alpha/commands/brpoplpush.html","spaLink":"#/alpha/commands/brpoplpush","title":"PATTERN: CIRCULAR LIST"},{"content":"<h2 id=\"-client-kill-and-redis-sentinel\">CLIENT KILL and Redis Sentinel</h2><p>Recent versions of Redis Sentinel (Redis 2.8.12 or greater) use CLIENT KILL\nin order to kill clients when an instance is reconfigured, in order to\nforce clients to perform the handshake with one Sentinel again and update\nits configuration.</p>","link":"/alpha/commands/client-kill.html","spaLink":"#/alpha/commands/client-kill","title":"CLIENT KILL AND REDIS SENTINEL"},{"content":"<h2 id=\"-notes\">Notes</h2><p>Due to the single-threaded nature of Redis, it is not possible to\nkill a client connection while it is executing a command. From\nthe client point of view, the connection can never be closed\nin the middle of the execution of a command. However, the client\nwill notice the connection has been closed only when the\nnext command is sent (and results in network error).</p><p>@return</p><p>When called with the three arguments format:</p><p>@simple-string-reply: <code>OK</code> if the connection exists and has been closed</p><p>When called with the filter / value format:</p><p>@integer-reply: the number of clients killed.</p>","link":"/alpha/commands/client-kill.html","spaLink":"#/alpha/commands/client-kill","title":"NOTES"},{"content":"<h2 id=\"-notes\">Notes</h2><p>New fields are regularly added for debugging purpose. Some could be removed\nin the future. A version safe Redis client using this command should parse\nthe output accordingly (i.e. handling gracefully missing fields, skipping\nunknown fields).</p>","link":"/alpha/commands/client-list.html","spaLink":"#/alpha/commands/client-list","title":"NOTES"},{"content":"<h2 id=\"-example\">Example</h2><p>For example the following command assigns slots 1 2 3 to the node receiving\nthe command:</p><p>However trying to execute it again results into an error since the slots\nare already assigned:</p>","link":"/alpha/commands/cluster-addslots.html","spaLink":"#/alpha/commands/cluster-addslots","title":"EXAMPLE"},{"content":"<h2 id=\"-usage-in-redis-cluster\">Usage in Redis Cluster</h2><p>This command only works in cluster mode and is useful in the following\nRedis Cluster operations:</p>","link":"/alpha/commands/cluster-addslots.html","spaLink":"#/alpha/commands/cluster-addslots","title":"USAGE IN REDIS CLUSTER"},{"content":"<h2 id=\"-information-about-slots-propagation-and-warnings\">Information about slots propagation and warnings</h2><p>Note that once a node assigns a set of slots to itself, it will start\npropagating this information in heartbeat packet headers. However the\nother nodes will accept the information only if they have the slot as\nnot already bound with another node, or if the configuration epoch of the\nnode advertising the new hash slot, is greater than the node currently listed\nin the table.</p><p>This means that this command should be used with care only by applications\norchestrating Redis Cluster, like <code>redis-trib</code>, and the command if used\nout of the right context can leave the cluster in a wrong state or cause\ndata loss.</p><p>@return</p><p>@simple-string-reply: <code>OK</code> if the command was successful. Otherwise an error is returned.</p>","link":"/alpha/commands/cluster-addslots.html","spaLink":"#/alpha/commands/cluster-addslots","title":"INFORMATION ABOUT SLOTS PROPAGATION AND WARNINGS"},{"content":"<h2 id=\"-example\">Example</h2><p>The following command removes the association for slots 5000 and\n5001 from the node receiving the command:</p>","link":"/alpha/commands/cluster-delslots.html","spaLink":"#/alpha/commands/cluster-delslots","title":"EXAMPLE"},{"content":"<h2 id=\"-usage-in-redis-cluster\">Usage in Redis Cluster</h2><p>This command only works in cluster mode and may be useful for\ndebugging and in order to manually orchestrate a cluster configuration\nwhen a new cluster is created. It is currently not used by <code>redis-trib</code>,\nand mainly exists for API completeness.</p><p>@return</p><p>@simple-string-reply: <code>OK</code> if the command was successful. Otherwise\nan error is returned.</p>","link":"/alpha/commands/cluster-delslots.html","spaLink":"#/alpha/commands/cluster-delslots","title":"USAGE IN REDIS CLUSTER"},{"content":"<h2 id=\"-force-option-manual-failover-when-the-master-is-down\">FORCE option: manual failover when the master is down</h2><p>The command behavior can be modified by two options: <strong>FORCE</strong> and <strong>TAKEOVER</strong>.</p><p>If the <strong>FORCE</strong> option is given, the slave does not perform any handshake\nwith the master, that may be not reachable, but instead just starts a\nfailover ASAP starting from point 4. This is useful when we want to start\na manual failover while the master is no longer reachable.</p><p>However using <strong>FORCE</strong> we still need the majority of masters to be available\nin order to authorize the failover and generate a new configuration epoch\nfor the slave that is going to become master.</p>","link":"/alpha/commands/cluster-failover.html","spaLink":"#/alpha/commands/cluster-failover","title":"FORCE OPTION: MANUAL FAILOVER WHEN THE MASTER IS DOWN"},{"content":"<h2 id=\"-takeover-option-manual-failover-without-cluster-consensus\">TAKEOVER option: manual failover without cluster consensus</h2><p>There are situations where this is not enough, and we want a slave to failover\nwithout any agreement with the rest of the cluster. A real world use case\nfor this is to mass promote slaves in a different data center to masters\nin order to perform a data center switch, while all the masters are down\nor partitioned away.</p><p>The <strong>TAKEOVER</strong> option implies everything <strong>FORCE</strong> implies, but also does\nnot uses any cluster authorization in order to failover. A slave receiving\n<code>CLUSTER FAILOVER TAKEOVER</code> will instead:</p><p>Note that <strong>TAKEOVER violates the last-failover-wins principle</strong> of Redis Cluster, since the configuration epoch generated by the slave violates the normal generation of configuration epochs in several ways:</p><p>Because of this the <strong>TAKEOVER</strong> option should be used with care.</p>","link":"/alpha/commands/cluster-failover.html","spaLink":"#/alpha/commands/cluster-failover","title":"TAKEOVER OPTION: MANUAL FAILOVER WITHOUT CLUSTER CONSENSUS"},{"content":"<h2 id=\"-implementation-details-and-notes\">Implementation details and notes</h2><p><code>CLUSTER FAILOVER</code>, unless the <strong>TAKEOVER</strong> option is specified,  does not\nexecute a failover synchronously, it only <em>schedules</em> a manual failover,\nbypassing the failure detection stage, so to check if the failover actually\nhappened, <code>CLUSTER NODES</code> or other means should be used in order to verify\nthat the state of the cluster changes after some time the command was sent.</p><p>@return</p><p>@simple-string-reply: <code>OK</code> if the command was accepted and a manual failover is going to be attempted. An error if the operation cannot be executed, for example if we are talking with a node which is already a master.</p>","link":"/alpha/commands/cluster-failover.html","spaLink":"#/alpha/commands/cluster-failover","title":"IMPLEMENTATION DETAILS AND NOTES"},{"content":"<h1 id=\"redis-administration\">Redis Administration</h1><p>This page contains topics related to the administration of Redis instances.\nEvery topic is self contained in form of a FAQ. New topics will be created in the future.</p>","link":"/alpha/topics/admin.html","spaLink":"#/alpha/topics/admin","title":"REDIS ADMINISTRATION"},{"content":"<h2 id=\"redis-administration-redis-setup-hints\">Redis setup hints</h2><ul>\n<li>We suggest deploying Redis using the <strong>Linux operating system</strong>. Redis is also tested heavily on OS X, and tested from time to time on FreeBSD and OpenBSD systems. However Linux is where we do all the major stress testing, and where most production deployments are working.</li>\n<li>Make sure to set the Linux kernel <strong>overcommit memory setting to 1</strong>. Add <code>vm.overcommit_memory = 1</code> to <code>/etc/sysctl.conf</code> and then reboot or run the command <code>sysctl vm.overcommit_memory=1</code> for this to take effect immediately.</li>\n<li>Make sure to disable Linux kernel feature <em>transparent huge pages</em>, it will affect greatly both memory usage and latency in a negative way. This is accomplished with the following command: <code>echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled</code>.</li>\n<li>Make sure to <strong>setup some swap</strong> in your system (we suggest as much as swap as memory). If Linux does not have swap and your Redis instance accidentally consumes too much memory, either Redis will crash for out of memory or the Linux kernel OOM killer will kill the Redis process.</li>\n<li>Set an explicit <code>maxmemory</code> option limit in your instance in order to make sure that the instance will report errors instead of failing when the system memory limit is near to be reached.</li>\n<li>If you are using Redis in a very write-heavy application, while saving an RDB file on disk or rewriting the AOF log <strong>Redis may use up to 2 times the memory normally used</strong>. The additional memory used is proportional to the number of memory pages modified by writes during the saving process, so it is often proportional to the number of keys (or aggregate types items) touched during this time. Make sure to size your memory accordingly.</li>\n<li>Use <code>daemonize no</code> when run under daemontools.</li>\n<li>Even if you have persistence disabled, Redis will need to perform RDB saves if you use replication, unless you use the new diskless replication feature, which is currently experimental.</li>\n<li>If you are using replication, make sure that either your master has persistence enabled, or that it does not automatically restarts on crashes: slaves will try to be an exact copy of the master, so if a master restarts with an empty data set, slaves will be wiped as well.</li>\n<li>By default Redis does not require <strong>any authentication and listens to all the network interfaces</strong>. This is a big security issue if you leave Redis exposed on the internet or other places where attackers can reach it. See for example <a href=\"http://antirez.com/news/96\">this attack</a> to see how dangerous it can be. Please check our <a href=\"/topics/security\">security page</a> and the <a href=\"/topic/quickstart\">quick start</a> for information about how to secure Redis.</li>\n</ul>","link":"/alpha/topics/admin.html","spaLink":"#/alpha/topics/admin","title":"REDIS SETUP HINTS"},{"content":"<h2 id=\"redis-administration-running-redis-on-ec2\">Running Redis on EC2</h2><ul>\n<li>Use HVM based instances, not PV based instances.</li>\n<li>Don’t use old instances families, for example: use m3.medium with HVM instead of m1.medium with PV.</li>\n<li>The use of Redis persistence with <strong>EC2 EBS volumes</strong> needs to be handled with care since sometimes EBS volumes have high latency characteristics.</li>\n<li>You may want to try the new <strong>diskless replication</strong> (currently experimental) if you have issues when slaves are synchronizing with the master.</li>\n</ul>","link":"/alpha/topics/admin.html","spaLink":"#/alpha/topics/admin","title":"RUNNING REDIS ON EC2"},{"content":"<h2 id=\"redis-administration-upgrading-or-restarting-a-redis-instance-without-downtime\">Upgrading or restarting a Redis instance without downtime</h2><p>Redis is designed to be a very long running process in your server.\nFor instance many configuration options can be modified without any kind of restart using the <a href=\"/commands/config-set\">CONFIG SET command</a>.</p><p>Starting from Redis 2.2 it is even possible to switch from AOF to RDB snapshots persistence or the other way around without restarting Redis. Check the output of the <code>CONFIG GET *</code> command for more information.</p><p>However from time to time a restart is mandatory, for instance in order to upgrade the Redis process to a newer version, or when you need to modify some configuration parameter that is currently not supported by the CONFIG command.</p><p>The following steps provide a very commonly used way in order to avoid any downtime.</p><ul>\n<li>Setup your new Redis instance as a slave for your current Redis instance. In order to do so you need a different server, or a server that has enough RAM to keep two instances of Redis running at the same time.</li>\n<li>If you use a single server, make sure that the slave is started in a different port than the master instance, otherwise the slave will not be able to start at all.</li>\n<li>Wait for the replication initial synchronization to complete (check the slave log file).</li>\n<li>Make sure using INFO that there are the same number of keys in the master and in the slave. Check with redis-cli that the slave is working as you wish and is replying to your commands.</li>\n<li>Allow writes to the slave using <strong>CONFIG SET slave-read-only no</strong></li>\n<li>Configure all your clients in order to use the new instance (that is, the slave).</li>\n<li>Once you are sure that the master is no longer receiving any query (you can check this with the <a href=\"/commands/monitor\">MONITOR command</a>), elect the slave to master using the <strong>SLAVEOF NO ONE</strong> command, and shut down your master.</li>\n</ul>","link":"/alpha/topics/admin.html","spaLink":"#/alpha/topics/admin","title":"UPGRADING OR RESTARTING A REDIS INSTANCE WITHOUT DOWNTIME"},{"content":"<h1 id=\"how-fast-is-redis\">How fast is Redis?</h1><p>Redis includes the <code>redis-benchmark</code> utility that simulates running commands done\nby N clients at the same time sending M total queries (it is similar to the\nApache’s <code>ab</code> utility). Below you’ll find the full output of a benchmark executed\nagainst a Linux box.</p><p>The following options are supported:</p><p>You need to have a running Redis instance before launching the benchmark.\nA typical example would be:</p><p>Using this tool is quite easy, and you can also write your own benchmark,\nbut as with any benchmarking activity, there are some pitfalls to avoid.</p>","link":"/alpha/topics/benchmarks.html","spaLink":"#/alpha/topics/benchmarks","title":"HOW FAST IS REDIS?"},{"content":"<h2 id=\"how-fast-is-redis-running-only-a-subset-of-the-tests\">Running only a subset of the tests</h2><p>You don’t need to run all the default tests every time you execute redis-benchmark.\nThe simplest thing to select only a subset of tests is to use the <code>-t</code> option\nlike in the following example:</p><p>In the above example we asked to just run test the SET and LPUSH commands,\nin quiet mode (see the <code>-q</code> switch).</p><p>It is also possible to specify the command to benchmark directly like in the\nfollowing example:</p>","link":"/alpha/topics/benchmarks.html","spaLink":"#/alpha/topics/benchmarks","title":"RUNNING ONLY A SUBSET OF THE TESTS"},{"content":"<h2 id=\"how-fast-is-redis-selecting-the-size-of-the-key-space\">Selecting the size of the key space</h2><p>By default the benchmark runs against a single key. In Redis the difference\nbetween such a synthetic benchmark and a real one is not huge since it is an\nin-memory system, however it is possible to stress cache misses and in general\nto simulate a more real-world work load by using a large key space.</p><p>This is obtained by using the <code>-r</code> switch. For instance if I want to run\none million SET operations, using a random key for every operation out of\n100k possible keys, I’ll use the following command line:</p>","link":"/alpha/topics/benchmarks.html","spaLink":"#/alpha/topics/benchmarks","title":"SELECTING THE SIZE OF THE KEY SPACE"},{"content":"<h2 id=\"how-fast-is-redis-using-pipelining\">Using pipelining</h2><p>By default every client (the benchmark simulates 50 clients if not otherwise\nspecified with <code>-c</code>) sends the next command only when the reply of the previous\ncommand is received, this means that the server will likely need a read call\nin order to read each command from every client. Also RTT is paid as well.</p><p>Redis supports <a href=\"pipelining\">/topics/pipelining</a>, so it is possible to send\nmultiple commands at once, a feature often exploited by real world applications.\nRedis pipelining is able to dramatically improve the number of operations per\nsecond a server is able do deliver.</p><p>This is an example of running the benchmark in a MacBook Air 11” using a\npipelining of 16 commands:</p><p>Using pipelining results in a significant increase in performance.</p>","link":"/alpha/topics/benchmarks.html","spaLink":"#/alpha/topics/benchmarks","title":"USING PIPELINING"},{"content":"<h2 id=\"how-fast-is-redis-pitfalls-and-misconceptions\">Pitfalls and misconceptions</h2><p>The first point is obvious: the golden rule of a useful benchmark is to\nonly compare apples and apples. Different versions of Redis can be compared\non the same workload for instance. Or the same version of Redis, but with\ndifferent options. If you plan to compare Redis to something else, then it is\nimportant to evaluate the functional and technical differences, and take them\nin account.</p><ul>\n<li>Redis is a server: all commands involve network or IPC round trips. It is\nmeaningless to compare it to embedded data stores such as SQLite, Berkeley DB,\nTokyo/Kyoto Cabinet, etc … because the cost of most operations is\nprimarily in network/protocol management.</li>\n<li>Redis commands return an acknowledgment for all usual commands. Some other\ndata stores do not (for instance MongoDB does not implicitly acknowledge write\noperations). Comparing Redis to stores involving one-way queries is only\nmildly useful.</li>\n<li>Naively iterating on synchronous Redis commands does not benchmark Redis\nitself, but rather measure your network (or IPC) latency. To really test Redis,\nyou need multiple connections (like redis-benchmark) and/or to use pipelining\nto aggregate several commands and/or multiple threads or processes.</li>\n<li>Redis is an in-memory data store with some optional persistence options. If\nyou plan to compare it to transactional servers (MySQL, PostgreSQL, etc …),\nthen you should consider activating AOF and decide on a suitable fsync policy.</li>\n<li>Redis is a single-threaded server. It is not designed to benefit from\nmultiple CPU cores. People are supposed to launch several Redis instances to\nscale out on several cores if needed. It is not really fair to compare one\nsingle Redis instance to a multi-threaded data store.</li>\n</ul><p>A common misconception is that redis-benchmark is designed to make Redis\nperformances look stellar, the throughput achieved by redis-benchmark being\nsomewhat artificial, and not achievable by a real application. This is\nactually plain wrong.</p><p>The redis-benchmark program is a quick and useful way to get some figures and\nevaluate the performance of a Redis instance on a given hardware. However,\nby default, it does not represent the maximum throughput a Redis instance can\nsustain. Actually, by using pipelining and a fast client (hiredis), it is fairly\neasy to write a program generating more throughput than redis-benchmark. The\ndefault behavior of redis-benchmark is to achieve throughput by exploiting\nconcurrency only (i.e. it creates several connections to the server).\nIt does not use pipelining or any parallelism at all (one pending query per\nconnection at most, and no multi-threading).</p><p>To run a benchmark using pipelining mode (and achieve higher throughput),\nyou need to explicitly use the -P option. Please note that it is still a\nrealistic behavior since a lot of Redis based applications actively use\npipelining to improve performance.</p><p>Finally, the benchmark should apply the same operations, and work in the same way\nwith the multiple data stores you want to compare. It is absolutely pointless to\ncompare the result of redis-benchmark to the result of another benchmark\nprogram and extrapolate.</p><p>For instance, Redis and memcached in single-threaded mode can be compared on\nGET/SET operations. Both are in-memory data stores, working mostly in the same\nway at the protocol level. Provided their respective benchmark application is\naggregating queries in the same way (pipelining) and use a similar number of\nconnections, the comparison is actually meaningful.</p><p>This perfect example is illustrated by the dialog between Redis (antirez) and\nmemcached (dormando) developers.</p><p><a href=\"http://antirez.com/post/redis-memcached-benchmark.html\">antirez 1 - On Redis, Memcached, Speed, Benchmarks and The Toilet</a></p><p><a href=\"http://dormando.livejournal.com/525147.html\">dormando - Redis VS Memcached (slightly better bench)</a></p><p><a href=\"http://antirez.com/post/update-on-memcached-redis-benchmark.html\">antirez 2 - An update on the Memcached/Redis benchmark</a></p><p>You can see that in the end, the difference between the two solutions is not\nso staggering, once all technical aspects are considered. Please note both\nRedis and memcached have been optimized further after these benchmarks.</p><p>Finally, when very efficient servers are benchmarked (and stores like Redis\nor memcached definitely fall in this category), it may be difficult to saturate\nthe server. Sometimes, the performance bottleneck is on client side,\nand not server-side. In that case, the client (i.e. the benchmark program itself)\nmust be fixed, or perhaps scaled out, in order to reach the maximum throughput.</p>","link":"/alpha/topics/benchmarks.html","spaLink":"#/alpha/topics/benchmarks","title":"PITFALLS AND MISCONCEPTIONS"},{"content":"<h2 id=\"how-fast-is-redis-factors-impacting-redis-performance\">Factors impacting Redis performance</h2><p>There are multiple factors having direct consequences on Redis performance.\nWe mention them here, since they can alter the result of any benchmarks.\nPlease note however, that a typical Redis instance running on a low end,\nuntuned box usually provides good enough performance for most applications.</p><ul>\n<li>Network bandwidth and latency usually have a direct impact on the performance.\nIt is a good practice to use the ping program to quickly check the latency\nbetween the client and server hosts is normal before launching the benchmark.\nRegarding the bandwidth, it is generally useful to estimate\nthe throughput in Gbit/s and compare it to the theoretical bandwidth\nof the network. For instance a benchmark setting 4 KB strings\nin Redis at 100000 q/s, would actually consume 3.2 Gbit/s of bandwidth\nand probably fit within a 10 Gbit/s link, but not a 1 Gbit/s one. In many real\nworld scenarios, Redis throughput is limited by the network well before being\nlimited by the CPU. To consolidate several high-throughput Redis instances\non a single server, it worth considering putting a 10 Gbit/s NIC\nor multiple 1 Gbit/s NICs with TCP/IP bonding.</li>\n<li>CPU is another very important factor. Being single-threaded, Redis favors\nfast CPUs with large caches and not many cores. At this game, Intel CPUs are\ncurrently the winners. It is not uncommon to get only half the performance on\nan AMD Opteron CPU compared to similar Nehalem EP/Westmere EP/Sandy Bridge\nIntel CPUs with Redis. When client and server run on the same box, the CPU is\nthe limiting factor with redis-benchmark.</li>\n<li>Speed of RAM and memory bandwidth seem less critical for global performance\nespecially for small objects. For large objects (&gt;10 KB), it may become\nnoticeable though. Usually, it is not really cost-effective to buy expensive\nfast memory modules to optimize Redis.</li>\n<li>Redis runs slower on a VM compared to running without virtualization using\nthe same hardware. If you have the chance to run Redis on a physical machine\nthis is preferred. However this does not mean that Redis is slow in\nvirtualized environments, the delivered performances are still very good\nand most of the serious performance issues you may incur in virtualized\nenvironments are due to over-provisioning, non-local disks with high latency,\nor old hypervisor software that have slow <code>fork</code> syscall implementation.</li>\n<li>When the server and client benchmark programs run on the same box, both\nthe TCP/IP loopback and unix domain sockets can be used. Depending on the\nplatform, unix domain sockets can achieve around 50% more throughput than\nthe TCP/IP loopback (on Linux for instance). The default behavior of\nredis-benchmark is to use the TCP/IP loopback.</li>\n<li>The performance benefit of unix domain sockets compared to TCP/IP loopback\ntends to decrease when pipelining is heavily used (i.e. long pipelines).</li>\n<li>When an ethernet network is used to access Redis, aggregating commands using\npipelining is especially efficient when the size of the data is kept under\nthe ethernet packet size (about 1500 bytes). Actually, processing 10 bytes,\n100 bytes, or 1000 bytes queries almost result in the same throughput.\nSee the graph below.</li>\n</ul><p><img src=\"https://github.com/dspezia/redis-doc/raw/client_command/topics/Data_size.png\" alt=\"Data size impact\"></p><ul>\n<li>On multi CPU sockets servers, Redis performance becomes dependent on the\nNUMA configuration and process location. The most visible effect is that\nredis-benchmark results seem non-deterministic because client and server\nprocesses are distributed randomly on the cores. To get deterministic results,\nit is required to use process placement tools (on Linux: taskset or numactl).\nThe most efficient combination is always to put the client and server on two\ndifferent cores of the same CPU to benefit from the L3 cache.\nHere are some results of 4 KB SET benchmark for 3 server CPUs (AMD Istanbul,\nIntel Nehalem EX, and Intel Westmere) with different relative placements.\nPlease note this benchmark is not meant to compare CPU models between themselves\n(CPUs exact model and frequency are therefore not disclosed).</li>\n</ul><p><img src=\"https://github.com/dspezia/redis-doc/raw/6374a07f93e867353e5e946c1e39a573dfc83f6c/topics/NUMA_chart.gif\" alt=\"NUMA chart\"></p><ul>\n<li>With high-end configurations, the number of client connections is also an\nimportant factor. Being based on epoll/kqueue, the Redis event loop is quite\nscalable. Redis has already been benchmarked at more than 60000 connections,\nand was still able to sustain 50000 q/s in these conditions. As a rule of thumb,\nan instance with 30000 connections can only process half the throughput\nachievable with 100 connections. Here is an example showing the throughput of\na Redis instance per number of connections:</li>\n</ul><p><img src=\"https://github.com/dspezia/redis-doc/raw/system_info/topics/Connections_chart.png\" alt=\"connections chart\"></p><ul>\n<li>With high-end configurations, it is possible to achieve higher throughput by\ntuning the NIC(s) configuration and associated interruptions. Best throughput\nis achieved by setting an affinity between Rx/Tx NIC queues and CPU cores,\nand activating RPS (Receive Packet Steering) support. More information in this\n<a href=\"https://groups.google.com/forum/#!msg/redis-db/gUhc19gnYgc/BruTPCOroiMJ\">thread</a>.\nJumbo frames may also provide a performance boost when large objects are used.</li>\n<li>Depending on the platform, Redis can be compiled against different memory\nallocators (libc malloc, jemalloc, tcmalloc), which may have different behaviors\nin term of raw speed, internal and external fragmentation.\nIf you did not compile Redis yourself, you can use the INFO command to check\nthe mem_allocator field. Please note most benchmarks do not run long enough to\ngenerate significant external fragmentation (contrary to production Redis\ninstances).</li>\n</ul>","link":"/alpha/topics/benchmarks.html","spaLink":"#/alpha/topics/benchmarks","title":"FACTORS IMPACTING REDIS PERFORMANCE"},{"content":"<h2 id=\"how-fast-is-redis-other-things-to-consider\">Other things to consider</h2><p>One important goal of any benchmark is to get reproducible results, so they\ncan be compared to the results of other tests.</p><ul>\n<li>A good practice is to try to run tests on isolated hardware as much as possible.\nIf it is not possible, then the system must be monitored to check the benchmark\nis not impacted by some external activity.</li>\n<li>Some configurations (desktops and laptops for sure, some servers as well)\nhave a variable CPU core frequency mechanism. The policy controlling this\nmechanism can be set at the OS level. Some CPU models are more aggressive than\nothers at adapting the frequency of the CPU cores to the workload. To get\nreproducible results, it is better to set the highest possible fixed frequency\nfor all the CPU cores involved in the benchmark.</li>\n<li>An important point is to size the system accordingly to the benchmark.\nThe system must have enough RAM and must not swap. On Linux, do not forget\nto set the overcommit_memory parameter correctly. Please note 32 and 64 bit\nRedis instances do not have the same memory footprint.</li>\n<li>If you plan to use RDB or AOF for your benchmark, please check there is no other\nI/O activity in the system. Avoid putting RDB or AOF files on NAS or NFS shares,\nor on any other devices impacting your network bandwidth and/or latency\n(for instance, EBS on Amazon EC2).</li>\n<li>Set Redis logging level (loglevel parameter) to warning or notice. Avoid putting\nthe generated log file on a remote filesystem.</li>\n<li>Avoid using monitoring tools which can alter the result of the benchmark. For\ninstance using INFO at regular interval to gather statistics is probably fine,\nbut MONITOR will impact the measured performance significantly.</li>\n</ul>","link":"/alpha/topics/benchmarks.html","spaLink":"#/alpha/topics/benchmarks","title":"OTHER THINGS TO CONSIDER"},{"content":"<h1 id=\"benchmark-results-on-different-virtualized-and-bare-metal-servers\">Benchmark results on different virtualized and bare-metal servers.</h1><ul>\n<li>The test was done with 50 simultaneous clients performing 2 million requests.</li>\n<li>Redis 2.6.14 is used for all the tests.</li>\n<li>Test was executed using the loopback interface.</li>\n<li>Test was executed using a key space of 1 million keys.</li>\n<li>Test was executed with and without pipelining (16 commands pipeline).</li>\n</ul><p><strong>Intel(R) Xeon(R) CPU E5520  @ 2.27GHz (with pipelining)</strong></p><p><strong>Intel(R) Xeon(R) CPU E5520  @ 2.27GHz (without pipelining)</strong></p><p><strong>Linode 2048 instance (with pipelining)</strong></p><p><strong>Linode 2048 instance (without pipelining)</strong></p>","link":"/alpha/topics/benchmarks.html","spaLink":"#/alpha/topics/benchmarks","title":"BENCHMARK RESULTS ON DIFFERENT VIRTUALIZED AND BARE-METAL SERVERS."},{"content":"<h2 id=\"benchmark-results-on-different-virtualized-and-bare-metal-servers-more-detailed-tests-without-pipelining\">More detailed tests without pipelining</h2><p>Notes: changing the payload from 256 to 1024 or 4096 bytes does not change the\nnumbers significantly (but reply packets are glued together up to 1024 bytes so\nGETs may be slower with big payloads). The same for the number of clients, from\n50 to 256 clients I got the same numbers. With only 10 clients it starts to get\na bit slower.</p><p>You can expect different results from different boxes. For example a low\nprofile box like <em>Intel core duo T5500 clocked at 1.66 GHz running Linux 2.6</em>\nwill output the following:</p><p>Another one using a 64-bit box, a Xeon L5420 clocked at 2.5 GHz:</p>","link":"/alpha/topics/benchmarks.html","spaLink":"#/alpha/topics/benchmarks","title":"MORE DETAILED TESTS WITHOUT PIPELINING"},{"content":"<h1 id=\"example-of-benchmark-results-with-optimized-high-end-server-hardware\">Example of benchmark results with optimized high-end server hardware</h1><ul>\n<li>Redis version <strong>2.4.2</strong></li>\n<li>Default number of connections, payload size = 256</li>\n<li>The Linux box is running <em>SLES10 SP3 2.6.16.60-0.54.5-smp</em>, CPU is 2 x <em>Intel X5670 @ 2.93 GHz</em>.</li>\n<li>Test executed while running Redis server and benchmark client on the same CPU, but different cores.</li>\n</ul><p>Using a unix domain socket:</p><p>Using the TCP loopback:</p>","link":"/alpha/topics/benchmarks.html","spaLink":"#/alpha/topics/benchmarks","title":"EXAMPLE OF BENCHMARK RESULTS WITH OPTIMIZED HIGH-END SERVER HARDWARE"},{"content":"<h1 id=\"redis-clients-handling\">Redis Clients Handling</h1><p>This document provides information about how Redis handles clients from the\npoint of view of the network layer: connections, timeouts, buffers, and\nother similar topics are covered here.</p><p>The information contained in this document is <strong>only applicable to Redis version 2.6 or greater</strong>.</p>","link":"/alpha/topics/clients.html","spaLink":"#/alpha/topics/clients","title":"REDIS CLIENTS HANDLING"},{"content":"<h2 id=\"redis-clients-handling-how-client-connections-are-accepted\">How client connections are accepted</h2><p>Redis accepts clients connections on the configured listening TCP port and\non the Unix socket if enabled. When a new client connection is accepted\nthe following operations are performed:</p><ul>\n<li>The client socket is put in non-blocking state since Redis uses multiplexing and non-blocking I/O.</li>\n<li>The <code>TCP_NODELAY</code> option is set in order to ensure that we don’t have delays in our connection.</li>\n<li>A <em>readable</em> file event is created so that Redis is able to collect the client queries as soon as new data is available to be read on the socket.</li>\n</ul><p>After the client is initialized, Redis checks if we are already at the limit\nof the number of clients that it is possible to handle simultaneously\n(this is configured using the <code>maxclients</code> configuration directive, see the\nnext section of this document for further information).</p><p>In case it can’t accept the current client because the maximum number of clients\nwas already accepted, Redis tries to send an error to the client in order to\nmake it aware of this condition, and closes the connection immediately.\nThe error message will be able to reach the client even if the connection is\nclosed immediately by Redis because the new socket output buffer is usually\nbig enough to contain the error, so the kernel will handle the transmission\nof the error.</p>","link":"/alpha/topics/clients.html","spaLink":"#/alpha/topics/clients","title":"HOW CLIENT CONNECTIONS ARE ACCEPTED"},{"content":"<h2 id=\"redis-clients-handling-in-what-order-clients-are-served\">In what order clients are served</h2><p>The order is determined by a combination of the client socket file descriptor\nnumber and order in which the kernel reports events, so the order is to be\nconsidered as unspecified.</p><p>However Redis does the following two things when serving clients:</p><ul>\n<li>It only performs a single <code>read()</code> system call every time there is something new to read from the client socket, in order to ensure that if we have multiple clients connected, and a few are very demanding clients sending queries at an high rate, other clients are not penalized and will not experience a bad latency figure.</li>\n<li>However once new data is read from a client, all the queries contained in the current buffers are processed sequentially. This improves locality and does not need iterating a second time to see if there are clients that need some processing time.</li>\n</ul>","link":"/alpha/topics/clients.html","spaLink":"#/alpha/topics/clients","title":"IN WHAT ORDER CLIENTS ARE SERVED"},{"content":"<h2 id=\"redis-clients-handling-maximum-number-of-clients\">Maximum number of clients</h2><p>In Redis 2.4 there was an hard-coded limit about the maximum number of clients\nthat was possible to handle simultaneously.</p><p>In Redis 2.6 this limit is dynamic: by default is set to 10000 clients, unless\notherwise stated by the <code>maxclients</code> directive in Redis.conf.</p><p>However Redis checks with the kernel what is the maximum number of file\ndescriptors that we are able to open (the <em>soft limit</em> is checked), if the\nlimit is smaller than the maximum number of clients we want to handle, plus\n32 (that is the number of file descriptors Redis reserves for internal uses),\nthen the number of maximum clients is modified by Redis to match the amount\nof clients we are <em>really able to handle</em> under the current operating system\nlimit.</p><p>When the configured number of maximum clients can not be honored, the condition\nis logged at startup as in the following example:</p><p>When Redis is configured in order to handle a specific number of clients it\nis a good idea to make sure that the operating system limit to the maximum\nnumber of file descriptors per process is also set accordingly.</p><p>Under Linux these limits can be set both in the current session and as a\nsystem-wide setting with the following commands:</p><ul>\n<li>ulimit -Sn 100000 # This will only work if hard limit is big enough.</li>\n<li>sysctl -w fs.file-max=100000</li>\n</ul>","link":"/alpha/topics/clients.html","spaLink":"#/alpha/topics/clients","title":"MAXIMUM NUMBER OF CLIENTS"},{"content":"<h2 id=\"redis-clients-handling-output-buffers-limits\">Output buffers limits</h2><p>Redis needs to handle a variable-length output buffer for every client, since\na command can produce a big amount of data that needs to be transferred to the\nclient.</p><p>However it is possible that a client sends more commands producing more output\nto serve at a faster rate at which Redis can send the existing output to the\nclient. This is especially true with Pub/Sub clients in case a client is not\nable to process new messages fast enough.</p><p>Both the conditions will cause the client output buffer to grow and consume\nmore and more memory. For this reason by default Redis sets limits to the\noutput buffer size for different kind of clients. When the limit is reached\nthe client connection is closed and the event logged in the Redis log file.</p><p>There are two kind of limits Redis uses:</p><ul>\n<li>The <strong>hard limit</strong> is a fixed limit that when reached will make Redis closing the client connection as soon as possible.</li>\n<li>The <strong>soft limit</strong> instead is a limit that depends on the time, for instance a soft limit of 32 megabytes per 10 seconds means that if the client has an output buffer bigger than 32 megabytes for, continuously, 10 seconds, the connection gets closed.</li>\n</ul><p>Different kind of clients have different default limits:</p><ul>\n<li><strong>Normal clients</strong> have a default limit of 0, that means, no limit at all, because most normal clients use blocking implementations sending a single command and waiting for the reply to be completely read before sending the next command, so it is always not desirable to close the connection in case of a normal client.</li>\n<li><strong>Pub/Sub clients</strong> have a default hard limit of 32 megabytes and a soft limit of 8 megabytes per 60 seconds.</li>\n<li><strong>Slaves</strong> have a default hard limit of 256 megabytes and a soft limit of 64 megabyte per 60 second.</li>\n</ul><p>It is possible to change the limit at runtime using the <code>CONFIG SET</code> command or in a permanent way using the Redis configuration file <code>redis.conf</code>. See the example <code>redis.conf</code> in the Redis distribution for more information about how to set the limit.</p>","link":"/alpha/topics/clients.html","spaLink":"#/alpha/topics/clients","title":"OUTPUT BUFFERS LIMITS"},{"content":"<h2 id=\"redis-clients-handling-query-buffer-hard-limit\">Query buffer hard limit</h2><p>Every client is also subject to a query buffer limit. This is a non-configurable hard limit that will close the connection when the client query buffer (that is the buffer we use to accumulate commands from the client) reaches 1 GB, and is actually only an extreme limit to avoid a server crash in case of client or server software bugs.</p>","link":"/alpha/topics/clients.html","spaLink":"#/alpha/topics/clients","title":"QUERY BUFFER HARD LIMIT"},{"content":"<h2 id=\"redis-clients-handling-client-timeouts\">Client timeouts</h2><p>By default recent versions of Redis don’t close the connection with the client\nif the client is idle for many seconds: the connection will remain open forever.</p><p>However if you don’t like this behavior, you can configure a timeout, so that\nif the client is idle for more than the specified number of seconds, the client connection will be closed.</p><p>You can configure this limit via <code>redis.conf</code> or simply using <code>CONFIG SET timeout &lt;value&gt;</code>.</p><p>Note that the timeout only applies to number clients and it <strong>does not apply to Pub/Sub clients</strong>, since a Pub/Sub connection is a <em>push style</em> connection so a client that is idle is the norm.</p><p>Even if by default connections are not subject to timeout, there are two conditions when it makes sense to set a timeout:</p><ul>\n<li>Mission critical applications where a bug in the client software may saturate the Redis server with idle connections, causing service disruption.</li>\n<li>As a debugging mechanism in order to be able to connect with the server if a bug in the client software saturates the server with idle connections, making it impossible to interact with the server.</li>\n</ul><p>Timeouts are not to be considered very precise: Redis avoids to set timer events or to run O(N) algorithms in order to check idle clients, so the check is performed incrementally from time to time. This means that it is possible that while the timeout is set to 10 seconds, the client connection will be closed, for instance, after 12 seconds if many clients are connected at the same time.</p>","link":"/alpha/topics/clients.html","spaLink":"#/alpha/topics/clients","title":"CLIENT TIMEOUTS"},{"content":"<h2 id=\"redis-clients-handling-client-command\">CLIENT command</h2><p>The Redis client command allows to inspect the state of every connected client, to kill a specific client, to set names to connections. It is a very powerful debugging tool if you use Redis at scale.</p><p><code>CLIENT LIST</code> is used in order to obtain a list of connected clients and their state:</p><p>In the above example session two clients are connected to the Redis server. The meaning of a few of the most interesting fields is the following:</p><ul>\n<li><strong>addr</strong>: The client address, that is, the client IP and the remote port number it used to connect with the Redis server.</li>\n<li><strong>fd</strong>: The client socket file descriptor number.</li>\n<li><strong>name</strong>: The client name as set by <code>CLIENT SETNAME</code>.</li>\n<li><strong>age</strong>: The number of seconds the connection existed for.</li>\n<li><strong>idle</strong>: The number of seconds the connection is idle.</li>\n<li><strong>flags</strong>: The kind of client (N means normal client, check the <a href=\"http://redis.io/commands/client-list\">full list of flags</a>).</li>\n<li><strong>omem</strong>: The amount of memory used by the client for the output buffer.</li>\n<li><strong>cmd</strong>: The last executed command.</li>\n</ul><p>See the <a href=\"http://redis.io/commands/client-list\">CLIENT LIST</a> documentation for the full list of fields and their meaning.</p><p>Once you have the list of clients, you can easily close the connection with a client using the <code>CLIENT KILL</code> command specifying the client address as argument.</p><p>The commands <code>CLIENT SETNAME</code> and <code>CLIENT GETNAME</code> can be used to set and get the connection name.</p>","link":"/alpha/topics/clients.html","spaLink":"#/alpha/topics/clients","title":"CLIENT COMMAND"},{"content":"<h1 id=\"redis-cluster-specification\">Redis Cluster Specification</h1><p>Welcome to the <strong>Redis Cluster Specification</strong>. Here you’ll find information\nabout algorithms and design rationales of Redis Cluster. This document is a work\nin progress as it is continuously synchronized with the actual implementation\nof Redis.</p>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"REDIS CLUSTER SPECIFICATION"},{"content":"<h1 id=\"main-properties-and-rationales-of-the-design\">Main properties and rationales of the design</h1>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"MAIN PROPERTIES AND RATIONALES OF THE DESIGN"},{"content":"<h2 id=\"main-properties-and-rationales-of-the-design-redis-cluster-goals\">Redis Cluster goals</h2><p>Redis Cluster is a distributed implementation of Redis with the following goals, in order of importance in the design:</p><ul>\n<li>High performance and linear scalability up to 1000 nodes. There are no proxies, asynchronous replication is used, and no merge operations are performed on values.</li>\n<li>Acceptable degree of write safety: the system tries (in a best-effort way) to retain all the writes originating from clients connected with the majority of the master nodes. Usually there are small windows where acknowledged writes can be lost. Windows to lose acknowledged writes are larger when clients are in a minority partition.</li>\n<li>Availability: Redis Cluster is able to survive partitions where the majority of the master nodes are reachable and there is at least one reachable slave for every master node that is no longer reachable. Moreover using <em>replicas migration</em>, masters no longer replicated by any slave will receive one from a master which is covered by multiple slaves.</li>\n</ul><p>What is described in this document is implemented in Redis 3.0 or greater.</p>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"REDIS CLUSTER GOALS"},{"content":"<h2 id=\"main-properties-and-rationales-of-the-design-implemented-subset\">Implemented subset</h2><p>Redis Cluster implements all the single key commands available in the\nnon-distributed version of Redis. Commands performing complex multi-key\noperations like Set type unions or intersections are implemented as well\nas long as the keys all belong to the same node.</p><p>Redis Cluster implements a concept called <strong>hash tags</strong> that can be used\nin order to force certain keys to be stored in the same node. However during\nmanual reshardings, multi-key operations may become unavailable for some time\nwhile single key operations are always available.</p><p>Redis Cluster does not support multiple databases like the stand alone version\nof Redis. There is just database 0 and the <code>SELECT</code> command is not allowed.</p>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"IMPLEMENTED SUBSET"},{"content":"<h2 id=\"main-properties-and-rationales-of-the-design-clients-and-servers-roles-in-the-redis-cluster-protocol\">Clients and Servers roles in the Redis Cluster protocol</h2><p>In Redis Cluster nodes are responsible for holding the data,\nand taking the state of the cluster, including mapping keys to the right nodes.\nCluster nodes are also able to auto-discover other nodes, detect non-working\nnodes, and promote slave nodes to master when needed in order\nto continue to operate when a failure occurs.</p><p>To perform their tasks all the cluster nodes are connected using a\nTCP bus and a binary protocol, called the <strong>Redis Cluster Bus</strong>.\nEvery node is connected to every other node in the cluster using the cluster\nbus. Nodes use a gossip protocol to propagate information about the cluster\nin order to discover new nodes, to send ping packets to make sure all the\nother nodes are working properly, and to send cluster messages needed to\nsignal specific conditions. The cluster bus is also used in order to\npropagate Pub/Sub messages across the cluster and to orchestrate manual\nfailovers when requested by users (manual failovers are failovers which\nare not initiated by the Redis Cluster failure detector, but by the\nsystem administrator directly).</p><p>Since cluster nodes are not able to proxy requests, clients may be redirected\nto other nodes using redirection errors <code>-MOVED</code> and <code>-ASK</code>.\nThe client is in theory free to send requests to all the nodes in the cluster,\ngetting redirected if needed, so the client is not required to hold the\nstate of the cluster. However clients that are able to cache the map between\nkeys and nodes can improve the performance in a sensible way.</p>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"CLIENTS AND SERVERS ROLES IN THE REDIS CLUSTER PROTOCOL"},{"content":"<h2 id=\"main-properties-and-rationales-of-the-design-write-safety\">Write safety</h2><p>Redis Cluster uses asynchronous replication between nodes, and <strong>last failover wins</strong> implicit merge function. This means that the last elected master dataset eventually replaces all the other replicas. There is always a window of time when it is possible to lose writes during partitions. However these windows are very different in the case of a client that is connected to the majority of masters, and a client that is connected to the minority of masters.</p><p>Redis Cluster tries harder to retain writes that are performed by clients connected to the majority of masters, compared to writes performed in the minority side.\nThe following are examples of scenarios that lead to loss of acknowledged\nwrites received in the majority partitions during failures:</p><p>A write may reach a master, but while the master may be able to reply to the client, the write may not be propagated to slaves via the asynchronous replication used between master and slave nodes. If the master dies without the write reaching the slaves, the write is lost forever if the master is unreachable for a long enough period that one of its slaves is promoted. This is usually hard to observe in the case of a total, sudden failure of a master node since masters try to reply to clients (with the acknowledge of the write) and slaves (propagating the write) at about the same time. However it is a real world failure mode.</p><p>Another theoretically possible failure mode where writes are lost is the following:</p><p>A master is unreachable because of a partition.</p><p>The second failure mode is unlikely to happen because master nodes unable to communicate with the majority of the other masters for enough time to be failed over will no longer accept writes, and when the partition is fixed writes are still refused for a small amount of time to allow other nodes to inform about configuration changes. This failure mode also requires that the client’s routing table has not yet been updated.</p><p>Writes targeting the minority side of a partition have a larger window in which to get lost. For example, Redis Cluster loses a non-trivial number of writes on partitions where there is a minority of masters and at least one or more clients, since all the writes sent to the masters may potentially get lost if the masters are failed over in the majority side.</p><p>Specifically, for a master to be failed over it must be unreachable by the majority of masters for at least <code>NODE_TIMEOUT</code>, so if the partition is fixed before that time, no writes are lost. When the partition lasts for more than <code>NODE_TIMEOUT</code>, all the writes performed in the minority side up to that point may be lost. However the minority side of a Redis Cluster will start refusing writes as soon as <code>NODE_TIMEOUT</code> time has elapsed without contact with the majority, so there is a maximum window after which the minority becomes no longer available. Hence, no writes are accepted or lost after that time.</p>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"WRITE SAFETY"},{"content":"<h2 id=\"main-properties-and-rationales-of-the-design-availability\">Availability</h2><p>Redis Cluster is not available in the minority side of the partition. In the majority side of the partition assuming that there are at least the majority of masters and a slave for every unreachable master, the cluster becomes available again after <code>NODE_TIMEOUT</code> time plus a few more seconds required for a slave to get elected and failover its master (failovers are usually executed in a matter of 1 or 2 seconds).</p><p>This means that Redis Cluster is designed to survive failures of a few nodes in the cluster, but it is not a suitable solution for applications that require availability in the event of large net splits.</p><p>In the example of a cluster composed of N master nodes where every node has a single slave, the majority side of the cluster will remain available as long as a single node is partitioned away, and will remain available with a probability of <code>1-(1/(N*2-1))</code> when two nodes are partitioned away (after the first node fails we are left with <code>N*2-1</code> nodes in total, and the probability of the only master without a replica to fail is <code>1/(N*2-1))</code>.</p><p>For example, in a cluster with 5 nodes and a single slave per node, there is a <code>1/(5*2-1) = 11.11%</code> probability that after two nodes are partitioned away from the majority, the cluster will no longer be available.</p><p>Thanks to a Redis Cluster feature called <strong>replicas migration</strong> the Cluster\navailability is improved in many real world scenarios by the fact that\nreplicas migrate to orphaned masters (masters no longer having replicas).\nSo at every successful failure event, the cluster may reconfigure the slaves\nlayout in order to better resist the next failure.</p>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"AVAILABILITY"},{"content":"<h2 id=\"main-properties-and-rationales-of-the-design-performance\">Performance</h2><p>In Redis Cluster nodes don’t proxy commands to the right node in charge for a given key, but instead they redirect clients to the right nodes serving a given portion of the key space.</p><p>Eventually clients obtain an up-to-date representation of the cluster and which node serves which subset of keys, so during normal operations clients directly contact the right nodes in order to send a given command.</p><p>Because of the use of asynchronous replication, nodes do not wait for other nodes’ acknowledgment of writes (if not explicitly requested using the <code>WAIT</code> command).</p><p>Also, because multi-key commands are only limited to <em>near</em> keys, data is never moved between nodes except when resharding.</p><p>Normal operations are handled exactly as in the case of a single Redis instance. This means that in a Redis Cluster with N master nodes you can expect the same performance as a single Redis instance multiplied by N as the design scales linearly. At the same time the query is usually performed in a single round trip, since clients usually retain persistent connections with the nodes, so latency figures are also the same as the single standalone Redis node case.</p><p>Very high performance and scalability while preserving weak but\nreasonable forms of data safety and availability is the main goal of\nRedis Cluster.</p>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"PERFORMANCE"},{"content":"<h2 id=\"main-properties-and-rationales-of-the-design-why-merge-operations-are-avoided\">Why merge operations are avoided</h2><p>Redis Cluster design avoids conflicting versions of the same key-value pair in multiple nodes as in the case of the Redis data model this is not always desirable. Values in Redis are often very large; it is common to see lists or sorted sets with millions of elements. Also data types are semantically complex. Transferring and merging these kind of values can be a major bottleneck and/or may require the non-trivial involvement of application-side logic, additional memory to store meta-data, and so forth.</p><p>There are no strict technological limits here. CRDTs or synchronously replicated\nstate machines can model complex data types similar to Redis. However, the\nactual run time behavior of such systems would not be similar to Redis Cluster.\nRedis Cluster was designed in order to cover the exact use cases of the\nnon-clustered Redis version.</p>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"WHY MERGE OPERATIONS ARE AVOIDED"},{"content":"<h1 id=\"overview-of-redis-cluster-main-components\">Overview of Redis Cluster main components</h1>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"OVERVIEW OF REDIS CLUSTER MAIN COMPONENTS"},{"content":"<h2 id=\"overview-of-redis-cluster-main-components-keys-distribution-model\">Keys distribution model</h2><p>The key space is split into 16384 slots, effectively setting an upper limit\nfor the cluster size of 16384 master nodes (however the suggested max size of\nnodes is in the order of ~ 1000 nodes).</p><p>Each master node in a cluster handles a subset of the 16384 hash slots.\nThe cluster is <strong>stable</strong> when there is no cluster reconfiguration in\nprogress (i.e. where hash slots are being moved from one node to another).\nWhen the cluster is stable, a single hash slot will be served by a single node\n(however the serving node can have one or more slaves that will replace it in the case of net splits or failures,\nand that can be used in order to scale read operations where reading stale data is acceptable).</p><p>The base algorithm used to map keys to hash slots is the following\n(read the next paragraph for the hash tag exception to this rule):</p><p>The CRC16 is specified as follows:</p><ul>\n<li>Name: XMODEM (also known as ZMODEM or CRC-16/ACORN)</li>\n<li>Width: 16 bit</li>\n<li>Poly: 1021 (That is actually x^16 + x^12 + x^5 + 1)</li>\n<li>Initialization: 0000</li>\n<li>Reflect Input byte: False</li>\n<li>Reflect Output CRC: False</li>\n<li>Xor constant to output CRC: 0000</li>\n<li>Output for “123456789”: 31C3</li>\n</ul><p>14 out of 16 CRC16 output bits are used (this is why there is\na modulo 16384 operation in the formula above).</p><p>In our tests CRC16 behaved remarkably well in distributing different kinds of\nkeys evenly across the 16384 slots.</p><p><strong>Note</strong>: A reference implementation of the CRC16 algorithm used is available in the Appendix A of this document.</p>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"KEYS DISTRIBUTION MODEL"},{"content":"<h2 id=\"overview-of-redis-cluster-main-components-keys-hash-tags\">Keys hash tags</h2><p>There is an exception for the computation of the hash slot that is used in order\nto implement <strong>hash tags</strong>. Hash tags are a way to ensure that multiple keys\nare allocated in the same hash slot. This is used in order to implement\nmulti-key operations in Redis Cluster.</p><p>In order to implement hash tags, the hash slot for a key is computed in a\nslightly different way in certain conditions.\nIf the key contains a “{…}” pattern only the substring between\n<code>{</code> and <code>}</code> is hashed in order to obtain the hash slot. However since it is\npossible that there are multiple occurrences of <code>{</code> or <code>}</code> the algorithm is\nwell specified by the following rules:</p><ul>\n<li>IF the key contains a <code>{</code> character.</li>\n<li>AND IF there is a <code>}</code> character to the right of <code>{</code></li>\n<li>AND IF there are one or more characters between the first occurrence of <code>{</code> and the first occurrence of <code>}</code>.</li>\n</ul><p>Then instead of hashing the key, only what is between the first occurrence of <code>{</code> and the following first occurrence of <code>}</code> is hashed.</p><p>Examples:</p><ul>\n<li>The two keys <code>{user1000}.following</code> and <code>{user1000}.followers</code> will hash to the same hash slot since only the substring <code>user1000</code> will be hashed in order to compute the hash slot.</li>\n<li>For the key <code>foo{}{bar}</code> the whole key will be hashed as usually since the first occurrence of <code>{</code> is followed by <code>}</code> on the right without characters in the middle.</li>\n<li>For the key <code>foo{{bar}}zap</code> the substring <code>{bar</code> will be hashed, because it is the substring between the first occurrence of <code>{</code> and the first occurrence of <code>}</code> on its right.</li>\n<li>For the key <code>foo{bar}{zap}</code> the substring <code>bar</code> will be hashed, since the algorithm stops at the first valid or invalid (without bytes inside) match of <code>{</code> and <code>}</code>.</li>\n<li>What follows from the algorithm is that if the key starts with <code>{}</code>, it is guaranteed to be hashed as a whole. This is useful when using binary data as key names.</li>\n</ul><p>Adding the hash tags exception, the following is an implementation of the <code>HASH_SLOT</code> function in Ruby and C language.</p><p>Ruby example code:</p><p>C example code:</p>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"KEYS HASH TAGS"},{"content":"<h2 id=\"overview-of-redis-cluster-main-components-cluster-nodes-attributes\">Cluster nodes attributes</h2><p>Every node has a unique name in the cluster. The node name is the\nhex representation of a 160 bit random number, obtained the first time a\nnode is started (usually using /dev/urandom).\nThe node will save its ID in the node configuration file, and will use the\nsame ID forever, or at least as long as the node configuration file is not\ndeleted by the system administrator, or a <em>hard reset</em> is requested\nvia the <code>CLUSTER RESET</code> command.</p><p>The node ID is used to identify every node across the whole cluster.\nIt is possible for a given node to change its IP address without any need\nto also change the node ID. The cluster is also able to detect the change\nin IP/port and reconfigure using the gossip protocol running over the cluster\nbus.</p><p>The node ID is not the only information associated with each node, but is\nthe only one that is always globally consistent. Every node has also the\nfollowing set of information associated. Some information is about the\ncluster configuration detail of this specific node, and is eventually\nconsistent across the cluster. Some other information, like the last time\na node was pinged, is instead local to each node.</p><p>Every node maintains the following information about other nodes that it is\naware of in the cluster: The node ID, IP and port of the node, a set of\nflags, what is the master of the node if it is flagged as <code>slave</code>, last time\nthe node was pinged and the last time the pong was received, the current\n<em>configuration epoch</em> of the node (explained later in this specification),\nthe link state and finally the set of hash slots served.</p><p>A detailed <a href=\"http://redis.io/commands/cluster-nodes\">explanation of all the node fields</a> is described in the <code>CLUSTER NODES</code> documentation.</p><p>The <code>CLUSTER NODES</code> command can be sent to any node in the cluster and provides the state of the cluster and the information for each node according to the local view the queried node has of the cluster.</p><p>The following is sample output of the <code>CLUSTER NODES</code> command sent to a master\nnode in a small cluster of three nodes.</p><p>In the above listing the different fields are in order: node id, address:port, flags, last ping sent, last pong received, configuration epoch, link state, slots. Details about the above fields will be covered as soon as we talk of specific parts of Redis Cluster.</p>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"CLUSTER NODES ATTRIBUTES"},{"content":"<h2 id=\"overview-of-redis-cluster-main-components-the-cluster-bus\">The Cluster bus</h2><p>Every Redis Cluster node has an additional TCP port for receiving\nincoming connections from other Redis Cluster nodes. This port is at a fixed\noffset from the normal TCP port used to receive incoming connections\nfrom clients. To obtain the Redis Cluster port, 10000 should be added to\nthe normal commands port. For example, if a Redis node is listening for\nclient connections on port 6379, the Cluster bus port 16379 will also be\nopened.</p><p>Node-to-node communication happens exclusively using the Cluster bus and\nthe Cluster bus protocol: a binary protocol composed of frames\nof different types and sizes. The Cluster bus binary protocol is not\npublicly documented since it is not intended for external software devices\nto talk with Redis Cluster nodes using this protocol. However you can\nobtain more details about the Cluster bus protocol by reading the\n<code>cluster.h</code> and <code>cluster.c</code> files in the Redis Cluster source code.</p>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"THE CLUSTER BUS"},{"content":"<h2 id=\"overview-of-redis-cluster-main-components-cluster-topology\">Cluster topology</h2><p>Redis Cluster is a full mesh where every node is connected with every other node using a TCP connection.</p><p>In a cluster of N nodes, every node has N-1 outgoing TCP connections, and N-1 incoming connections.</p><p>These TCP connections are kept alive all the time and are not created on demand.\nWhen a node expects a pong reply in response to a ping in the cluster bus, before waiting long enough to mark the node as unreachable, it will try to\nrefresh the connection with the node by reconnecting from scratch.</p><p>While Redis Cluster nodes form a full mesh, <strong>nodes use a gossip protocol and\na configuration update mechanism in order to avoid exchanging too many\nmessages between nodes during normal conditions</strong>, so the number of messages\nexchanged is not exponential.</p>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"CLUSTER TOPOLOGY"},{"content":"<h2 id=\"overview-of-redis-cluster-main-components-nodes-handshake\">Nodes handshake</h2><p>Nodes always accept connections on the cluster bus port, and even reply to\npings when received, even if the pinging node is not trusted.\nHowever, all other packets will be discarded by the receiving node if the\nsending node is not considered part of the cluster.</p><p>A node will accept another node as part of the cluster only in two ways:</p><ul>\n<li><p>If a node presents itself with a <code>MEET</code> message. A meet message is exactly\nlike a <code>PING</code> message, but forces the receiver to accept the node as part of\nthe cluster. Nodes will send <code>MEET</code> messages to other nodes <strong>only if</strong> the system administrator requests this via the following command:</p>\n<p>  CLUSTER MEET ip port</p>\n</li>\n<li><p>A node will also register another node as part of the cluster if a node that is already trusted will gossip about this other node. So if A knows B, and B knows C, eventually B will send gossip messages to A about C. When this happens, A will register C as part of the network, and will try to connect with C.</p>\n</li>\n</ul><p>If a node presents itself with a <code>MEET</code> message. A meet message is exactly\nlike a <code>PING</code> message, but forces the receiver to accept the node as part of\nthe cluster. Nodes will send <code>MEET</code> messages to other nodes <strong>only if</strong> the system administrator requests this via the following command:</p><p>  CLUSTER MEET ip port</p><p>A node will also register another node as part of the cluster if a node that is already trusted will gossip about this other node. So if A knows B, and B knows C, eventually B will send gossip messages to A about C. When this happens, A will register C as part of the network, and will try to connect with C.</p><p>This means that as long as we join nodes in any connected graph, they’ll eventually form a fully connected graph automatically. This means that the cluster is able to auto-discover other nodes, but only if there is a trusted relationship that was forced by the system administrator.</p><p>This mechanism makes the cluster more robust but prevents different Redis clusters from accidentally mixing after change of IP addresses or other network related events.</p>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"NODES HANDSHAKE"},{"content":"<h1 id=\"redirection-and-resharding\">Redirection and resharding</h1>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"REDIRECTION AND RESHARDING"},{"content":"<h2 id=\"redirection-and-resharding-moved-redirection\">MOVED Redirection</h2><p>A Redis client is free to send queries to every node in the cluster, including\nslave nodes. The node will analyze the query, and if it is acceptable\n(that is, only a single key is mentioned in the query, or the multiple keys\nmentioned are all to the same hash slot) it will lookup what\nnode is responsible for the hash slot where the key or keys belong.</p><p>If the hash slot is served by the node, the query is simply processed, otherwise\nthe node will check its internal hash slot to node map, and will reply\nto the client with a MOVED error, like in the following example:</p><p>The error includes the hash slot of the key (3999) and the ip:port of the\ninstance that can serve the query. The client needs to reissue the query\nto the specified node’s IP address and port.\nNote that even if the client waits a long time before reissuing the query,\nand in the meantime the cluster configuration changed, the destination node\nwill reply again with a MOVED error if the hash slot 3999 is now served by\nanother node. The same happens if the contacted node had no updated information.</p><p>So while from the point of view of the cluster nodes are identified by\nIDs we try to simplify our interface with the client just exposing a map\nbetween hash slots and Redis nodes identified by IP:port pairs.</p><p>The client is not required to, but should try to memorize that hash slot\n3999 is served by 127.0.0.1:6381. This way once a new command needs to\nbe issued it can compute the hash slot of the target key and have a\ngreater chance of choosing the right node.</p><p>An alternative is to just refresh the whole client-side cluster layout\nusing the <code>CLUSTER NODES</code> or <code>CLUSTER SLOTS</code> commands\nwhen a MOVED redirection is received. When a redirection is encountered, it\nis likely multiple slots were reconfigured rather than just one, so updating\nthe client configuration as soon as possible is often the best strategy.</p><p>Note that when the Cluster is stable (no ongoing changes in the configuration),\neventually all the clients will obtain a map of hash slots -&gt; nodes, making\nthe cluster efficient, with clients directly addressing the right nodes\nwithout redirections, proxies or other single point of failure entities.</p><p>A client <strong>must be also able to handle -ASK redirections</strong> that are described\nlater in this document, otherwise it is not a complete Redis Cluster client.</p>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"MOVED REDIRECTION"},{"content":"<h2 id=\"redirection-and-resharding-cluster-live-reconfiguration\">Cluster live reconfiguration</h2><p>Redis Cluster supports the ability to add and remove nodes while the cluster\nis running. Adding or removing a node is abstracted into the same\noperation: moving a hash slot from one node to another. This means\nthat the same basic mechanism can be used in order to rebalance the cluster, add\nor remove nodes, and so forth.</p><ul>\n<li>To add a new node to the cluster an empty node is added to the cluster and some set of hash slots are moved from existing nodes to the new node.</li>\n<li>To remove a node from the cluster the hash slots assigned to that node are moved to other existing nodes.</li>\n<li>To rebalance the cluster a given set of hash slots are moved between nodes.</li>\n</ul><p>The core of the implementation is the ability to move hash slots around.\nFrom a practical point of view a hash slot is just a set of keys, so\nwhat Redis Cluster really does during <em>resharding</em> is to move keys from\nan instance to another instance. Moving a hash slot means moving all the keys\nthat happen to hash into this hash slot.</p><p>To understand how this works we need to show the <code>CLUSTER</code> subcommands\nthat are used to manipulate the slots translation table in a Redis Cluster node.</p><p>The following subcommands are available (among others not useful in this case):</p><ul>\n<li><code>CLUSTER ADDSLOTS</code> slot1 [slot2] … [slotN]</li>\n<li><code>CLUSTER DELSLOTS</code> slot1 [slot2] … [slotN]</li>\n<li><code>CLUSTER SETSLOT</code> slot NODE node</li>\n<li><code>CLUSTER SETSLOT</code> slot MIGRATING node</li>\n<li><code>CLUSTER SETSLOT</code> slot IMPORTING node</li>\n</ul><p>The first two commands, <code>ADDSLOTS</code> and <code>DELSLOTS</code>, are simply used to assign\n(or remove) slots to a Redis node. Assigning a slot means to tell a given\nmaster node that it will be in charge of storing and serving content for\nthe specified hash slot.</p><p>After the hash slots are assigned they will propagate across the cluster\nusing the gossip protocol, as specified later in the\n<em>configuration propagation</em> section.</p><p>The <code>ADDSLOTS</code> command is usually used when a new cluster is created\nfrom scratch to assign each master node a subset of all the 16384 hash\nslots available.</p><p>The <code>DELSLOTS</code> is mainly used for manual modification of a cluster configuration\nor for debugging tasks: in practice it is rarely used.</p><p>The <code>SETSLOT</code> subcommand is used to assign a slot to a specific node ID if\nthe <code>SETSLOT &lt;slot&gt; NODE</code> form is used. Otherwise the slot can be set in the\ntwo special states <code>MIGRATING</code> and <code>IMPORTING</code>. Those two special states\nare used in order to migrate a hash slot from one node to another.</p><ul>\n<li>When a slot is set as MIGRATING, the node will accept all queries that\nare about this hash slot, but only if the key in question\nexists, otherwise the query is forwarded using a <code>-ASK</code> redirection to the\nnode that is target of the migration.</li>\n<li>When a slot is set as IMPORTING, the node will accept all queries that\nare about this hash slot, but only if the request is\npreceded by an <code>ASKING</code> command. If the <code>ASKING</code> command was not given\nby the client, the query is redirected to the real hash slot owner via\na <code>-MOVED</code> redirection error, as would happen normally.</li>\n</ul><p>Let’s make this clearer with an example of hash slot migration.\nAssume that we have two Redis master nodes, called A and B.\nWe want to move hash slot 8 from A to B, so we issue commands like this:</p><ul>\n<li>We send B: CLUSTER SETSLOT 8 IMPORTING A</li>\n<li>We send A: CLUSTER SETSLOT 8 MIGRATING B</li>\n</ul><p>All the other nodes will continue to point clients to node “A” every time\nthey are queried with a key that belongs to hash slot 8, so what happens\nis that:</p><ul>\n<li>All queries about existing keys are processed by “A”.</li>\n<li>All queries about non-existing keys in A are processed by “B”, because “A” will redirect clients to “B”.</li>\n</ul><p>This way we no longer create new keys in “A”.\nIn the meantime, a special program called <code>redis-trib</code> used during reshardings\nand Redis Cluster configuration will migrate existing keys in\nhash slot 8 from A to B.\nThis is performed using the following command:</p><p>The above command will return <code>count</code> keys in the specified hash slot.\nFor every key returned, <code>redis-trib</code> sends node “A” a <code>MIGRATE</code> command, that\nwill migrate the specified key from A to B in an atomic way (both instances\nare locked for the time (usually very small time) needed to migrate a key so\nthere are no race conditions). This is how <code>MIGRATE</code> works:</p><p><code>MIGRATE</code> will connect to the target instance, send a serialized version of\nthe key, and once an OK code is received will delete the old key from its own\ndataset. From the point of view of an external client a key exists either\nin A or B at any given time.</p><p>In Redis Cluster there is no need to specify a database other than 0, but\n<code>MIGRATE</code> is a general command that can be used for other tasks not\ninvolving Redis Cluster.\n<code>MIGRATE</code> is optimized to be as fast as possible even when moving complex\nkeys such as long lists, but in Redis Cluster reconfiguring the\ncluster where big keys are present is not considered a wise procedure if\nthere are latency constraints in the application using the database.</p><p>When the migration process is finally finished, the <code>SETSLOT &lt;slot&gt; NODE &lt;node-id&gt;</code> command is sent to the two nodes involved in the migration in order to\nset the slots to their normal state again. The same command is usually\nsent to all other nodes to avoid waiting for the natural\npropagation of the new configuration across the cluster.</p>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"CLUSTER LIVE RECONFIGURATION"},{"content":"<h2 id=\"redirection-and-resharding-ask-redirection\">ASK redirection</h2><p>In the previous section we briefly talked about ASK redirection. Why can’t\nwe simply use MOVED redirection? Because while MOVED means that\nwe think the hash slot is permanently served by a different node and the\nnext queries should be tried against the specified node, ASK means to\nsend only the next query to the specified node.</p><p>This is needed because the next query about hash slot 8 can be about a\nkey that is still in A, so we always want the client to try A and\nthen B if needed. Since this happens only for one hash slot out of 16384\navailable, the performance hit on the cluster is acceptable.</p><p>We need to force that client behavior, so to make sure\nthat clients will only try node B after A was tried, node B will only\naccept queries of a slot that is set as IMPORTING if the client sends the\nASKING command before sending the query.</p><p>Basically the ASKING command sets a one-time flag on the client that forces\na node to serve a query about an IMPORTING slot.</p><p>The full semantics of ASK redirection from the point of view of the client is as follows:</p><ul>\n<li>If ASK redirection is received, send only the query that was redirected to the specified node but continue sending subsequent queries to the old node.</li>\n<li>Start the redirected query with the ASKING command.</li>\n<li>Don’t yet update local client tables to map hash slot 8 to B.</li>\n</ul><p>Once hash slot 8 migration is completed, A will send a MOVED message and\nthe client may permanently map hash slot 8 to the new IP and port pair.\nNote that if a buggy client performs the map earlier this is not\na problem since it will not send the ASKING command before issuing the query,\nso B will redirect the client to A using a MOVED redirection error.</p><p>Slots migration is explained in similar terms but with different wording\n(for the sake of redundancy in the documentation) in the <code>CLUSTER SETSLOT</code>\ncommand documentation.</p>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"ASK REDIRECTION"},{"content":"<h2 id=\"redirection-and-resharding-clients-first-connection-and-handling-of-redirections\">Clients first connection and handling of redirections</h2><p>While it is possible to have a Redis Cluster client implementation that does not\nremember the slots configuration (the map between slot numbers and addresses of\nnodes serving it) in memory and only works by contacting random nodes waiting to\nbe redirected, such a client would be very inefficient.</p><p>Redis Cluster clients should try to be smart enough to memorize the slots\nconfiguration. However this configuration is not <em>required</em> to be up to date.\nSince contacting the wrong node will simply result in a redirection, that\nshould trigger an update of the client view.</p><p>Clients usually need to fetch a complete list of slots and mapped node\naddresses in two different situations:</p><ul>\n<li>At startup in order to populate the initial slots configuration.</li>\n<li>When a <code>MOVED</code> redirection is received.</li>\n</ul><p>Note that a client may handle the <code>MOVED</code> redirection by updating just the\nmoved slot in its table, however this is usually not efficient since often\nthe configuration of multiple slots is modified at once (for example if a\nslave is promoted to master, all the slots served by the old master will\nbe remapped). It is much simpler to react to a <code>MOVED</code> redirection by\nfetching the full map of slots to nodes from scratch.</p><p>In order to retrieve the slots configuration Redis Cluster offers\nan alternative to the <code>CLUSTER NODES</code> command that does not\nrequire parsing, and only provides the information strictly needed to clients.</p><p>The new command is called <code>CLUSTER SLOTS</code> and provides an array of slots\nranges, and the associated master and slave nodes serving the specified range.</p><p>The following is an example of output of <code>CLUSTER SLOTS</code>:</p><p>The first two sub-elements of every element of the returned array are the\nstart-end slots of the range. The additional elements represent address-port\npairs. The first address-port pair is the master serving the slot, and the\nadditional address-port pairs are all the slaves serving the same slot\nthat are not in an error condition (i.e. the FAIL flag is not set).</p><p>For example the first element of the output says that slots from 5461 to 10922\n(start and end included) are served by 127.0.0.1:7001, and it is possible\nto scale read-only load contacting the slave at 127.0.0.1:7004.</p><p><code>CLUSTER SLOTS</code> is not guaranteed to return ranges that cover the full\n16384 slots if the cluster is misconfigured, so clients should initialize the\nslots configuration map filling the target nodes with NULL objects, and\nreport an error if the user tries to execute commands about keys\nthat belong to unassigned slots.</p><p>Before returning an error to the caller when a slot is found to\nbe unassigned, the client should try to fetch the slots configuration\nagain to check if the cluster is now configured properly.</p>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"CLIENTS FIRST CONNECTION AND HANDLING OF REDIRECTIONS"},{"content":"<h2 id=\"redirection-and-resharding-multiple-keys-operations\">Multiple keys operations</h2><p>Using hash tags, clients are free to use multi-key operations.\nFor example the following operation is valid:</p><p>Multi-key operations may become unavailable when a resharding of the\nhash slot the keys belong to is in progress.</p><p>More specifically, even during a resharding the multi-key operations\ntargeting keys that all exist and are all still in the same node (either\nthe source or destination node) are still available.</p><p>Operations on keys that don’t exist or are - during the resharding - split\nbetween the source and destination nodes, will generate a <code>-TRYAGAIN</code> error.\nThe client can try the operation after some time, or report back the error.</p><p>As soon as migration of the specified hash slot has terminated, all\nmulti-key operations are available again for that hash slot.</p>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"MULTIPLE KEYS OPERATIONS"},{"content":"<h2 id=\"redirection-and-resharding-scaling-reads-using-slave-nodes\">Scaling reads using slave nodes</h2><p>Normally slave nodes will redirect clients to the authoritative master for\nthe hash slot involved in a given command, however clients can use slaves\nin order to scale reads using the <code>READONLY</code> command.</p><p><code>READONLY</code> tells a Redis Cluster slave node that the client is ok reading\npossibly stale data and is not interested in running write queries.</p><p>When the connection is in readonly mode, the cluster will send a redirection\nto the client only if the operation involves keys not served\nby the slave’s master node. This may happen because:</p><p>When this happens the client should update its hashslot map as explained in\nthe previous sections.</p><p>The readonly state of the connection can be cleared using the <code>READWRITE</code> command.</p>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"SCALING READS USING SLAVE NODES"},{"content":"<h1 id=\"fault-tolerance\">Fault Tolerance</h1>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"FAULT TOLERANCE"},{"content":"<h2 id=\"fault-tolerance-heartbeat-and-gossip-messages\">Heartbeat and gossip messages</h2><p>Redis Cluster nodes continuously exchange ping and pong packets. Those two kind of packets have the same structure, and both carry important configuration information. The only actual difference is the message type field. We’ll refer to the sum of ping and pong packets as <em>heartbeat packets</em>.</p><p>Usually nodes send ping packets that will trigger the receivers to reply with pong packets. However this is not necessarily true. It is possible for nodes to just send pong packets to send information to other nodes about their configuration, without triggering a reply. This is useful, for example, in order to broadcast a new configuration as soon as possible.</p><p>Usually a node will ping a few random nodes every second so that the total number of ping packets sent (and pong packets received) by each node is a constant amount regardless of the number of nodes in the cluster.</p><p>However every node makes sure to ping every other node that hasn’t sent a ping or received a pong for longer than half the <code>NODE_TIMEOUT</code> time. Before <code>NODE_TIMEOUT</code> has elapsed, nodes also try to reconnect the TCP link with another node to make sure nodes are not believed to be unreachable only because there is a problem in the current TCP connection.</p><p>The number of messages globally exchanged can be sizable if <code>NODE_TIMEOUT</code> is set to a small figure and the number of nodes (N) is very large, since every node will try to ping every other node for which they don’t have fresh information every half the <code>NODE_TIMEOUT</code> time.</p><p>For example in a 100 node cluster with a node timeout set to 60 seconds, every node will try to send 99 pings every 30 seconds, with a total amount of pings of 3.3 per second. Multiplied by 100 nodes, this is 330 pings per second in the total cluster.</p><p>There are ways to lower the number of messages, however there have been no\nreported issues with the bandwidth currently used by Redis Cluster failure\ndetection, so for now the obvious and direct design is used. Note that even\nin the above example, the 330 packets per second exchanged are evenly\ndivided among 100 different nodes, so the traffic each node receives\nis acceptable.</p>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"HEARTBEAT AND GOSSIP MESSAGES"},{"content":"<h2 id=\"fault-tolerance-heartbeat-packet-content\">Heartbeat packet content</h2><p>Ping and pong packets contain a header that is common to all types of packets (for instance packets to request a failover vote), and a special Gossip Section that is specific of Ping and Pong packets.</p><p>The common header has the following information:</p><ul>\n<li>Node ID, a 160 bit pseudorandom string that is assigned the first time a node is created and remains the same for all the life of a Redis Cluster node.</li>\n<li>The <code>currentEpoch</code> and <code>configEpoch</code> fields of the sending node that are used to mount the distributed algorithms used by Redis Cluster (this is explained in detail in the next sections). If the node is a slave the <code>configEpoch</code> is the last known <code>configEpoch</code> of its master.</li>\n<li>The node flags, indicating if the node is a slave, a master, and other single-bit node information.</li>\n<li>A bitmap of the hash slots served by the sending node, or if the node is a slave, a bitmap of the slots served by its master.</li>\n<li>The sender TCP base port (that is, the port used by Redis to accept client commands; add 10000 to this to obtain the cluster bus port).</li>\n<li>The state of the cluster from the point of view of the sender (down or ok).</li>\n<li>The master node ID of the sending node, if it is a slave.</li>\n</ul><p>Ping and pong packets also contain a gossip section. This section offers to the receiver a view of what the sender node thinks about other nodes in the cluster. The gossip section only contains information about a few random nodes among the set of nodes known to the sender. The number of nodes mentioned in a gossip section is proportional to the cluster size.</p><p>For every node added in the gossip section the following fields are reported:</p><ul>\n<li>Node ID.</li>\n<li>IP and port of the node.</li>\n<li>Node flags.</li>\n</ul><p>Gossip sections allow receiving nodes to get information about the state of other nodes from the point of view of the sender. This is useful both for failure detection and to discover other nodes in the cluster.</p>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"HEARTBEAT PACKET CONTENT"},{"content":"<h2 id=\"fault-tolerance-failure-detection\">Failure detection</h2><p>Redis Cluster failure detection is used to recognize when a master or slave node is no longer reachable by the majority of nodes and then respond by promoting a slave to the role of master. When slave promotion is not possible the cluster is put in an error state to stop receiving queries from clients.</p><p>As already mentioned, every node takes a list of flags associated with other known nodes. There are two flags that are used for failure detection that are called <code>PFAIL</code> and <code>FAIL</code>. <code>PFAIL</code> means <em>Possible failure</em>, and is a non-acknowledged failure type. <code>FAIL</code> means that a node is failing and that this condition was confirmed by a majority of masters within a fixed amount of time.</p><p><strong>PFAIL flag:</strong></p><p>A node flags another node with the <code>PFAIL</code> flag when the node is not reachable for more than <code>NODE_TIMEOUT</code> time. Both master and slave nodes can flag another node as <code>PFAIL</code>, regardless of its type.</p><p>The concept of non-reachability for a Redis Cluster node is that we have an <strong>active ping</strong> (a ping that we sent for which we have yet to get a reply) pending for longer than <code>NODE_TIMEOUT</code>. For this mechanism to work the <code>NODE_TIMEOUT</code> must be large compared to the network round trip time. In order to add reliability during normal operations, nodes will try to reconnect with other nodes in the cluster as soon as half of the <code>NODE_TIMEOUT</code> has elapsed without a reply to a ping. This mechanism ensures that connections are kept alive so broken connections usually won’t result in false failure reports between nodes.</p><p><strong>FAIL flag:</strong></p><p>The <code>PFAIL</code> flag alone is just local information every node has about other nodes, but it is not sufficient to trigger a slave promotion. For a node to be considered down the <code>PFAIL</code> condition needs to be escalated to a <code>FAIL</code> condition.</p><p>As outlined in the node heartbeats section of this document, every node sends gossip messages to every other node including the state of a few random known nodes. Every node eventually receives a set of node flags for every other node. This way every node has a mechanism to signal other nodes about failure conditions they have detected.</p><p>A <code>PFAIL</code> condition is escalated to a <code>FAIL</code> condition when the following set of conditions are met:</p><ul>\n<li>Some node, that we’ll call A, has another node B flagged as <code>PFAIL</code>.</li>\n<li>Node A collected, via gossip sections, information about the state of B from the point of view of the majority of masters in the cluster.</li>\n<li>The majority of masters signaled the <code>PFAIL</code> or <code>PFAIL</code> condition within <code>NODE_TIMEOUT * FAIL_REPORT_VALIDITY_MULT</code> time. (The validity factor is set to 2 in the current implementation, so this is just two times the <code>NODE_TIMEOUT</code> time).</li>\n</ul><p>If all the above conditions are true, Node A will:</p><ul>\n<li>Mark the node as <code>FAIL</code>.</li>\n<li>Send a <code>FAIL</code> message to all the reachable nodes.</li>\n</ul><p>The <code>FAIL</code> message will force every receiving node to mark the node in <code>FAIL</code> state, whether or not it already flagged the node in <code>PFAIL</code> state.</p><p>Note that <em>the FAIL flag is mostly one way</em>. That is, a node can go from <code>PFAIL</code> to <code>FAIL</code>, but a <code>FAIL</code> flag can only be cleared in the following situations:</p><ul>\n<li>The node is already reachable and is a slave. In this case the <code>FAIL</code> flag can be cleared as slaves are not failed over.</li>\n<li>The node is already reachable and is a master not serving any slot. In this case the <code>FAIL</code> flag can be cleared as masters without slots do not really participate in the cluster and are waiting to be configured in order to join the cluster.</li>\n<li>The node is already reachable and is a master, but a long time (N times the <code>NODE_TIMEOUT</code>) has elapsed without any detectable slave promotion. It’s better for it to rejoin the cluster and continue in this case.</li>\n</ul><p>It is useful to note that while the <code>PFAIL</code> -&gt; <code>FAIL</code> transition uses a form of agreement, the agreement used is weak:</p><p>However the Redis Cluster failure detection has a liveness requirement: eventually all the nodes should agree about the state of a given node. There are two cases that can originate from split brain conditions. Either some minority of nodes believe the node is in <code>FAIL</code> state, or a minority of nodes believe the node is not in <code>FAIL</code> state. In both the cases eventually the cluster will have a single view of the state of a given node:</p><p><strong>Case 1</strong>: If a majority of masters have flagged a node as <code>FAIL</code>, because of failure detection and the <em>chain effect</em> it generates, every other node will eventually flag the master as <code>FAIL</code>, since in the specified window of time enough failures will be reported.</p><p><strong>Case 2</strong>: When only a minority of masters have flagged a node as <code>FAIL</code>, the slave promotion will not happen (as it uses a more formal algorithm that makes sure everybody knows about the promotion eventually) and every node will clear the <code>FAIL</code> state as per the <code>FAIL</code> state clearing rules above (i.e. no promotion after N times the <code>NODE_TIMEOUT</code> has elapsed).</p><p><strong>The <code>FAIL</code> flag is only used as a trigger to run the safe part of the algorithm</strong> for the slave promotion. In theory a slave may act independently and start a slave promotion when its master is not reachable, and wait for the masters to refuse to provide the acknowledgment if the master is actually reachable by the majority. However the added complexity of the <code>PFAIL -&gt; FAIL</code> state, the weak agreement, and the <code>FAIL</code> message forcing the propagation of the state in the shortest amount of time in the reachable part of the cluster, have practical advantages. Because of these mechanisms, usually all the nodes will stop accepting writes at about the same time if the cluster is in an error state. This is a desirable feature from the point of view of applications using Redis Cluster. Also erroneous election attempts initiated by slaves that can’t reach its master due to local problems (the master is otherwise reachable by the majority of other master nodes) are avoided.</p>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"FAILURE DETECTION"},{"content":"<h1 id=\"configuration-handling-propagation-and-failovers\">Configuration handling, propagation, and failovers</h1>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"CONFIGURATION HANDLING, PROPAGATION, AND FAILOVERS"},{"content":"<h2 id=\"configuration-handling-propagation-and-failovers-cluster-current-epoch\">Cluster current epoch</h2><p>Redis Cluster uses a concept similar to the Raft algorithm “term”. In Redis Cluster the term is called epoch instead, and it is used in order to give incremental versioning to events. When multiple nodes provide conflicting information, it becomes possible for another node to understand which state is the most up to date.</p><p>The <code>currentEpoch</code> is a 64 bit unsigned number.</p><p>At node creation every Redis Cluster node, both slaves and master nodes, set the <code>currentEpoch</code> to 0.</p><p>Every time a packet is received from another node, if the epoch of the sender (part of the cluster bus messages header) is greater than the local node epoch, the <code>currentEpoch</code> is updated to the sender epoch.</p><p>Because of these semantics, eventually all the nodes will agree to the greatest <code>configEpoch</code> in the cluster.</p><p>This information is used when the state of the cluster is changed and a node seeks agreement in order to perform some action.</p><p>Currently this happens only during slave promotion, as described in the next section. Basically the epoch is a logical clock for the cluster and dictates that given information wins over one with a smaller epoch.</p>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"CLUSTER CURRENT EPOCH"},{"content":"<h2 id=\"configuration-handling-propagation-and-failovers-configuration-epoch\">Configuration epoch</h2><p>Every master always advertises its <code>configEpoch</code> in ping and pong packets along with a bitmap advertising the set of slots it serves.</p><p>The <code>configEpoch</code> is set to zero in masters when a new node is created.</p><p>A new <code>configEpoch</code> is created during slave election. Slaves trying to replace\nfailing masters increment their epoch and try to get authorization from\na majority of masters. When a slave is authorized, a new unique <code>configEpoch</code>\nis created and the slave turns into a master using the new <code>configEpoch</code>.</p><p>As explained in the next sections the <code>configEpoch</code> helps to resolve conflicts when different nodes claim divergent configurations (a condition that may happen because of network partitions and node failures).</p><p>Slave nodes also advertise the <code>configEpoch</code> field in ping and pong packets, but in the case of slaves the field represents the <code>configEpoch</code> of its master as of the last time they exchanged packets. This allows other instances to detect when a slave has an old configuration that needs to be updated (master nodes will not grant votes to slaves with an old configuration).</p><p>Every time the <code>configEpoch</code> changes for some known node, it is permanently stored in the nodes.conf file by all the nodes that receive this information. The same also happens for the <code>currentEpoch</code> value. These two variables are guaranteed to be saved and <code>fsync-ed</code> to disk when updated before a node continues its operations.</p><p>The <code>configEpoch</code> values generated using a simple algorithm during failovers\nare guaranteed to be new, incremental, and unique.</p>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"CONFIGURATION EPOCH"},{"content":"<h2 id=\"configuration-handling-propagation-and-failovers-slave-election-and-promotion\">Slave election and promotion</h2><p>Slave election and promotion is handled by slave nodes, with the help of master nodes that vote for the slave to promote.\nA slave election happens when a master is in <code>FAIL</code> state from the point of view of at least one of its slaves that has the prerequisites in order to become a master.</p><p>In order for a slave to promote itself to master, it needs to start an election and win it. All the slaves for a given master can start an election if the master is in <code>FAIL</code> state, however only one slave will win the election and promote itself to master.</p><p>A slave starts an election when the following conditions are met:</p><ul>\n<li>The slave’s master is in <code>FAIL</code> state.</li>\n<li>The master was serving a non-zero number of slots.</li>\n<li>The slave replication link was disconnected from the master for no longer than a given amount of time, in order to ensure the promoted slave’s data is reasonably fresh. This time is user configurable.</li>\n</ul><p>In order to be elected, the first step for a slave is to increment its <code>currentEpoch</code> counter, and request votes from master instances.</p><p>Votes are requested by the slave by broadcasting a <code>FAILOVER_AUTH_REQUEST</code> packet to every master node of the cluster. Then it waits for a maximum time of two times the <code>NODE_TIMEOUT</code> for replies to arrive (but always for at least 2 seconds).</p><p>Once a master has voted for a given slave, replying positively with a <code>FAILOVER_AUTH_ACK</code>, it can no longer vote for another slave of the same master for a period of <code>NODE_TIMEOUT * 2</code>. In this period it will not be able to reply to other authorization requests for the same master. This is not needed to guarantee safety, but useful for preventing multiple slaves from getting elected (even if with a different <code>configEpoch</code>) at around the same time, which is usually not wanted.</p><p>A slave discards any <code>AUTH_ACK</code> replies with an epoch that is less than the <code>currentEpoch</code> at the time the vote request was sent. This ensures it doesn’t count votes intended for a previous election.</p><p>Once the slave receives ACKs from the majority of masters, it wins the election.\nOtherwise if the majority is not reached within the period of two times <code>NODE_TIMEOUT</code> (but always at least 2 seconds), the election is aborted and a new one will be tried again after <code>NODE_TIMEOUT * 4</code> (and always at least 4 seconds).</p>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"SLAVE ELECTION AND PROMOTION"},{"content":"<h2 id=\"configuration-handling-propagation-and-failovers-slave-rank\">Slave rank</h2><p>As soon as a master is in <code>FAIL</code> state, a slave waits a short period of time before trying to get elected. That delay is computed as follows:</p><p>The fixed delay ensures that we wait for the <code>FAIL</code> state to propagate across the cluster, otherwise the slave may try to get elected while the masters are still unaware of the <code>FAIL</code> state, refusing to grant their vote.</p><p>The random delay is used to desynchronize slaves so they’re unlikely to start an election at the same time.</p><p>The <code>SLAVE_RANK</code> is the rank of this slave regarding the amount of replication data it has processed from the master.\nSlaves exchange messages when the master is failing in order to establish a (best effort) rank:\nthe slave with the most updated replication offset is at rank 0, the second most updated at rank 1, and so forth.\nIn this way the most updated slaves try to get elected before others.</p><p>Rank order is not strictly enforced; if a slave of higher rank fails to be\nelected, the others will try shortly.</p><p>Once a slave wins the election, it obtains a new unique and incremental <code>configEpoch</code> which is higher than that of any other existing master. It starts advertising itself as master in ping and pong packets, providing the set of served slots with a <code>configEpoch</code> that will win over the past ones.</p><p>In order to speedup the reconfiguration of other nodes, a pong packet is broadcast to all the nodes of the cluster. Currently unreachable nodes will eventually be reconfigured when they receive a ping or pong packet from another node or will receive an <code>UPDATE</code> packet from another node if the information it publishes via heartbeat packets are detected to be out of date.</p><p>The other nodes will detect that there is a new master serving the same slots served by the old master but with a greater <code>configEpoch</code>, and will upgrade their configuration. Slaves of the old master (or the failed over master if it rejoins the cluster) will not just upgrade the configuration but will also reconfigure to replicate from the new master. How nodes rejoining the cluster are configured is explained in the next sections.</p>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"SLAVE RANK"},{"content":"<h2 id=\"configuration-handling-propagation-and-failovers-masters-reply-to-slave-vote-request\">Masters reply to slave vote request</h2><p>In the previous section it was discussed how slaves try to get elected. This section explains what happens from the point of view of a master that is requested to vote for a given slave.</p><p>Masters receive requests for votes in form of <code>FAILOVER_AUTH_REQUEST</code> requests from slaves.</p><p>For a vote to be granted the following conditions need to be met:</p><p>Example of the issue caused by not using rule number 3:</p><p>Master <code>currentEpoch</code> is 5, lastVoteEpoch is 1 (this may happen after a few failed elections)</p><ul>\n<li>Slave <code>currentEpoch</code> is 3.</li>\n<li>Slave tries to be elected with epoch 4 (3+1), master replies with an ok with <code>currentEpoch</code> 5, however the reply is delayed.</li>\n<li><p>Slave will try to be elected again, at a later time, with epoch 5 (4+1), the delayed reply reaches the slave with <code>currentEpoch</code> 5, and is accepted as valid.</p>\n</li>\n<li><p>Masters don’t vote for a slave of the same master before <code>NODE_TIMEOUT * 2</code> has elapsed if a slave of that master was already voted for. This is not strictly required as it is not possible for two slaves to win the election in the same epoch. However, in practical terms it ensures that when a slave is elected it has plenty of time to inform the other slaves and avoid the possibility that another slave will win a new election, performing an unnecessary second failover.</p>\n</li>\n<li>Masters make no effort to select the best slave in any way. If the slave’s master is in <code>FAIL</code> state and the master did not vote in the current term, a positive vote is granted. The best slave is the most likely to start an election and win it before the other slaves, since it will usually be able to start the voting process earlier because of its <em>higher rank</em> as explained in the previous section.</li>\n<li>When a master refuses to vote for a given slave there is no negative response, the request is simply ignored.</li>\n<li>Masters don’t vote for slaves sending a <code>configEpoch</code> that is less than any <code>configEpoch</code> in the master table for the slots claimed by the slave. Remember that the slave sends the <code>configEpoch</code> of its master, and the bitmap of the slots served by its master. This means that the slave requesting the vote must have a configuration for the slots it wants to failover that is newer or equal the one of the master granting the vote.</li>\n</ul><p>Slave will try to be elected again, at a later time, with epoch 5 (4+1), the delayed reply reaches the slave with <code>currentEpoch</code> 5, and is accepted as valid.</p><p>Masters don’t vote for a slave of the same master before <code>NODE_TIMEOUT * 2</code> has elapsed if a slave of that master was already voted for. This is not strictly required as it is not possible for two slaves to win the election in the same epoch. However, in practical terms it ensures that when a slave is elected it has plenty of time to inform the other slaves and avoid the possibility that another slave will win a new election, performing an unnecessary second failover.</p>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"MASTERS REPLY TO SLAVE VOTE REQUEST"},{"content":"<h2 id=\"configuration-handling-propagation-and-failovers-practical-example-of-configuration-epoch-usefulness-during-partitions\">Practical example of configuration epoch usefulness during partitions</h2><p>This section illustrates how the epoch concept is used to make the slave promotion process more resistant to partitions.</p><ul>\n<li>A master is no longer reachable indefinitely. The master has three slaves A, B, C.</li>\n<li>Slave A wins the election and is promoted to master.</li>\n<li>A network partition makes A not available for the majority of the cluster.</li>\n<li>Slave B wins the election and is promoted as master.</li>\n<li>A partition makes B not available for the majority of the cluster.</li>\n<li>The previous partition is fixed, and A is available again.</li>\n</ul><p>At this point B is down and A is available again with a role of master (actually <code>UPDATE</code> messages would reconfigure it promptly, but here we assume all <code>UPDATE</code> messages were lost). At the same time, slave C will try to get elected in order to fail over B. This is what happens:</p><p>As you’ll see in the next sections, a stale node rejoining a cluster\nwill usually get notified as soon as possible about the configuration change\nbecause as soon as it pings any other node, the receiver will detect it\nhas stale information and will send an <code>UPDATE</code> message.</p>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"PRACTICAL EXAMPLE OF CONFIGURATION EPOCH USEFULNESS DURING PARTITIONS"},{"content":"<h2 id=\"configuration-handling-propagation-and-failovers-hash-slots-configuration-propagation\">Hash slots configuration propagation</h2><p>An important part of Redis Cluster is the mechanism used to propagate the information about which cluster node is serving a given set of hash slots. This is vital to both the startup of a fresh cluster and the ability to upgrade the configuration after a slave was promoted to serve the slots of its failing master.</p><p>The same mechanism allows nodes partitioned away for an indefinite amount of\ntime to rejoin the cluster in a sensible way.</p><p>There are two ways hash slot configurations are propagated:</p><p>The receiver of a heartbeat or <code>UPDATE</code> message uses certain simple rules in\norder to update its table mapping hash slots to nodes. When a new Redis Cluster node is created, its local hash slot table is simply initialized to <code>NULL</code> entries so that each hash slot is not bound or linked to any node. This looks similar to the following:</p><p>The first rule followed by a node in order to update its hash slot table is the following:</p><p><strong>Rule 1</strong>: If a hash slot is unassigned (set to <code>NULL</code>), and a known node claims it, I’ll modify my hash slot table and associate the claimed hash slots to it.</p><p>So if we receive a heartbeat from node A claiming to serve hash slots 1 and 2 with a configuration epoch value of 3, the table will be modified to:</p><p>When a new cluster is created, a system administrator needs to manually assign (using the <code>CLUSTER ADDSLOTS</code> command, via the redis-trib command line tool, or by any other means) the slots served by each master node only to the node itself, and the information will rapidly propagate across the cluster.</p><p>However this rule is not enough. We know that hash slot mapping can change\nduring two events:</p><p>For now let’s focus on failovers. When a slave fails over its master, it obtains\na configuration epoch which is guaranteed to be greater than the one of its\nmaster (and more generally greater than any other configuration epoch\ngenerated previously). For example node B, which is a slave of A, may failover\nB with configuration epoch of 4. It will start to send heartbeat packets\n(the first time mass-broadcasting cluster-wide) and because of the following\nsecond rule, receivers will update their hash slot tables:</p><p><strong>Rule 2</strong>: If a hash slot is already assigned, and a known node is advertising it using a <code>configEpoch</code> that is greater than the <code>configEpoch</code> of the master currently associated with the slot, I’ll rebind the hash slot to the new node.</p><p>So after receiving messages from B that claim to serve hash slots 1 and 2 with configuration epoch of 4, the receivers will update their table in the following way:</p><p>Liveness property: because of the second rule, eventually all nodes in the cluster will agree that the owner of a slot is the one with the greatest <code>configEpoch</code> among the nodes advertising it.</p><p>This mechanism in Redis Cluster is called <strong>last failover wins</strong>.</p><p>The same happens during reshardings. When a node importing a hash slot\ncompletes the import operation, its configuration epoch is incremented to make\nsure the change will be propagated throughout the cluster.</p>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"HASH SLOTS CONFIGURATION PROPAGATION"},{"content":"<h2 id=\"configuration-handling-propagation-and-failovers-update-messages-a-closer-look\">UPDATE messages, a closer look</h2><p>With the previous section in mind, it is easier to see how update messages\nwork. Node A may rejoin the cluster after some time. It will send heartbeat\npackets where it claims it serves hash slots 1 and 2 with configuration epoch\nof 3. All the receivers with updated information will instead see that\nthe same hash slots are associated with node B having an higher configuration\nepoch. Because of this they’ll send an <code>UPDATE</code> message to A with the new\nconfiguration for the slots. A will update its configuration because of the\n<strong>rule 2</strong> above.</p>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"UPDATE MESSAGES, A CLOSER LOOK"},{"content":"<h2 id=\"configuration-handling-propagation-and-failovers-how-nodes-rejoin-the-cluster\">How nodes rejoin the cluster</h2><p>The same basic mechanism is used when a node rejoins a cluster.\nContinuing with the example above, node A will be notified\nthat hash slots 1 and 2 are now served by B. Assuming that these two were\nthe only hash slots served by A, the count of hash slots served by A will\ndrop to 0! So A will <strong>reconfigure to be a slave of the new master</strong>.</p><p>The actual rule followed is a bit more complex than this. In general it may\nhappen that A rejoins after a lot of time, in the meantime it may happen that\nhash slots originally served by A are served by multiple nodes, for example\nhash slot 1 may be served by B, and hash slot 2 by C.</p><p>So the actual <em>Redis Cluster node role switch rule</em> is: <strong>A master node will change its configuration to replicate (be a slave of) the node that stole its last hash slot</strong>.</p><p>During reconfiguration, eventually the number of served hash slots will drop to zero, and the node will reconfigure accordingly. Note that in the base case this just means that the old master will be a slave of the slave that replaced it after a failover. However in the general form the rule covers all possible cases.</p><p>Slaves do exactly the same: they reconfigure to replicate the node that\nstole the last hash slot of its former master.</p>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"HOW NODES REJOIN THE CLUSTER"},{"content":"<h2 id=\"configuration-handling-propagation-and-failovers-replica-migration\">Replica migration</h2><p>Redis Cluster implements a concept called <em>replica migration</em> in order to\nimprove the availability of the system. The idea is that in a cluster with\na master-slave setup, if the map between slaves and masters is fixed\navailability is limited over time if multiple independent failures of single\nnodes happen.</p><p>For example in a cluster where every master has a single slave, the cluster\ncan continue operations as long as either the master or the slave fail, but not\nif both fail the same time. However there is a class of failures that are\nthe independent failures of single nodes caused by hardware or software issues\nthat can accumulate over time. For example:</p><ul>\n<li>Master A has a single slave A1.</li>\n<li>Master A fails. A1 is promoted as new master.</li>\n<li>Three hours later A1 fails in an independent manner (unrelated to the failure of A). No other slave is available for promotion since node A is still down. The cluster cannot continue normal operations.</li>\n</ul><p>If the map between masters and slaves is fixed, the only way to make the cluster\nmore resistant to the above scenario is to add slaves to every master, however\nthis is costly as it requires more instances of Redis to be executed, more\nmemory, and so forth.</p><p>An alternative is to create an asymmetry in the cluster, and let the cluster\nlayout automatically change over time. For example the cluster may have three\nmasters A, B, C. A and B have a single slave each, A1 and B1. However the master\nC is different and has two slaves: C1 and C2.</p><p>Replica migration is the process of automatic reconfiguration of a slave\nin order to <em>migrate</em> to a master that has no longer coverage (no working\nslaves). With replica migration the scenario mentioned above turns into the\nfollowing:</p><ul>\n<li>Master A fails. A1 is promoted.</li>\n<li>C2 migrates as slave of A1, that is otherwise not backed by any slave.</li>\n<li>Three hours later A1 fails as well.</li>\n<li>C2 is promoted as new master to replace A1.</li>\n<li>The cluster can continue the operations.</li>\n</ul>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"REPLICA MIGRATION"},{"content":"<h2 id=\"configuration-handling-propagation-and-failovers-replica-migration-algorithm\">Replica migration algorithm</h2><p>The migration algorithm does not use any form of agreement since the slave\nlayout in a Redis Cluster is not part of the cluster configuration that needs\nto be consistent and/or versioned with config epochs. Instead it uses an\nalgorithm to avoid mass-migration of slaves when a master is not backed.\nThe algorithm guarantees that eventually (once the cluster configuration is\nstable) every master will be backed by at least one slave.</p><p>This is how the algorithm works. To start we need to define what is a\n<em>good slave</em> in this context: a good slave is a slave not in <code>FAIL</code> state\nfrom the point of view of a given node.</p><p>The execution of the algorithm is triggered in every slave that detects that\nthere is at least a single master without good slaves. However among all the\nslaves detecting this condition, only a subset should act. This subset is\nactually often a single slave unless different slaves have in a given moment\na slightly different view of the failure state of other nodes.</p><p>The <em>acting slave</em> is the slave among the masters with the maximum number\nof attached slaves, that is not in FAIL state and has the smallest node ID.</p><p>So for example if there are 10 masters with 1 slave each, and 2 masters with\n5 slaves each, the slave that will try to migrate is - among the 2 masters\nhaving 5 slaves - the one with the lowest node ID. Given that no agreement\nis used, it is possible that when the cluster configuration is not stable,\na race condition occurs where multiple slaves believe themselves to be\nthe non-failing slave with the lower node ID (it is unlikely for this to happen\nin practice). If this happens, the result is multiple slaves migrating to the\nsame master, which is harmless. If the race happens in a way that will leave\nthe ceding master without slaves, as soon as the cluster is stable again\nthe algorithm will be re-executed again and will migrate a slave back to\nthe original master.</p><p>Eventually every master will be backed by at least one slave. However,\nthe normal behavior is that a single slave migrates from a master with\nmultiple slaves to an orphaned master.</p><p>The algorithm is controlled by a user-configurable parameter called\n<code>cluster-migration-barrier</code>: the number of good slaves a master\nmust be left with before a slave can migrate away. For example, if this\nparameter is set to 2, a slave can try to migrate only if its master remains\nwith two working slaves.</p>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"REPLICA MIGRATION ALGORITHM"},{"content":"<h2 id=\"configuration-handling-propagation-and-failovers-configepoch-conflicts-resolution-algorithm\">configEpoch conflicts resolution algorithm</h2><p>When new <code>configEpoch</code> values are created via slave promotion during\nfailovers, they are guaranteed to be unique.</p><p>However there are two distinct events where new configEpoch values are\ncreated in an unsafe way, just incrementing the local <code>currentEpoch</code> of\nthe local node and hoping there are no conflicts at the same time.\nBoth the events are system-administrator triggered:</p><p>Specifically, during manual reshardings, when a hash slot is migrated from\na node A to a node B, the resharding program will force B to upgrade\nits configuration to an epoch which is the greatest found in the cluster,\nplus 1 (unless the node is already the one with the greatest configuration\nepoch), without requiring agreement from other nodes.\nUsually a real world resharding involves moving several hundred hash slots\n(especially in small clusters). Requiring an agreement to generate new\nconfiguration epochs during reshardings, for each hash slot moved, is\ninefficient. Moreover it requires an fsync in each of the cluster nodes\nevery time in order to store the new configuration. Because of the way it is\nperformed instead, we only need a new config epoch when the first hash slot is moved,\nmaking it much more efficient in production environments.</p><p>However because of the two cases above, it is possible (though unlikely) to end\nwith multiple nodes having the same configuration epoch. A resharding operation\nperformed by the system administrator, and a failover happening at the same\ntime (plus a lot of bad luck) could cause <code>currentEpoch</code> collisions if\nthey are not propagated fast enough.</p><p>Moreover, software bugs and filesystem corruptions can also contribute\nto multiple nodes having the same configuration epoch.</p><p>When masters serving different hash slots have the same <code>configEpoch</code>, there\nare no issues. It is more important that slaves failing over a master have\nunique configuration epochs.</p><p>That said, manual interventions or reshardings may change the cluster\nconfiguration in different ways. The Redis Cluster main liveness property\nrequires that slot configurations always converge, so under every circumstance\nwe really want all the master nodes to have a different <code>configEpoch</code>.</p><p>In order to enforce this, <strong>a conflict resolution algorithm</strong> is used in the\nevent that two nodes end up with the same <code>configEpoch</code>.</p><ul>\n<li>IF a master node detects another master node is advertising itself with\nthe same <code>configEpoch</code>.</li>\n<li>AND IF the node has a lexicographically smaller Node ID compared to the other node claiming the same <code>configEpoch</code>.</li>\n<li>THEN it increments its <code>currentEpoch</code> by 1, and uses it as the new <code>configEpoch</code>.</li>\n</ul><p>If there are any set of nodes with the same <code>configEpoch</code>, all the nodes but the one with the greatest Node ID will move forward, guaranteeing that, eventually, every node will pick a unique configEpoch regardless of what happened.</p><p>This mechanism also guarantees that after a fresh cluster is created, all\nnodes start with a different <code>configEpoch</code> (even if this is not actually\nused) since <code>redis-trib</code> makes sure to use <code>CONFIG SET-CONFIG-EPOCH</code> at startup.\nHowever if for some reason a node is left misconfigured, it will update\nits configuration to a different configuration epoch automatically.</p>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"CONFIGEPOCH CONFLICTS RESOLUTION ALGORITHM"},{"content":"<h2 id=\"configuration-handling-propagation-and-failovers-node-resets\">Node resets</h2><p>Nodes can be software reset (without restarting them) in order to be reused\nin a different role or in a different cluster. This is useful in normal\noperations, in testing, and in cloud environments where a given node can\nbe reprovisioned to join a different set of nodes to enlarge or create a new\ncluster.</p><p>In Redis Cluster nodes are reset using the <code>CLUSTER RESET</code> command. The\ncommand is provided in two variants:</p><ul>\n<li><code>CLUSTER RESET SOFT</code></li>\n<li><code>CLUSTER RESET HARD</code></li>\n</ul><p>The command must be sent directly to the node to reset. If no reset type is\nprovided, a soft reset is performed.</p><p>The following is a list of operations performed by a reset:</p><p>Master nodes with non-empty data sets can’t be reset (since normally you want to reshard data to the other nodes). However, under special conditions when this is appropriate (e.g. when a cluster is totally destroyed with the intent of creating a new one), <code>FLUSHALL</code> must be executed before proceeding with the reset.</p>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"NODE RESETS"},{"content":"<h2 id=\"configuration-handling-propagation-and-failovers-removing-nodes-from-a-cluster\">Removing nodes from a cluster</h2><p>It is possible to practically remove a node from an existing cluster by\nresharding all its data to other nodes (if it is a master node) and\nshutting it down. However, the other nodes will still remember its node\nID and address, and will attempt to connect with it.</p><p>For this reason, when a node is removed we want to also remove its entry\nfrom all the other nodes tables. This is accomplished by using the\n<code>CLUSTER FORGET &lt;node-id&gt;</code> command.</p><p>The command does two things:</p><p>The second operation is needed because Redis Cluster uses gossip in order to auto-discover nodes, so removing the node X from node A, could result in node B gossiping about node X to A again. Because of the 60 second ban, the Redis Cluster administration tools have 60 seconds in order to remove the node from all the nodes, preventing the re-addition of the node due to auto discovery.</p><p>Further information is available in the <code>CLUSTER FORGET</code> documentation.</p>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"REMOVING NODES FROM A CLUSTER"},{"content":"<h1 id=\"publishsubscribe\">Publish/Subscribe</h1><p>In a Redis Cluster clients can subscribe to every node, and can also\npublish to every other node. The cluster will make sure that published\nmessages are forwarded as needed.</p><p>The current implementation will simply broadcast each published message\nto all other nodes, but at some point this will be optimized either\nusing Bloom filters or other algorithms.</p>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"PUBLISH/SUBSCRIBE"},{"content":"<h1 id=\"appendix\">Appendix</h1>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"APPENDIX"},{"content":"<h2 id=\"appendix-appendix-a-crc16-reference-implementation-in-ansi-c\">Appendix A: CRC16 reference implementation in ANSI C</h2>","link":"/alpha/topics/cluster-spec.html","spaLink":"#/alpha/topics/cluster-spec","title":"APPENDIX A: CRC16 REFERENCE IMPLEMENTATION IN ANSI C"},{"content":"<h1 id=\"redis-cluster-tutorial\">Redis cluster tutorial</h1><p>This document is a gentle introduction to Redis Cluster, that does not use\ncomplex to understand distributed systems concepts. It provides instructions\nabout how to setup a cluster, test, and operate it, without\ngoing into the details that are covered in\nthe <a href=\"/topics/cluster-spec\">Redis Cluster specification</a> but just describing\nhow the system behaves from the point of view of the user.</p><p>However this tutorial tries to provide information about the availability\nand consistency characteristics of Redis Cluster from the point of view\nof the final user, stated in a simple to understand way.</p><p>Note this tutorial requires Redis version 3.0 or higher.</p><p>If you plan to run a serious Redis Cluster deployment, the\nmore formal specification is a suggested reading, even if not\nstrictly required. However it is a good idea to start from this document,\nplay with Redis Cluster some time, and only later read the specification.</p>","link":"/alpha/topics/cluster-tutorial.html","spaLink":"#/alpha/topics/cluster-tutorial","title":"REDIS CLUSTER TUTORIAL"},{"content":"<h2 id=\"redis-cluster-tutorial-redis-cluster-101\">Redis Cluster 101</h2><p>Redis Cluster provides a way to run a Redis installation where data is\n<strong>automatically sharded across multiple Redis nodes</strong>.</p><p>Redis Cluster also provides <strong>some degree of availability during partitions</strong>,\nthat is in practical terms the ability to continue the operations when\nsome nodes fail or are not able to communicate. However the cluster stops\nto operate in the event of larger failures (for example when the majority of\nmasters are unavailable).</p><p>So in practical terms, what you get with Redis Cluster?</p><ul>\n<li>The ability to <strong>automatically split your dataset among multiple nodes</strong>.</li>\n<li>The ability to <strong>continue operations when a subset of the nodes are experiencing failures</strong> or are unable to communicate with the rest of the cluster.</li>\n</ul>","link":"/alpha/topics/cluster-tutorial.html","spaLink":"#/alpha/topics/cluster-tutorial","title":"REDIS CLUSTER 101"},{"content":"<h2 id=\"redis-cluster-tutorial-redis-cluster-tcp-ports\">Redis Cluster TCP ports</h2><p>Every Redis Cluster node requires two TCP connections open. The normal Redis\nTCP port used to serve clients, for example 6379, plus the port obtained by\nadding 10000 to the data port, so 16379 in the example.</p><p>This second <em>high</em> port is used for the Cluster bus, that is a node-to-node\ncommunication channel using a binary protocol. The Cluster bus is used by\nnodes for failure detection, configuration update, failover authorization\nand so forth. Clients should never try to communicate with the cluster bus\nport, but always with the normal Redis command port, however make sure you\nopen both ports in your firewall, otherwise Redis cluster nodes will be\nnot able to communicate.</p><p>The command port and cluster bus port offset is fixed and is always 10000.</p><p>Note that for a Redis Cluster to work properly you need, for each node:</p><p>If you don’t open both TCP ports, your cluster will not work as expected.</p><p>The cluster bus uses a different, binary protocol, for node to node data\nexchange, which is more suited to exchange information between nodes using\nlittle bandwidth and processing time.</p>","link":"/alpha/topics/cluster-tutorial.html","spaLink":"#/alpha/topics/cluster-tutorial","title":"REDIS CLUSTER TCP PORTS"},{"content":"<h2 id=\"redis-cluster-tutorial-redis-cluster-and-docker\">Redis Cluster and Docker</h2><p>Currently Redis Cluster does not support NATted environments and in general\nenvironments where IP addresses or TCP ports are remapped.</p><p>Docker uses a technique called <em>port mapping</em>: programs running inside Docker\ncontainers may be exposed with a different port compared to the one the\nprogram believes to be using. This is useful in order to run multiple\ncontainers using the same ports, at the same time, in the same server.</p><p>In order to make Docker compatible with Redis Cluster you need to use\nthe <strong>host networking mode</strong> of Docker. Please check the <code>--net=host</code> option\nin the <a href=\"https://docs.docker.com/engine/userguide/networking/dockernetworks/\">Docker documentation</a> for more information.</p>","link":"/alpha/topics/cluster-tutorial.html","spaLink":"#/alpha/topics/cluster-tutorial","title":"REDIS CLUSTER AND DOCKER"},{"content":"<h2 id=\"redis-cluster-tutorial-redis-cluster-data-sharding\">Redis Cluster data sharding</h2><p>Redis Cluster does not use consistent hashing, but a different form of sharding\nwhere every key is conceptually part of what we call an <strong>hash slot</strong>.</p><p>There are 16384 hash slots in Redis Cluster, and to compute what is the hash\nslot of a given key, we simply take the CRC16 of the key modulo\n16384.</p><p>Every node in a Redis Cluster is responsible for a subset of the hash slots,\nso for example you may have a cluster with 3 nodes, where:</p><ul>\n<li>Node A contains hash slots from 0 to 5500.</li>\n<li>Node B contains hash slots from 5501 to 11000.</li>\n<li>Node C contains hash slots from 11001 to 16383.</li>\n</ul><p>This allows to add and remove nodes in the cluster easily. For example if\nI want to add a new node D, I need to move some hash slot from nodes A, B, C\nto D. Similarly if I want to remove node A from the cluster I can just\nmove the hash slots served by A to B and C. When the node A will be empty\nI can remove it from the cluster completely.</p><p>Because moving hash slots from a node to another does not require to stop\noperations, adding and removing nodes, or changing the percentage of hash\nslots hold by nodes, does not require any downtime.</p><p>Redis Cluster supports multiple key operations as long as all the keys involved\ninto a single command execution (or whole transaction, or Lua script\nexecution) all belong to the same hash slot. The user can force multiple keys\nto be part of the same hash slot by using a concept called <em>hash tags</em>.</p><p>Hash tags are documented in the Redis Cluster specification, but the gist is\nthat if there is a substring between {} brackets in a key, only what is\ninside the string is hashed, so for example <code>this{foo}key</code> and <code>another{foo}key</code>\nare guaranteed to be in the same hash slot, and can be used together in a\ncommand with multiple keys as arguments.</p>","link":"/alpha/topics/cluster-tutorial.html","spaLink":"#/alpha/topics/cluster-tutorial","title":"REDIS CLUSTER DATA SHARDING"},{"content":"<h2 id=\"redis-cluster-tutorial-redis-cluster-master-slave-model\">Redis Cluster master-slave model</h2><p>In order to remain available when a subset of master nodes are failing or are\nnot able to communicate with the majority of nodes, Redis Cluster uses a\nmaster-slave model where every hash slot has from 1 (the master itself) to N\nreplicas (N-1 additional slaves nodes).</p><p>In our example cluster with nodes A, B, C, if node B fails the cluster is not\nable to continue, since we no longer have a way to serve hash slots in the\nrange 5501-11000.</p><p>However when the cluster is created (or at a latter time) we add a slave\nnode to every master, so that the final cluster is composed of A, B, C\nthat are masters nodes, and A1, B1, C1 that are slaves nodes, the system is\nable to continue if node B fails.</p><p>Node B1 replicates B, and B fails, the cluster will promote node B1 as the new\nmaster and will continue to operate correctly.</p><p>However note that if nodes B and B1 fail at the same time Redis Cluster is not\nable to continue to operate.</p>","link":"/alpha/topics/cluster-tutorial.html","spaLink":"#/alpha/topics/cluster-tutorial","title":"REDIS CLUSTER MASTER-SLAVE MODEL"},{"content":"<h2 id=\"redis-cluster-tutorial-redis-cluster-consistency-guarantees\">Redis Cluster consistency guarantees</h2><p>Redis Cluster is not able to guarantee <strong>strong consistency</strong>. In practical\nterms this means that under certain conditions it is possible that Redis\nCluster will lose writes that were acknowledged by the system to the client.</p><p>The first reason why Redis Cluster can lose writes is because it uses\nasynchronous replication. This means that during writes the following\nhappens:</p><ul>\n<li>Your client writes to the master B.</li>\n<li>The master B replies OK to your client.</li>\n<li>The master B propagates the write to its slaves B1, B2 and B3.</li>\n</ul><p>As you can see B does not wait for an acknowledge from B1, B2, B3 before\nreplying to the client, since this would be a prohibitive latency penalty\nfor Redis, so if your client writes something, B acknowledges the write,\nbut crashes before being able to send the write to its slaves, one of the\nslaves (that did not receive the write) can be promoted to master, losing\nthe write forever.</p><p>This is <strong>very similar to what happens</strong> with most databases that are\nconfigured to flush data to disk every second, so it is a scenario you\nare already able to reason about because of past experiences with traditional\ndatabase systems not involving distributed systems. Similarly you can\nimprove consistency by forcing the database to flush data on disk before\nreplying to the client, but this usually results into prohibitively low\nperformance. That would be the equivalent of synchronous replication in\nthe case of Redis Cluster.</p><p>Basically there is a trade-off to take between performance and consistency.</p><p>Redis Cluster has support for synchronous writes when absolutely needed,\nimplemented via the <code>WAIT</code> command, this makes losing writes a lot less\nlikely, however note that Redis Cluster does not implement strong consistency\neven when synchronous replication is used: it is always possible under more\ncomplex failure scenarios that a slave that was not able to receive the write\nis elected as master.</p><p>There is another notable scenario where Redis Cluster will lose writes, that\nhappens during a network partition where a client is isolated with a minority\nof instances including at least a master.</p><p>Take as an example our 6 nodes cluster composed of A, B, C, A1, B1, C1,\nwith 3 masters and 3 slaves. There is also a client, that we will call Z1.</p><p>After a partition occurs, it is possible that in one side of the\npartition we have A, C, A1, B1, C1, and in the other side we have B and Z1.</p><p>Z1 is still able to write to B, that will accept its writes. If the\npartition heals in a very short time, the cluster will continue normally.\nHowever if the partition lasts enough time for B1 to be promoted to master\nin the majority side of the partition, the writes that Z1 is sending to B\nwill be lost.</p><p>Note that there is a <strong>maximum window</strong> to the amount of writes Z1 will be able\nto send to B: if enough time has elapsed for the majority side of the\npartition to elect a slave as master, every master node in the minority\nside stops accepting writes.</p><p>This amount of time is a very important configuration directive of Redis\nCluster, and is called the <strong>node timeout</strong>.</p><p>After node timeout has elapsed, a master node is considered to be failing,\nand can be replaced by one of its replicas.\nSimilarly after node timeout has elapsed without a master node to be able\nto sense the majority of the other master nodes, it enters an error state\nand stops accepting writes.</p>","link":"/alpha/topics/cluster-tutorial.html","spaLink":"#/alpha/topics/cluster-tutorial","title":"REDIS CLUSTER CONSISTENCY GUARANTEES"},{"content":"<h1 id=\"redis-cluster-configuration-parameters\">Redis Cluster configuration parameters</h1><p>We are about to create an example cluster deployment. Before to continue\nlet’s introduce the configuration parameters that Redis Cluster introduces\nin the <code>redis.conf</code> file. Some will be obvious, others will be more clear\nas you continue reading.</p><ul>\n<li><strong>cluster-enabled <code>&lt;yes/no&gt;</code></strong>: If yes enables Redis Cluster support in a specific Redis instance. Otherwise the instance starts as a stand alone instance as usually.</li>\n<li><strong>cluster-config-file <code>&lt;filename&gt;</code></strong>: Note that despite the name of this option, this is not an user editable configuration file, but the file where a Redis Cluster node automatically persists the cluster configuration (the state, basically) every time there is a change, in order to be able to re-read it at startup. The file lists things like the other nodes in the cluster, their state, persistent variables, and so forth. Often this file is rewritten and flushed on disk as a result of some message reception.</li>\n<li><strong>cluster-node-timeout <code>&lt;milliseconds&gt;</code></strong>: The maximum amount of time a Redis Cluster node can be unavailable, without it being considered as failing. If a master node is not reachable for more than the specified amount of time, it will be failed over by its slaves. This parameter controls other important things in Redis Cluster. Notably, every node that can’t reach the majority of master nodes for the specified amount of time, will stop accepting queries.</li>\n<li><strong>cluster-slave-validity-factor <code>&lt;factor&gt;</code></strong>: If set to zero, a slave will always try to failover a master, regardless of the amount of time the link between the master and the slave remained disconnected. If the value is positive, a maximum disconnection time is calculated as the <em>node timeout</em> value multiplied by the factor provided with this option, and if the node is a slave, it will not try to start a failover if the master link was disconnected for more than the specified amount of time. For example if the node timeout is set to 5 seconds, and the validity factor is set to 10, a slave disconnected from the master for more than 50 seconds will not try to failover its master. Note that any value different than zero may result in Redis Cluster to be unavailable after a master failure if there is no slave able to failover it. In that case the cluster will return back available only when the original master rejoins the cluster.</li>\n<li><strong>cluster-migration-barrier <code>&lt;count&gt;</code></strong>: Minimum number of slaves a master will remain connected with, for another slave to migrate to a master which is no longer covered by any slave. See the appropriate section about replica migration in this tutorial for more information.</li>\n<li><strong>cluster-require-full-coverage <code>&lt;yes/no&gt;</code></strong>: If this is set to yes, as it is by default, the cluster stops accepting writes if some percentage of the key space is not covered by any node. If the option is set to no, the cluster will still serve queries even if only requests about a subset of keys can be processed.</li>\n</ul>","link":"/alpha/topics/cluster-tutorial.html","spaLink":"#/alpha/topics/cluster-tutorial","title":"REDIS CLUSTER CONFIGURATION PARAMETERS"},{"content":"<h1 id=\"creating-and-using-a-redis-cluster\">Creating and using a Redis Cluster</h1><p>Note: to deploy a Redis Cluster manually is <strong>very important to learn</strong> certain\noperation aspects of it. However if you want to get a cluster up and running\nASAP skip this section and the next one and go directly to <strong>Creating a Redis Cluster using the create-cluster script</strong>.</p><p>To create a cluster, the first thing we need is to have a few empty\nRedis instances running in <strong>cluster mode</strong>. This basically means that\nclusters are not created using normal Redis instances, but a special mode\nneeds to be configured so that the Redis instance will enable the Cluster\nspecific features and commands.</p><p>The following is a minimal Redis cluster configuration file:</p><p>As you can see what enables the cluster mode is simply the <code>cluster-enabled</code>\ndirective. Every instance also contains the path of a file where the\nconfiguration for this node is stored, that by default is <code>nodes.conf</code>.\nThis file is never touched by humans, it is simply generated at startup\nby the Redis Cluster instances, and updated every time it is needed.</p><p>Note that the <strong>minimal cluster</strong> that works as expected requires to contain\nat least three master nodes. For your first tests it is strongly suggested\nto start a six nodes cluster with three masters and three slaves.</p><p>To do so, enter a new directory, and create the following directories named\nafter the port number of the instance we’ll run inside any given directory.</p><p>Something like:</p><p>Create a <code>redis.conf</code> file inside each of the directories, from 7000 to 7005.\nAs a template for your configuration file just use the small example above,\nbut make sure to replace the port number <code>7000</code> with the right port number\naccording to the directory name.</p><p>Now copy your redis-server executable, <strong>compiled from the latest sources in the unstable branch at GitHub</strong>, into the <code>cluster-test</code> directory, and finally open 6 terminal tabs in your favorite terminal application.</p><p>Start every instance like that, one every tab:</p><p>As you can see from the logs of every instance, since no <code>nodes.conf</code> file\nexisted, every node assigns itself a new ID.</p><p>This ID will be used forever by this specific instance in order for the instance\nto have a unique name in the context of the cluster. Every node\nremembers every other node using this IDs, and not by IP or port.\nIP addresses and ports may change, but the unique node identifier will never\nchange for all the life of the node. We call this identifier simply <strong>Node ID</strong>.</p>","link":"/alpha/topics/cluster-tutorial.html","spaLink":"#/alpha/topics/cluster-tutorial","title":"CREATING AND USING A REDIS CLUSTER"},{"content":"<h2 id=\"creating-and-using-a-redis-cluster-creating-the-cluster\">Creating the cluster</h2><p>Now that we have a number of instances running, we need to create our\ncluster by writing some meaningful configuration to the nodes.</p><p>This is very easy to accomplish as we are helped by the Redis Cluster\ncommand line utility called <code>redis-trib</code>, a Ruby program\nexecuting special commands on instances in order to create new clusters,\ncheck or reshard an existing cluster, and so forth.</p><p>The <code>redis-trib</code> utility is in the <code>src</code> directory of the Redis source code\ndistribution.\nYou need to install <code>redis</code> gem to be able to run <code>redis-trib</code>.</p><p> To create your cluster simply type:</p><p>The command used here is <strong>create</strong>, since we want to create a new cluster.\nThe option <code>--replicas 1</code> means that we want a slave for every master created.\nThe other arguments are the list of addresses of the instances I want to use\nto create the new cluster.</p><p>Obviously the only setup with our requirements is to create a cluster with\n3 masters and 3 slaves.</p><p>Redis-trib will propose you a configuration. Accept typing <strong>yes</strong>.\nThe cluster will be configured and <em>joined</em>, that means, instances will be\nbootstrapped into talking with each other. Finally if everything went ok\nyou’ll see a message like that:</p><p>This means that there is at least a master instance serving each of the\n16384 slots available.</p>","link":"/alpha/topics/cluster-tutorial.html","spaLink":"#/alpha/topics/cluster-tutorial","title":"CREATING THE CLUSTER"},{"content":"<h2 id=\"creating-and-using-a-redis-cluster-creating-a-redis-cluster-using-the-create-cluster-script\">Creating a Redis Cluster using the create-cluster script</h2><p>If you don’t want to create a Redis Cluster by configuring and executing\nindividual instances manually as explained above, there is a much simpler\nsystem (but you’ll not learn the same amount of operational details).</p><p>Just check <code>utils/create-cluster</code> directory in the Redis distribution.\nThere is a script called <code>create-cluster</code> inside (same name as the directory\nit is contained into), it’s a simple bash script. In order to start\na 6 nodes cluster with 3 masters and 3 slaves just type the following\ncommands:</p><p>Reply to <code>yes</code> in step 2 when the <code>redis-trib</code> utility wants you to accept\nthe cluster layout.</p><p>You can now interact with the cluster, the first node will start at port 30001\nby default. When you are done, stop the cluster with:</p><p>Please read the <code>README</code> inside this directory for more information on how\nto run the script.</p>","link":"/alpha/topics/cluster-tutorial.html","spaLink":"#/alpha/topics/cluster-tutorial","title":"CREATING A REDIS CLUSTER USING THE CREATE-CLUSTER SCRIPT"},{"content":"<h2 id=\"creating-and-using-a-redis-cluster-playing-with-the-cluster\">Playing with the cluster</h2><p>At this stage one of the problems with Redis Cluster is the lack of\nclient libraries implementations.</p><p>I’m aware of the following implementations:</p><ul>\n<li><a href=\"http://github.com/antirez/redis-rb-cluster\">redis-rb-cluster</a> is a Ruby implementation written by me (@antirez) as a reference for other languages. It is a simple wrapper around the original redis-rb, implementing the minimal semantics to talk with the cluster efficiently.</li>\n<li><a href=\"https://github.com/Grokzen/redis-py-cluster\">redis-py-cluster</a> A port of redis-rb-cluster to Python. Supports majority of <em>redis-py</em> functionality. Is in active development.</li>\n<li>The popular <a href=\"https://github.com/nrk/predis\">Predis</a> has support for Redis Cluster, the support was recently updated and is in active development.</li>\n<li>The most used Java client, <a href=\"https://github.com/xetorthio/jedis\">Jedis</a> recently added support for Redis Cluster, see the <em>Jedis Cluster</em> section in the project README.</li>\n<li><a href=\"https://github.com/StackExchange/StackExchange.Redis\">StackExchange.Redis</a> offers support for C# (and should work fine with most .NET languages; VB, F#, etc)</li>\n<li><a href=\"https://github.com/thunks/thunk-redis\">thunk-redis</a> offers support for Node.js and io.js, it is a thunk/promise-based redis client with pipelining and cluster.</li>\n<li><a href=\"https://github.com/chasex/redis-go-cluster\">redis-go-cluster</a> is an implementation of Redis Cluster for the Go language using the <a href=\"https://github.com/garyburd/redigo\">Redigo library client</a> as the base client. Implements MGET/MSET via result aggregation.</li>\n<li>The <code>redis-cli</code> utility in the unstable branch of the Redis repository at GitHub implements a very basic cluster support when started with the <code>-c</code> switch.</li>\n</ul><p>An easy way to test Redis Cluster is either to try any of the above clients\nor simply the <code>redis-cli</code> command line utility. The following is an example\nof interaction using the latter:</p><p><strong>Note:</strong> if you created the cluster using the script your nodes may listen\nto different ports, starting from 30001 by default.</p><p>The redis-cli cluster support is very basic so it always uses the fact that\nRedis Cluster nodes are able to redirect a client to the right node.\nA serious client is able to do better than that, and cache the map between\nhash slots and nodes addresses, to directly use the right connection to the\nright node. The map is refreshed only when something changed in the cluster\nconfiguration, for example after a failover or after the system administrator\nchanged the cluster layout by adding or removing nodes.</p>","link":"/alpha/topics/cluster-tutorial.html","spaLink":"#/alpha/topics/cluster-tutorial","title":"PLAYING WITH THE CLUSTER"},{"content":"<h2 id=\"creating-and-using-a-redis-cluster-writing-an-example-app-with-redis-rb-cluster\">Writing an example app with redis-rb-cluster</h2><p>Before going forward showing how to operate the Redis Cluster, doing things\nlike a failover, or a resharding, we need to create some example application\nor at least to be able to understand the semantics of a simple Redis Cluster\nclient interaction.</p><p>In this way we can run an example and at the same time try to make nodes\nfailing, or start a resharding, to see how Redis Cluster behaves under real\nworld conditions. It is not very helpful to see what happens while nobody\nis writing to the cluster.</p><p>This section explains some basic usage of redis-rb-cluster showing two\nexamples. The first is the following, and is the <code>example.rb</code> file inside\nthe redis-rb-cluster distribution:</p><p>The application does a very simple thing, it sets keys in the form <code>foo&lt;number&gt;</code> to <code>number</code>, one after the other. So if you run the program the result is the\nfollowing stream of commands:</p><ul>\n<li>SET foo0 0</li>\n<li>SET foo1 1</li>\n<li>SET foo2 2</li>\n<li>And so forth…</li>\n</ul><p>The program looks more complex than it should usually as it is designed to\nshow errors on the screen instead of exiting with an exception, so every\noperation performed with the cluster is wrapped by <code>begin</code> <code>rescue</code> blocks.</p><p>The <strong>line 7</strong> is the first interesting line in the program. It creates the\nRedis Cluster object, using as argument a list of <em>startup nodes</em>, the maximum\nnumber of connections this object is allowed to take against different nodes,\nand finally the timeout after a given operation is considered to be failed.</p><p>The startup nodes don’t need to be all the nodes of the cluster. The important\nthing is that at least one node is reachable. Also note that redis-rb-cluster\nupdates this list of startup nodes as soon as it is able to connect with the\nfirst node. You should expect such a behavior with any other serious client.</p><p>Now that we have the Redis Cluster object instance stored in the <strong>rc</strong> variable\nwe are ready to use the object like if it was a normal Redis object instance.</p><p>This is exactly what happens in <strong>line 11 to 19</strong>: when we restart the example\nwe don’t want to start again with <code>foo0</code>, so we store the counter inside\nRedis itself. The code above is designed to read this counter, or if the\ncounter does not exist, to assign it the value of zero.</p><p>However note how it is a while loop, as we want to try again and again even\nif the cluster is down and is returning errors. Normal applications don’t need\nto be so careful.</p><p><strong>Lines between 21 and 30</strong> start the main loop where the keys are set or\nan error is displayed.</p><p>Note the <code>sleep</code> call at the end of the loop. In your tests you can remove\nthe sleep if you want to write to the cluster as fast as possible (relatively\nto the fact that this is a busy loop without real parallelism of course, so\nyou’ll get the usually 10k ops/second in the best of the conditions).</p><p>Normally writes are slowed down in order for the example application to be\neasier to follow by humans.</p><p>Starting the application produces the following output:</p><p>This is not a very interesting program and we’ll use a better one in a moment\nbut we can already see what happens during a resharding when the program\nis running.</p>","link":"/alpha/topics/cluster-tutorial.html","spaLink":"#/alpha/topics/cluster-tutorial","title":"WRITING AN EXAMPLE APP WITH REDIS-RB-CLUSTER"},{"content":"<h2 id=\"creating-and-using-a-redis-cluster-resharding-the-cluster\">Resharding the cluster</h2><p>Now we are ready to try a cluster resharding. To do this please\nkeep the example.rb program running, so that you can see if there is some\nimpact on the program running. Also you may want to comment the <code>sleep</code>\ncall in order to have some more serious write load during resharding.</p><p>Resharding basically means to move hash slots from a set of nodes to another\nset of nodes, and like cluster creation it is accomplished using the\nredis-trib utility.</p><p>To start a resharding just type:</p><p>You only need to specify a single node, redis-trib will find the other nodes\nautomatically.</p><p>Currently redis-trib is only able to reshard with the administrator support,\nyou can’t just say move 5% of slots from this node to the other one (but\nthis is pretty trivial to implement). So it starts with questions. The first\nis how much a big resharding do you want to do:</p><p>We can try to reshard 1000 hash slots, that should already contain a non\ntrivial amount of keys if the example is still running without the sleep\ncall.</p><p>Then redis-trib needs to know what is the target of the resharding, that is,\nthe node that will receive the hash slots.\nI’ll use the first master node, that is, 127.0.0.1:7000, but I need\nto specify the Node ID of the instance. This was already printed in a\nlist by redis-trib, but I can always find the ID of a node with the following\ncommand if I need:</p><p>Ok so my target node is 97a3a64667477371c4479320d683e4c8db5858b1.</p><p>Now you’ll get asked from what nodes you want to take those keys.\nI’ll just type <code>all</code> in order to take a bit of hash slots from all the\nother master nodes.</p><p>After the final confirmation you’ll see a message for every slot that\nredis-trib is going to move from a node to another, and a dot will be printed\nfor every actual key moved from one side to the other.</p><p>While the resharding is in progress you should be able to see your\nexample program running unaffected. You can stop and restart it multiple times\nduring the resharding if you want.</p><p>At the end of the resharding, you can test the health of the cluster with\nthe following command:</p><p>All the slots will be covered as usually, but this time the master at\n127.0.0.1:7000 will have more hash slots, something around 6461.</p>","link":"/alpha/topics/cluster-tutorial.html","spaLink":"#/alpha/topics/cluster-tutorial","title":"RESHARDING THE CLUSTER"},{"content":"<h2 id=\"creating-and-using-a-redis-cluster-scripting-a-resharding-operation\">Scripting a resharding operation</h2><p>Reshardings can be performed automatically without the need to manually\nenter the parameters in an interactive way. This is possible using a command\nline like the following:</p><p>This allows to build some automatism if you are likely to reshard often,\nhowever currently there is no way for <code>redis-trib</code> to automatically\nrebalance the cluster checking the distribution of keys across the cluster\nnodes and intelligently moving slots as needed. This feature will be added\nin the future.</p>","link":"/alpha/topics/cluster-tutorial.html","spaLink":"#/alpha/topics/cluster-tutorial","title":"SCRIPTING A RESHARDING OPERATION"},{"content":"<h2 id=\"creating-and-using-a-redis-cluster-a-more-interesting-example-application\">A more interesting example application</h2><p>The example application we wrote early is not very good.\nIt writes to the cluster in a simple way without even checking if what was\nwritten is the right thing.</p><p>From our point of view the cluster receiving the writes could just always\nwrite the key <code>foo</code> to <code>42</code> to every operation, and we would not notice at\nall.</p><p>So in the <code>redis-rb-cluster</code> repository, there is a more interesting application\nthat is called <code>consistency-test.rb</code>. It uses a set of counters, by default 1000, and sends <code>INCR</code> commands in order to increment the counters.</p><p>However instead of just writing, the application does two additional things:</p><ul>\n<li>When a counter is updated using <code>INCR</code>, the application remembers the write.</li>\n<li>It also reads a random counter before every write, and check if the value is what we expected it to be, comparing it with the value it has in memory.</li>\n</ul><p>What this means is that this application is a simple <strong>consistency checker</strong>,\nand is able to tell you if the cluster lost some write, or if it accepted\na write that we did not receive acknowledgment for. In the first case we’ll\nsee a counter having a value that is smaller than the one we remember, while\nin the second case the value will be greater.</p><p>Running the consistency-test application produces a line of output every\nsecond:</p><p>The line shows the number of <strong>R</strong>eads and <strong>W</strong>rites performed, and the\nnumber of errors (query not accepted because of errors since the system was\nnot available).</p><p>If some inconsistency is found, new lines are added to the output.\nThis is what happens, for example, if I reset a counter manually while\nthe program is running:</p><p>When I set the counter to 0 the real value was 114, so the program reports\n114 lost writes (<code>INCR</code> commands that are not remembered by the cluster).</p><p>This program is much more interesting as a test case, so we’ll use it\nto test the Redis Cluster failover.</p>","link":"/alpha/topics/cluster-tutorial.html","spaLink":"#/alpha/topics/cluster-tutorial","title":"A MORE INTERESTING EXAMPLE APPLICATION"},{"content":"<h2 id=\"creating-and-using-a-redis-cluster-testing-the-failover\">Testing the failover</h2><p>Note: during this test, you should take a tab open with the consistency test\napplication running.</p><p>In order to trigger the failover, the simplest thing we can do (that is also\nthe semantically simplest failure that can occur in a distributed system)\nis to crash a single process, in our case a single master.</p><p>We can identify a cluster and crash it with the following command:</p><p>Ok, so 7000, 7001, and 7002 are masters. Let’s crash node 7002 with the\n<strong>DEBUG SEGFAULT</strong> command:</p><p>Now we can look at the output of the consistency test to see what it reported.</p><p>As you can see during the failover the system was not able to accept 578 reads and 577 writes, however no inconsistency was created in the database. This may\nsound unexpected as in the first part of this tutorial we stated that Redis\nCluster can lose writes during the failover because it uses asynchronous\nreplication. What we did not say is that this is not very likely to happen\nbecause Redis sends the reply to the client, and the commands to replicate\nto the slaves, about at the same time, so there is a very small window to\nlose data. However the fact that it is hard to trigger does not mean that it\nis impossible, so this does not change the consistency guarantees provided\nby Redis cluster.</p><p>We can now check what is the cluster setup after the failover (note that\nin the meantime I restarted the crashed instance so that it rejoins the\ncluster as a slave):</p><p>Now the masters are running on ports 7000, 7001 and 7005. What was previously\na master, that is the Redis instance running on port 7002, is now a slave of\n7005.</p><p>The output of the <code>CLUSTER NODES</code> command may look intimidating, but it is actually pretty simple, and is composed of the following tokens:</p><ul>\n<li>Node ID</li>\n<li>ip:port</li>\n<li>flags: master, slave, myself, fail, …</li>\n<li>if it is a slave, the Node ID of the master</li>\n<li>Time of the last pending PING still waiting for a reply.</li>\n<li>Time of the last PONG received.</li>\n<li>Configuration epoch for this node (see the Cluster specification).</li>\n<li>Status of the link to this node.</li>\n<li>Slots served…</li>\n</ul>","link":"/alpha/topics/cluster-tutorial.html","spaLink":"#/alpha/topics/cluster-tutorial","title":"TESTING THE FAILOVER"},{"content":"<h2 id=\"creating-and-using-a-redis-cluster-manual-failover\">Manual failover</h2><p>Sometimes it is useful to force a failover without actually causing any problem\non a master. For example in order to upgrade the Redis process of one of the\nmaster nodes it is a good idea to failover it in order to turn it into a slave\nwith minimal impact on availability.</p><p>Manual failovers are supported by Redis Cluster using the <code>CLUSTER FAILOVER</code>\ncommand, that must be executed in one of the <strong>slaves</strong> of the master you want\nto failover.</p><p>Manual failovers are special and are safer compared to failovers resulting from\nactual master failures, since they occur in a way that avoid data loss in the\nprocess, by switching clients from the original master to the new master only\nwhen the system is sure that the new master processed all the replication stream\nfrom the old one.</p><p>This is what you see in the slave log when you perform a manual failover:</p><p>Basically clients connected to the master we are failing over are stopped.\nAt the same time the master sends its replication offset to the slave, that\nwaits to reach the offset on its side. When the replication offset is reached,\nthe failover starts, and the old master is informed about the configuration\nswitch. When the clients are unblocked on the old master, they are redirected\nto the new master.</p>","link":"/alpha/topics/cluster-tutorial.html","spaLink":"#/alpha/topics/cluster-tutorial","title":"MANUAL FAILOVER"},{"content":"<h2 id=\"creating-and-using-a-redis-cluster-adding-a-new-node\">Adding a new node</h2><p>Adding a new node is basically the process of adding an empty node and then\nmoving some data into it, in case it is a new master, or telling it to\nsetup as a replica of a known node, in case it is a slave.</p><p>We’ll show both, starting with the addition of a new master instance.</p><p>In both cases the first step to perform is <strong>adding an empty node</strong>.</p><p>This is as simple as to start a new node in port 7006 (we already used\nfrom 7000 to 7005 for our existing 6 nodes) with the same configuration\nused for the other nodes, except for the port number, so what you should\ndo in order to conform with the setup we used for the previous nodes:</p><ul>\n<li>Create a new tab in your terminal application.</li>\n<li>Enter the <code>cluster-test</code> directory.</li>\n<li>Create a directory named <code>7006</code>.</li>\n<li>Create a redis.conf file inside, similar to the one used for the other nodes but using 7006 as port number.</li>\n<li>Finally start the server with <code>../redis-server ./redis.conf</code></li>\n</ul><p>At this point the server should be running.</p><p>Now we can use <strong>redis-trib</strong> as usually in order to add the node to\nthe existing cluster.</p><p>As you can see I used the <strong>add-node</strong> command specifying the address of the\nnew node as first argument, and the address of a random existing node in the\ncluster as second argument.</p><p>In practical terms redis-trib here did very little to help us, it just\nsent a <code>CLUSTER MEET</code> message to the node, something that is also possible\nto accomplish manually. However redis-trib also checks the state of the\ncluster before to operate, so it is a good idea to perform cluster operations\nalways via redis-trib even when you know how the internals work.</p><p>Now we can connect to the new node to see if it really joined the cluster:</p><p>Note that since this node is already connected to the cluster it is already\nable to redirect client queries correctly and is generally speaking part of\nthe cluster. However it has two peculiarities compared to the other masters:</p><ul>\n<li>It holds no data as it has no assigned hash slots.</li>\n<li>Because it is a master without assigned slots, it does not participate in the election process when a slave wants to become a master.</li>\n</ul><p>Now it is possible to assign hash slots to this node using the resharding\nfeature of <code>redis-trib</code>. It is basically useless to show this as we already\ndid in a previous section, there is no difference, it is just a resharding\nhaving as a target the empty node.</p>","link":"/alpha/topics/cluster-tutorial.html","spaLink":"#/alpha/topics/cluster-tutorial","title":"ADDING A NEW NODE"},{"content":"<h2 id=\"creating-and-using-a-redis-cluster-adding-a-new-node-as-a-replica\">Adding a new node as a replica</h2><p>Adding a new Replica can be performed in two ways. The obvious one is to\nuse redis-trib again, but with the —slave option, like this:</p><p>Note that the command line here is exactly like the one we used to add\na new master, so we are not specifying to which master we want to add\nthe replica. In this case what happens is that redis-trib will add the new\nnode as replica of a random master among the masters with less replicas.</p><p>However you can specify exactly what master you want to target with your\nnew replica with the following command line:</p><p>This way we assign the new replica to a specific master.</p><p>A more manual way to add a replica to a specific master is to add the new\nnode as an empty master, and then turn it into a replica using the\n<code>CLUSTER REPLICATE</code> command. This also works if the node was added as a slave\nbut you want to move it as a replica of a different master.</p><p>For example in order to add a replica for the node 127.0.0.1:7005 that is\ncurrently serving hash slots in the range 11423-16383, that has a Node ID\n3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e, all I need to do is to connect\nwith the new node (already added as empty master) and send the command:</p><p>That’s it. Now we have a new replica for this set of hash slots, and all\nthe other nodes in the cluster already know (after a few seconds needed to\nupdate their config). We can verify with the following command:</p><p>The node 3c3a0c… now has two slaves, running on ports 7002 (the existing one) and 7006 (the new one).</p>","link":"/alpha/topics/cluster-tutorial.html","spaLink":"#/alpha/topics/cluster-tutorial","title":"ADDING A NEW NODE AS A REPLICA"},{"content":"<h2 id=\"creating-and-using-a-redis-cluster-removing-a-node\">Removing a node</h2><p>To remove a slave node just use the <code>del-node</code> command of redis-trib:</p><p>The first argument is just a random node in the cluster, the second argument\nis the ID of the node you want to remove.</p><p>You can remove a master node in the same way as well, <strong>however in order to\nremove a master node it must be empty</strong>. If the master is not empty you need\nto reshard data away from it to all the other master nodes before.</p><p>An alternative to remove a master node is to perform a manual failover of it\nover one of its slaves and remove the node after it turned into a slave of the\nnew master. Obviously this does not help when you want to reduce the actual\nnumber of masters in your cluster, in that case, a resharding is needed.</p>","link":"/alpha/topics/cluster-tutorial.html","spaLink":"#/alpha/topics/cluster-tutorial","title":"REMOVING A NODE"},{"content":"<h2 id=\"creating-and-using-a-redis-cluster-replicas-migration\">Replicas migration</h2><p>In Redis Cluster it is possible to reconfigure a slave to replicate with a\ndifferent master at any time just using the following command:</p><p>However there is a special scenario where you want replicas to move from one\nmaster to another one automatically, without the help of the system administrator.\nThe automatic reconfiguration of replicas is called <em>replicas migration</em> and is\nable to improve the reliability of a Redis Cluster.</p><p>Note: you can read the details of replicas migration in the <a href=\"/topics/cluster-spec\">Redis Cluster Specification</a>, here we’ll only provide some information about the\ngeneral idea and what you should do in order to benefit from it.</p><p>The reason why you may want to let your cluster replicas to move from one master\nto another under certain condition, is that usually the Redis Cluster is as\nresistant to failures as the number of replicas attached to a given master.</p><p>For example a cluster where every master has a single replica can’t continue\noperations if the master and its replica fail at the same time, simply because\nthere is no other instance to have a copy of the hash slots the master was\nserving. However while netsplits are likely to isolate a number of nodes\nat the same time, many other kind of failures, like hardware or software failures\nlocal to a single node, are a very notable class of failures that are unlikely\nto happen at the same time, so it is possible that in your cluster where\nevery master has a slave, the slave is killed at 4am, and the master is killed\nat 6am. This still will result in a cluster that can no longer operate.</p><p>To improve reliability of the system we have the option to add additional\nreplicas to every master, but this is expensive. Replica migration allows to\nadd more slaves to just a few masters. So you have 10 masters with 1 slave\neach, for a total of 20 instances. However you add, for example, 3 instances\nmore as slaves of some of your masters, so certain masters will have more\nthan a single slave.</p><p>With replicas migration what happens is that if a master is left without\nslaves, a replica from a master that has multiple slaves will migrate to\nthe <em>orphaned</em> master. So after your slave goes down at 4am as in the example\nwe made above, another slave will take its place, and when the master\nwill fail as well at 5am, there is still a slave that can be elected so that\nthe cluster can continue to operate.</p><p>So what you should know about replicas migration in short?</p><ul>\n<li>The cluster will try to migrate a replica from the master that has the greatest number of replicas in a given moment.</li>\n<li>To benefit from replica migration you have just to add a few more replicas to a single master in your cluster, it does not matter what master.</li>\n<li>There is a configuration parameter that controls the replica migration feature that is called <code>cluster-migration-barrier</code>: you can read more about it in the example <code>redis.conf</code> file provided with Redis Cluster.</li>\n</ul>","link":"/alpha/topics/cluster-tutorial.html","spaLink":"#/alpha/topics/cluster-tutorial","title":"REPLICAS MIGRATION"},{"content":"<h2 id=\"creating-and-using-a-redis-cluster-upgrading-nodes-in-a-redis-cluster\">Upgrading nodes in a Redis Cluster</h2><p>Upgrading slave nodes is easy since you just need to stop the node and restart\nit with an updated version of Redis. If there are clients scaling reads using\nslave nodes, they should be able to reconnect to a different slave if a given\none is not available.</p><p>Upgrading masters is a bit more complex, and the suggested procedure is:</p><p>Following this procedure you should upgrade one node after the other until\nall the nodes are upgraded.</p>","link":"/alpha/topics/cluster-tutorial.html","spaLink":"#/alpha/topics/cluster-tutorial","title":"UPGRADING NODES IN A REDIS CLUSTER"},{"content":"<h2 id=\"creating-and-using-a-redis-cluster-migrating-to-redis-cluster\">Migrating to Redis Cluster</h2><p>Users willing to migrate to Redis Cluster may have just a single master, or\nmay already using a preexisting sharding setup, where keys\nare split among N nodes, using some in-house algorithm or a sharding algorithm\nimplemented by their client library or Redis proxy.</p><p>In both cases it is possible to migrate to Redis Cluster easily, however\nwhat is the most important detail is if multiple-keys operations are used\nby the application, and how. There are three different cases:</p><p>The third case is not handled by Redis Cluster: the application requires to\nbe modified in order to don’t use multi keys operations or only use them in\nthe context of the same hash tag.</p><p>Case 1 and 2 are covered, so we’ll focus on those two cases, that are handled\nin the same way, so no distinction will be made in the documentation.</p><p>Assuming you have your preexisting data set split into N masters, where\nN=1 if you have no preexisting sharding, the following steps are needed\nin order to migrate your data set to Redis Cluster:</p><p>There is an alternative way to import data from external instances to a Redis\nCluster, which is to use the <code>redis-trib import</code> command.</p><p>The command moves all the keys of a running instance (deleting the keys from\nthe source instance) to the specified pre-existing Redis Cluster. However\nnote that if you use a Redis 2.8 instance as source instance the operation\nmay be slow since 2.8 does not implement migrate connection caching, so you\nmay want to restart your source instance with a Redis 3.x version before\nto perform such operation.</p>","link":"/alpha/topics/cluster-tutorial.html","spaLink":"#/alpha/topics/cluster-tutorial","title":"MIGRATING TO REDIS CLUSTER"},{"content":"<h1 id=\"redis-configuration\">Redis configuration</h1><p>Redis is able to start without a configuration file using a built-in default\nconfiguration, however this setup is only recommended for testing and\ndevelopment purposes.</p><p>The proper way to configure Redis is by providing a Redis configuration file,\nusually called <code>redis.conf</code>.</p><p>The <code>redis.conf</code> file contains a number of directives that have a very simple\nformat:</p><p>This is an example of configuration directive:</p><p>It is possible to provide strings containing spaces as arguments using\nquotes, as in the following example:</p><p>The list of configuration directives, and their meaning and intended usage\nis available in the self documented example redis.conf shipped into the\nRedis distribution.</p><ul>\n<li>The self documented <a href=\"https://raw.githubusercontent.com/antirez/redis/3.0/redis.conf\">redis.conf for Redis 3.0</a></li>\n<li>The self documented <a href=\"https://raw.githubusercontent.com/antirez/redis/2.8/redis.conf\">redis.conf for Redis 2.8</a></li>\n<li>The self documented <a href=\"https://raw.githubusercontent.com/antirez/redis/2.6/redis.conf\">redis.conf for Redis 2.6</a>.</li>\n<li>The self documented <a href=\"https://raw.githubusercontent.com/antirez/redis/2.4/redis.conf\">redis.conf for Redis 2.4</a>.</li>\n</ul>","link":"/alpha/topics/config.html","spaLink":"#/alpha/topics/config","title":"REDIS CONFIGURATION"},{"content":"<h2 id=\"redis-configuration-passing-arguments-via-the-command-line\">Passing arguments via the command line</h2><p>Since Redis 2.6 it is possible to also pass Redis configuration parameters\nusing the command line directly. This is very useful for testing purposes.\nThe following is an example that starts a new Redis instance using port 6380\nas a slave of the instance running at 127.0.0.1 port 6379.</p><p>The format of the arguments passed via the command line is exactly the same\nas the one used in the redis.conf file, with the exception that the keyword\nis prefixed with <code>--</code>.</p><p>Note that internally this generates an in-memory temporary config file\n(possibly concatenating the config file passed by the user if any) where\narguments are translated into the format of redis.conf.</p>","link":"/alpha/topics/config.html","spaLink":"#/alpha/topics/config","title":"PASSING ARGUMENTS VIA THE COMMAND LINE"},{"content":"<h2 id=\"redis-configuration-changing-redis-configuration-while-the-server-is-running\">Changing Redis configuration while the server is running</h2><p>It is possible to reconfigure Redis on the fly without stopping and restarting\nthe service, or querying the current configuration programmatically using the\nspecial commands <a href=\"/commands/config-set\">CONFIG SET</a> and\n<a href=\"/commands/config-get\">CONFIG GET</a></p><p>Not all the configuration directives are supported in this way, but most\nare supported as expected. Please refer to the\n<a href=\"/commands/config-set\">CONFIG SET</a> and <a href=\"/commands/config-get\">CONFIG GET</a>\npages for more information.</p><p>Note that modifying the configuration on the fly <strong>has no effects on the\nredis.conf file</strong> so at the next restart of Redis the old configuration will\nbe used instead.</p><p>Make sure to also modify the <code>redis.conf</code> file accordingly to the configuration\nyou set using <a href=\"/commands/config-set\">CONFIG SET</a>. You can do it manually, or starting with Redis 2.8, you can just use <a href=\"/commands/config-rewrite\">CONFIG REWRITE</a>, which will automatically scan your <code>redis.conf</code> file and update the fields which don’t match the current configuration value. Fields non existing but set to the default value are not added. Comments inside your configuration file are retained.</p>","link":"/alpha/topics/config.html","spaLink":"#/alpha/topics/config","title":"CHANGING REDIS CONFIGURATION WHILE THE SERVER IS RUNNING"},{"content":"<h2 id=\"redis-configuration-configuring-redis-as-a-cache\">Configuring Redis as a cache</h2><p>If you plan to use Redis just as a cache where every key will have an\nexpire set, you may consider using the following configuration instead\n(assuming a max memory limit of 2 megabytes as an example):</p><p>In this configuration there is no need for the application to set a\ntime to live for keys using the <code>EXPIRE</code> command (or equivalent) since\nall the keys will be evicted using an approximated LRU algorithm as long\nas we hit the 2 megabyte memory limit.</p><p>Basically in this configuration Redis acts in a similar way to memcached.\nWe have more extensive documentation about <a href=\"/topics/lru-cache\">using Redis as an LRU cache</a>.</p>","link":"/alpha/topics/config.html","spaLink":"#/alpha/topics/config","title":"CONFIGURING REDIS AS A CACHE"},{"content":"<h1 id=\"an-introduction-to-redis-data-types-and-abstractions\">An introduction to Redis data types and abstractions</h1><p>Redis is not a <em>plain</em> key-value store, actually it is a <em>data structures server</em>, supporting different kind of values. What this means is that, while in\ntraditional key-value stores you associated string keys to string values, in\nRedis the value is not limited to a simple string, but can also hold more complex\ndata structures. The following is the list of all the data structures supported\nby Redis, which will be covered separately in this tutorial:</p><ul>\n<li>Binary-safe strings.</li>\n<li>Lists: collections of string elements sorted according to the order of insertion. They are basically <em>linked lists</em>.</li>\n<li>Sets: collections of unique, unsorted string elements.</li>\n<li>Sorted sets, similar to Sets but where every string element is associated to a\nfloating number value, called <em>score</em>. The elements are always taken sorted\nby their score, so unlike Sets it is possible to retrieve a range of elements\n(for example you may ask: give me the top 10, or the bottom 10).</li>\n<li>Hashes, which are maps composed of fields associated with values. Both the\nfield and the value are strings. This is very similar to Ruby or Python\nhashes.</li>\n<li>Bit arrays (or simply bitmaps): it is possible, using special commands, to\nhandle String values like an array of bits: you can set and clear individual\nbits, count all the bits set to 1, find the first set or unset bit, and so\nforth.</li>\n<li>HyperLogLogs: this is a probabilistic data structure which is used in order\nto estimate the cardinality of a set. Don’t be scared, it is simpler than\nit seems… See later in the HyperLogLog section of this tutorial.</li>\n</ul><p>It’s not always trivial to grasp how these data types work and what to use in\norder to solve a given problem from the <a href=\"/commands\">command reference</a>, so this\ndocument is a crash course to Redis data types and their most common patterns.</p><p>For all the examples we’ll use the <code>redis-cli</code> utility, a simple but\nhandy command-line utility, to issue commands against the Redis server.</p>","link":"/alpha/topics/data-types-intro.html","spaLink":"#/alpha/topics/data-types-intro","title":"AN INTRODUCTION TO REDIS DATA TYPES AND ABSTRACTIONS"},{"content":"<h2 id=\"an-introduction-to-redis-data-types-and-abstractions-redis-keys\">Redis keys</h2><p>Redis keys are binary safe, this means that you can use any binary sequence as a\nkey, from a string like “foo” to the content of a JPEG file.\nThe empty string is also a valid key.</p><p>A few other rules about keys:</p><ul>\n<li>Very long keys are not a good idea. For instance a key of 1024 bytes is a bad\nidea not only memory-wise, but also because the lookup of the key in the\ndataset may require several costly key-comparisons. Even when the task at hand\nis to match the existence of a large value, hashing it (for example\nwith SHA1) is a better idea, especially from the perspective of memory\nand bandwidth.</li>\n<li>Very short keys are often not a good idea. There is little point in writing\n“u1000flw” as a key if you can instead write “user:1000:followers”.  The latter\nis more readable and the added space is minor compared to the space used by\nthe key object itself and the value object. While short keys will obviously\nconsume a bit less memory, your job is to find the right balance.</li>\n<li>Try to stick with a schema. For instance “object-type:id” is a good\nidea, as in “user:1000”. Dots or dashes are often used for multi-word\nfields, as in “comment:1234:reply.to” or “comment:1234:reply-to”.</li>\n<li>The maximum allowed key size is 512 MB.</li>\n</ul><p><a name=\"strings\"></a></p>","link":"/alpha/topics/data-types-intro.html","spaLink":"#/alpha/topics/data-types-intro","title":"REDIS KEYS"},{"content":"<h2 id=\"an-introduction-to-redis-data-types-and-abstractions-redis-strings\">Redis Strings</h2><p>The Redis String type is the simplest type of value you can associate with\na Redis key. It is the only data type in Memcached, so it is also very natural\nfor newcomers to use it in Redis.</p><p>Since Redis keys are strings, when we use the string type as a value too,\nwe are mapping a string to another string. The string data type is useful\nfor a number of use cases, like caching HTML fragments or pages.</p><p>Let’s play a bit with the string type, using <code>redis-cli</code> (all the examples\nwill be performed via <code>redis-cli</code> in this tutorial).</p><p>As you can see using the <code>SET</code> and the <code>GET</code> commands are the way we set\nand retrieve a string value. Note that <code>SET</code> will replace any existing value\nalready stored into the key, in the case that the key already exists, even if\nthe key is associated with a non-string value. So <code>SET</code> performs an assignment.</p><p>Values can be strings (including binary data) of every kind, for instance you\ncan store a jpeg image inside a key. A value can’t be bigger than 512 MB.</p><p>The <code>SET</code> command has interesting options, that are provided as additional\narguments. For example, I may ask <code>SET</code> to fail if the key already exists,\nor the opposite, that it only succeed if the key already exists:</p><p>Even if strings are the basic values of Redis, there are interesting operations\nyou can perform with them. For instance, one is atomic increment:</p><p>The <a href=\"/commands/incr\">INCR</a> command parses the string value as an integer,\nincrements it by one, and finally sets the obtained value as the new value.\nThere are other similar commands like <a href=\"/commands/incrby\">INCRBY</a>,\n<a href=\"/commands/decr\">DECR</a> and <a href=\"/commands/decrby\">DECRBY</a>. Internally it’s\nalways the same command, acting in a slightly different way.</p><p>What does it mean that INCR is atomic?\nThat even multiple clients issuing INCR against\nthe same key will never enter into a race condition. For instance, it will never\nhappen that client 1 reads “10”, client 2 reads “10” at the same time, both\nincrement to 11, and set the new value to 11. The final value will always be\n12 and the read-increment-set operation is performed while all the other\nclients are not executing a command at the same time.</p><p>There are a number of commands for operating on strings. For example\nthe <code>GETSET</code> command sets a key to a new value, returning the old value as the\nresult. You can use this command, for example, if you have a\nsystem that increments a Redis key using <code>INCR</code>\nevery time your web site receives a new visitor. You may want to collect this\ninformation once every hour, without losing a single increment.\nYou can <code>GETSET</code> the key, assigning it the new value of “0” and reading the\nold value back.</p><p>The ability to set or retrieve the value of multiple keys in a single\ncommand is also useful for reduced latency. For this reason there are\nthe <code>MSET</code> and <code>MGET</code> commands:</p><p>When <code>MGET</code> is used, Redis returns an array of values.</p>","link":"/alpha/topics/data-types-intro.html","spaLink":"#/alpha/topics/data-types-intro","title":"REDIS STRINGS"},{"content":"<h2 id=\"an-introduction-to-redis-data-types-and-abstractions-altering-and-querying-the-key-space\">Altering and querying the key space</h2><p>There are commands that are not defined on particular types, but are useful\nin order to interact with the space of keys, and thus, can be used with\nkeys of any type.</p><p>For example the <code>EXISTS</code> command returns 1 or 0 to signal if a given key\nexists or not in the database, while the <code>DEL</code> command deletes a key\nand associated value, whatever the value is.</p><p>From the examples you can also see how <code>DEL</code> itself returns 1 or 0 depending on whether\nthe key was removed (it existed) or not (there was no such key with that\nname).</p><p>There are many key space related commands, but the above two are the\nessential ones together with the <code>TYPE</code> command, which returns the kind\nof value stored at the specified key:</p>","link":"/alpha/topics/data-types-intro.html","spaLink":"#/alpha/topics/data-types-intro","title":"ALTERING AND QUERYING THE KEY SPACE"},{"content":"<h2 id=\"an-introduction-to-redis-data-types-and-abstractions-redis-expires-keys-with-limited-time-to-live\">Redis expires: keys with limited time to live</h2><p>Before continuing with more complex data structures, we need to discuss\nanother feature which works regardless of the value type, and is\ncalled <strong>Redis expires</strong>. Basically you can set a timeout for a key, which\nis a limited time to live. When the time to live elapses, the key is\nautomatically destroyed, exactly as if the user called the <code>DEL</code> command\nwith the key.</p><p>A few quick info about Redis expires:</p><ul>\n<li>They can be set both using seconds or milliseconds precision.</li>\n<li>However the expire time resolution is always 1 millisecond.</li>\n<li>Information about expires are replicated and persisted on disk, the time virtually passes when your Redis server remains stopped (this means that Redis saves the date at which a key will expire).</li>\n</ul><p>Setting an expire is trivial:</p><p>The key vanished between the two <code>GET</code> calls, since the second call was\ndelayed more than 5 seconds. In the example above we used <code>EXPIRE</code> in\norder to set the expire (it can also be used in order to set a different\nexpire to a key already having one, like <code>PERSIST</code> can be used in order\nto remove the expire and make the key persistent forever). However we\ncan also create keys with expires using other Redis commands. For example\nusing <code>SET</code> options:</p><p>The example above sets a key with the string value <code>100</code>, having an expire\nof ten seconds. Later the <code>TTL</code> command is called in order to check the\nremaining time to live for the key.</p><p>In order to set and check expires in milliseconds, check the <code>PEXPIRE</code> and\nthe <code>PTTL</code> commands, and the full list of <code>SET</code> options.</p><p><a name=\"lists\"></a></p>","link":"/alpha/topics/data-types-intro.html","spaLink":"#/alpha/topics/data-types-intro","title":"REDIS EXPIRES: KEYS WITH LIMITED TIME TO LIVE"},{"content":"<h2 id=\"an-introduction-to-redis-data-types-and-abstractions-redis-lists\">Redis Lists</h2><p>To explain the List data type it’s better to start with a little bit of theory,\nas the term <em>List</em> is often used in an improper way by information technology\nfolks. For instance “Python Lists” are not what the name may suggest (Linked\nLists), but rather Arrays (the same data type is called Array in\nRuby actually).</p><p>From a very general point of view a List is just a sequence of ordered\nelements: 10,20,1,2,3 is a list. But the properties of a List implemented using\nan Array are very different from the properties of a List implemented using a\n<em>Linked List</em>.</p><p>Redis lists are implemented via Linked Lists. This means that even if you have\nmillions of elements inside a list, the operation of adding a new element in\nthe head or in the tail of the list is performed <em>in constant time</em>. The speed of adding a\nnew element with the <code>LPUSH</code> command to the head of a list with ten\nelements is the same as adding an element to the head of list with 10\nmillion elements.</p><p>What’s the downside? Accessing an element <em>by index</em> is very fast in lists\nimplemented with an Array (constant time indexed access) and not so fast in\nlists implemented by linked lists (where the operation requires an amount of\nwork proportional to the index of the accessed element).</p><p>Redis Lists are implemented with linked lists because for a database system it\nis crucial to be able to add elements to a very long list in a very fast way.\nAnother strong advantage, as you’ll see in a moment, is that Redis Lists can be\ntaken at constant length in constant time.</p><p>When fast access to the middle of a large collection of elements is important,\nthere is a different data structure that can be used, called sorted sets.\nSorted sets will be covered later in this tutorial.</p>","link":"/alpha/topics/data-types-intro.html","spaLink":"#/alpha/topics/data-types-intro","title":"REDIS LISTS"},{"content":"<h2 id=\"an-introduction-to-redis-data-types-and-abstractions-first-steps-with-redis-lists\">First steps with Redis Lists</h2><p>The <code>LPUSH</code> command adds a new element into a list, on the\nleft (at the head), while the <code>RPUSH</code> command adds a new\nelement into a list ,on the right (at the tail). Finally the\n<code>LRANGE</code> command extracts ranges of elements from lists:</p><p>Note that <a href=\"/commands/lrange\">LRANGE</a> takes two indexes, the first and the last\nelement of the range to return. Both the indexes can be negative, telling Redis\nto start counting from the end: so -1 is the last element, -2 is the\npenultimate element of the list, and so forth.</p><p>As you can see <code>RPUSH</code> appended the elements on the right of the list, while\nthe final <code>LPUSH</code> appended the element on the left.</p><p>Both commands are <em>variadic commands</em>, meaning that you are free to push\nmultiple elements into a list in a single call:</p><p>An important operation defined on Redis lists is the ability to <em>pop elements</em>.\nPopping elements is the operation of both retrieving the element from the list,\nand eliminating it from the list, at the same time. You can pop elements\nfrom left and right, similarly to how you can push elements in both sides\nof the list:</p><p>We added three elements and popped three elements, so at the end of this\nsequence of commands the list is empty and there are no more elements to\npop. If we try to pop yet another element, this is the result we get:</p><p>Redis returned a NULL value to signal that there are no elements into the\nlist.</p>","link":"/alpha/topics/data-types-intro.html","spaLink":"#/alpha/topics/data-types-intro","title":"FIRST STEPS WITH REDIS LISTS"},{"content":"<h2 id=\"an-introduction-to-redis-data-types-and-abstractions-common-use-cases-for-lists\">Common use cases for lists</h2><p>Lists are useful for a number of tasks, two very representative use cases\nare the following:</p><ul>\n<li>Remember the latest updates posted by users into a social network.</li>\n<li>Communication between processes, using a consumer-producer pattern where the producer pushes items into a list, and a consumer (usually a <em>worker</em>) consumes those items and executed actions. Redis has special list commands to make this use case both more reliable and efficient.</li>\n</ul><p>For example both the popular Ruby libraries <a href=\"https://github.com/resque/resque\">resque</a> and\n<a href=\"https://github.com/mperham/sidekiq\">sidekiq</a> use Redis lists under the hood in order to\nimplement background jobs.</p><p>The popular Twitter social network <a href=\"http://www.infoq.com/presentations/Real-Time-Delivery-Twitter\">takes the latest tweets</a>\nposted by users into Redis lists.</p><p>To describe a common use case step by step, imagine your home page shows the latest\nphotos published in a photo sharing social network and you want to speedup access.</p><ul>\n<li>Every time a user posts a new photo, we add its ID into a list with <code>LPUSH</code>.</li>\n<li>When users visit the home page, we use <code>LRANGE 0 9</code> in order to get the latest 10 posted items.</li>\n</ul>","link":"/alpha/topics/data-types-intro.html","spaLink":"#/alpha/topics/data-types-intro","title":"COMMON USE CASES FOR LISTS"},{"content":"<h2 id=\"an-introduction-to-redis-data-types-and-abstractions-capped-lists\">Capped lists</h2><p>In many use cases we just want to use lists to store the <em>latest items</em>,\nwhatever they are: social network updates, logs, or anything else.</p><p>Redis allows us to use lists as a capped collection, only remembering the latest\nN items and discarding all the oldest items using the <code>LTRIM</code> command.</p><p>The <code>LTRIM</code> command is similar to <code>LRANGE</code>, but <strong>instead of displaying the\nspecified range of elements</strong> it sets this range as the new list value. All\nthe elements outside the given range are removed.</p><p>An example will make it more clear:</p><p>The above <code>LTRIM</code> command tells Redis to take just list elements from index\n0 to 2, everything else will be discarded. This allows for a very simple but\nuseful pattern: doing a List push operation + a List trim operation together\nin order to add a new element and discard elements exceeding a limit:</p><p>The above combination adds a new element and takes only the 1000\nnewest elements into the list. With <code>LRANGE</code> you can access the top items\nwithout any need to remember very old data.</p><p>Note: while <code>LRANGE</code> is technically an O(N) command, accessing small ranges\ntowards the head or the tail of the list is a constant time operation.</p>","link":"/alpha/topics/data-types-intro.html","spaLink":"#/alpha/topics/data-types-intro","title":"CAPPED LISTS"},{"content":"<h2 id=\"an-introduction-to-redis-data-types-and-abstractions-blocking-operations-on-lists\">Blocking operations on lists</h2><p>Lists have a special feature that make them suitable to implement queues,\nand in general as a building block for inter process communication systems:\nblocking operations.</p><p>Imagine you want to push items into a list with one process, and use\na different process in order to actually do some kind of work with those\nitems. This is the usual producer / consumer setup, and can be implemented\nin the following simple way:</p><ul>\n<li>To push items into the list, producers call <code>LPUSH</code>.</li>\n<li>To extract / process items from the list, consumers call <code>RPOP</code>.</li>\n</ul><p>However it is possible that sometimes the list is empty and there is nothing\nto process, so <code>RPOP</code> just returns NULL. In this case a consumer is forced to wait\nsome time and retry again with <code>RPOP</code>. This is called <em>polling</em>, and is not\na good idea in this context because it has several drawbacks:</p><p>So Redis implements commands called <code>BRPOP</code> and <code>BLPOP</code> which are versions\nof <code>RPOP</code> and <code>LPOP</code> able to block if the list is empty: they’ll return to\nthe caller only when a new element is added to the list, or when a user-specified\ntimeout is reached.</p><p>This is an example of a <code>BRPOP</code> call we could use in the worker:</p><p>It means: “wait for elements in the list <code>tasks</code>, but return if after 5 seconds\nno element is available”.</p><p>Note that you can use 0 as timeout to wait for elements forever, and you can\nalso specify multiple lists and not just one, in order to wait on multiple\nlists at the same time, and get notified when the first list receives an\nelement.</p><p>A few things to note about <code>BRPOP</code>:</p><p>There are more things you should know about lists and blocking ops. We\nsuggest that you read more on the following:</p><ul>\n<li>It is possible to build safer queues or rotating queues using <code>RPOPLPUSH</code>.</li>\n<li>There is also a blocking variant of the command, called <code>BRPOPLPUSH</code>.</li>\n</ul>","link":"/alpha/topics/data-types-intro.html","spaLink":"#/alpha/topics/data-types-intro","title":"BLOCKING OPERATIONS ON LISTS"},{"content":"<h2 id=\"an-introduction-to-redis-data-types-and-abstractions-automatic-creation-and-removal-of-keys\">Automatic creation and removal of keys</h2><p>So far in our examples we never had to create empty lists before pushing\nelements, or removing empty lists when they no longer have elements inside.\nIt is Redis’ responsibility to delete keys when lists are left empty, or to create\nan empty list if the key does not exist and we are trying to add elements\nto it, for example, with <code>LPUSH</code>.</p><p>This is not specific to lists, it applies to all the Redis data types\ncomposed of multiple elements — Sets, Sorted Sets and Hashes.</p><p>Basically we can summarize the behavior with three rules:</p><p>Examples of rule 1:</p><p>However we can’t perform operations against the wrong type of the key exists:</p><p>Example of rule 2:</p><p>The key no longer exists after all the elements are popped.</p><p>Example of rule 3:</p><p><a name=\"hashes\"></a></p>","link":"/alpha/topics/data-types-intro.html","spaLink":"#/alpha/topics/data-types-intro","title":"AUTOMATIC CREATION AND REMOVAL OF KEYS"},{"content":"<h2 id=\"an-introduction-to-redis-data-types-and-abstractions-redis-hashes\">Redis Hashes</h2><p>Redis hashes look exactly how one might expect a “hash” to look, with field-value pairs:</p><p>While hashes are handy to represent <em>objects</em>, actually the number of fields you can\nput inside a hash has no practical limits (other than available memory), so you can use\nhashes in many different ways inside your application.</p><p>The command <code>HMSET</code> sets multiple fields of the hash, while <code>HGET</code> retrieves\na single field. <code>HMGET</code> is similar to <code>HGET</code> but returns an array of values:</p><p>There are commands that are able to perform operations on individual fields\nas well, like <code>HINCRBY</code>:</p><p>You can find the <a href=\"http://redis.io/commands#hash\">full list of hash commands in the documentation</a>.</p><p>It is worth noting that small hashes (i.e., a few elements with small values) are\nencoded in special way in memory that make them very memory efficient.</p><p><a name=\"sets\"></a></p>","link":"/alpha/topics/data-types-intro.html","spaLink":"#/alpha/topics/data-types-intro","title":"REDIS HASHES"},{"content":"<h2 id=\"an-introduction-to-redis-data-types-and-abstractions-redis-sets\">Redis Sets</h2><p>Redis Sets are unordered collections of strings. The\n<code>SADD</code> command adds new elements to a set. It’s also possible\nto do a number of other operations against sets like testing if a given element\nalready exists, performing the intersection, union or difference between\nmultiple sets, and so forth.</p><p>Here I’ve added three elements to my set and told Redis to return all the\nelements. As you can see they are not sorted — Redis is free to return the\nelements in any order at every call, since there is no contract with the\nuser about element ordering.</p><p>Redis has commands to test for membership. For example, checking if an element exists:</p><p>“3” is a member of the set, while “30” is not.</p><p>Sets are good for expressing relations between objects.\nFor instance we can easily use sets in order to implement tags.</p><p>A simple way to model this problem is to have a set for every object we\nwant to tag. The set contains the IDs of the tags associated with the object.</p><p>One illustration is tagging news articles.\nIf article ID 1000 is tagged with tags 1, 2, 5 and 77, a set\ncan associate these tag IDs with the news item:</p><p>We may also want to have the inverse relation as well: the list\nof all the news tagged with a given tag:</p><p>To get all the tags for a given object is trivial:</p><p>Note: in the example we assume you have another data structure, for example\na Redis hash, which maps tag IDs to tag names.</p><p>There are other non trivial operations that are still easy to implement\nusing the right Redis commands. For instance we may want a list of all the\nobjects with the tags 1, 2, 10, and 27 together. We can do this using\nthe <code>SINTER</code> command, which performs the intersection between different\nsets. We can use:</p><p>In addition to intersection you can also perform\nunions, difference, extract a random element, and so forth.</p><p>The command to extract an element is called <code>SPOP</code>, and is handy to model\ncertain problems. For example in order to implement a web-based poker game,\nyou may want to represent your deck with a set. Imagine we use a one-char\nprefix for (C)lubs, (D)iamonds, (H)earts, (S)pades:</p><p>Now we want to provide each player with 5 cards. The <code>SPOP</code> command\nremoves a random element, returning it to the client, so it is the\nperfect operation in this case.</p><p>However if we call it against our deck directly, in the next play of the\ngame we’ll need to populate the deck of cards again, which may not be\nideal. So to start, we can make a copy of the set stored in the <code>deck</code> key\ninto the <code>game:1:deck</code> key.</p><p>This is accomplished using <code>SUNIONSTORE</code>, which normally performs the\nunion between multiple sets, and stores the result into another set.\nHowever, since the union of a single set is itself, I can copy my deck\nwith:</p><p>Now I’m ready to provide the first player with five cards:</p><p>One pair of jacks, not great…</p><p>This is a good time to introduce the set command that provides the number\nof elements inside a set. This is often called the <em>cardinality of a set</em>\nin the context of set theory, so the Redis command is called <code>SCARD</code>.</p><p>The math works: 52 - 5 = 47.</p><p>When you need to just get random elements without removing them from the\nset, there is the <code>SRANDMEMBER</code> command suitable for the task. It also features\nthe ability to return both repeating and non-repeating elements.</p><p><a name=\"sorted-sets\"></a></p>","link":"/alpha/topics/data-types-intro.html","spaLink":"#/alpha/topics/data-types-intro","title":"REDIS SETS"},{"content":"<h2 id=\"an-introduction-to-redis-data-types-and-abstractions-redis-sorted-sets\">Redis Sorted sets</h2><p>Sorted sets are a data type which is similar to a mix between a Set and\na Hash. Like sets, sorted sets are composed of unique, non-repeating\nstring elements, so in some sense a sorted set is a set as well.</p><p>However while elements inside sets are not ordered, every element in\na sorted set is associated with a floating point value, called <em>the score</em>\n(this is why the type is also similar to a hash, since every element\nis mapped to a value).</p><p>Moreover, elements in a sorted sets are <em>taken in order</em> (so they are not\nordered on request, order is a peculiarity of the data structure used to\nrepresent sorted sets). They are ordered according to the following rule:</p><ul>\n<li>If A and B are two elements with a different score, then A &gt; B if A.score is &gt; B.score.</li>\n<li>If A and B have exactly the same score, then A &gt; B if the A string is lexicographically greater than the B string. A and B strings can’t be equal since sorted sets only have unique elements.</li>\n</ul><p>Let’s start with a simple example, adding a few selected hackers names as\nsorted set elements, with their year of birth as “score”.</p><p>As you can see <code>ZADD</code> is similar to <code>SADD</code>, but takes one additional argument\n(placed before the element to be added) which is the score.\n<code>ZADD</code> is also variadic, so you are free to specify multiple score-value\npairs, even if this is not used in the example above.</p><p>With sorted sets it is trivial to return a list of hackers sorted by their\nbirth year because actually <em>they are already sorted</em>.</p><p>Implementation note: Sorted sets are implemented via a\ndual-ported data structure containing both a skip list and a hash table, so\nevery time we add an element Redis performs an O(log(N)) operation. That’s\ngood, but when we ask for sorted elements Redis does not have to do any work at\nall, it’s already all sorted:</p><p>Note: 0 and -1 means from element index 0 to the last element (-1 works\nhere just as it does in the case of the <code>LRANGE</code> command).</p><p>What if I want to order them the opposite way, youngest to oldest?\nUse <a href=\"/commands/zrevrange\">ZREVRANGE</a> instead of <a href=\"/commands/zrange\">ZRANGE</a>:</p><p>It is possible to return scores as well, using the <code>WITHSCORES</code> argument:</p>","link":"/alpha/topics/data-types-intro.html","spaLink":"#/alpha/topics/data-types-intro","title":"REDIS SORTED SETS"},{"content":"<h2 id=\"an-introduction-to-redis-data-types-and-abstractions-operating-on-ranges\">Operating on ranges</h2><p>Sorted sets are more powerful than this. They can operate on ranges.\nLet’s get all the individuals that were born up to 1950 inclusive. We\nuse the <code>ZRANGEBYSCORE</code> command to do it:</p><p>We asked Redis to return all the elements with a score between negative\ninfinity and 1950 (both extremes are included).</p><p>It’s also possible to remove ranges of elements. Let’s remove all\nthe hackers born between 1940 and 1960 from the sorted set:</p><p><code>ZREMRANGEBYSCORE</code> is perhaps not the best command name,\nbut it can be very useful, and returns the number of removed elements.</p><p>Another extremely useful operation defined for sorted set elements\nis the get-rank operation. It is possible to ask what is the\nposition of an element in the set of the ordered elements.</p><p>The <code>ZREVRANK</code> command is also available in order to get the rank, considering\nthe elements sorted a descending way.</p>","link":"/alpha/topics/data-types-intro.html","spaLink":"#/alpha/topics/data-types-intro","title":"OPERATING ON RANGES"},{"content":"<h2 id=\"an-introduction-to-redis-data-types-and-abstractions-lexicographical-scores\">Lexicographical scores</h2><p>With recent versions of Redis 2.8, a new feature was introduced that allows\ngetting ranges lexicographically, assuming elements in a sorted set are all\ninserted with the same identical score (elements are compared with the C\n<code>memcmp</code> function, so it is guaranteed that there is no collation, and every\nRedis instance will reply with the same output).</p><p>The main commands to operate with lexicographical ranges are <code>ZRANGEBYLEX</code>,\n<code>ZREVRANGEBYLEX</code>, <code>ZREMRANGEBYLEX</code> and <code>ZLEXCOUNT</code>.</p><p>For example, let’s add again our list of famous hackers, but this time\nuse a score of zero for all the elements:</p><p>Because of the sorted sets ordering rules, they are already sorted\nlexicographically:</p><p>Using <code>ZRANGEBYLEX</code> we can ask for lexicographical ranges:</p><p>Ranges can be inclusive or exclusive (depending on the first character),\nalso string infinite and minus infinite are specified respectively with\nthe <code>+</code> and <code>-</code> strings. See the documentation for more information.</p><p>This feature is important because it allows us to use sorted sets as a generic\nindex. For example, if you want to index elements by a 128-bit unsigned\ninteger argument, all you need to do is to add elements into a sorted\nset with the same score (for example 0) but with an 8 byte prefix\nconsisting of <strong>the 128 bit number in big endian</strong>. Since numbers in big\nendian, when ordered lexicographically (in raw bytes order) are actually\nordered numerically as well, you can ask for ranges in the 128 bit space,\nand get the element’s value discarding the prefix.</p><p>If you want to see the feature in the context of a more serious demo,\ncheck the <a href=\"http://autocomplete.redis.io\">Redis autocomplete demo</a>.</p>","link":"/alpha/topics/data-types-intro.html","spaLink":"#/alpha/topics/data-types-intro","title":"LEXICOGRAPHICAL SCORES"},{"content":"<h2 id=\"an-introduction-to-redis-data-types-and-abstractions-updating-the-score-leader-boards\">Updating the score: leader boards</h2><p>Just a final note about sorted sets before switching to the next topic.\nSorted sets’ scores can be updated at any time. Just calling <code>ZADD</code> against\nan element already included in the sorted set will update its score\n(and position) with O(log(N)) time complexity.  As such, sorted sets are suitable\nwhen there are tons of updates.</p><p>Because of this characteristic a common use case is leader boards.\nThe typical application is a Facebook game where you combine the ability to\ntake users sorted by their high score, plus the get-rank operation, in order\nto show the top-N users, and the user rank in the leader board (e.g., “you are\nthe #4932 best score here”).</p><p><a name=\"bitmaps\"></a></p>","link":"/alpha/topics/data-types-intro.html","spaLink":"#/alpha/topics/data-types-intro","title":"UPDATING THE SCORE: LEADER BOARDS"},{"content":"<h2 id=\"an-introduction-to-redis-data-types-and-abstractions-bitmaps\">Bitmaps</h2><p>Bitmaps are not an actual data type, but a set of bit-oriented operations\ndefined on the String type. Since strings are binary safe blobs and their\nmaximum length is 512 MB, they are suitable to set up to 2^32 different\nbits.</p><p>Bit operations are divided into two groups: constant-time single bit\noperations, like setting a bit to 1 or 0, or getting its value, and\noperations on groups of bits, for example counting the number of set\nbits in a given range of bits (e.g., population counting).</p><p>One of the biggest advantages of bitmaps is that they often provide\nextreme space savings when storing information. For example in a system\nwhere different users are represented by incremental user IDs, it is possible\nto remember a single bit information (for example, knowing whether\na user wants to receive a newsletter) of 4 billion of users using just 512 MB of memory.</p><p>Bits are set and retrieved using the <code>SETBIT</code> and <code>GETBIT</code> commands:</p><p>The <code>SETBIT</code> command takes as its first argument the bit number, and as its second\nargument the value to set the bit to, which is 1 or 0. The command\nautomatically enlarges the string if the addressed bit is outside the\ncurrent string length.</p><p><code>GETBIT</code> just returns the value of the bit at the specified index.\nOut of range bits (addressing a bit that is outside the length of the string\nstored into the target key) are always considered to be zero.</p><p>There are three commands operating on group of bits:</p><p>Both <code>BITPOS</code> and <code>BITCOUNT</code> are able to operate with byte ranges of the\nstring, instead of running for the whole length of the string. The following\nis a trivial example of <code>BITCOUNT</code> call:</p><p>Common user cases for bitmaps are:</p><ul>\n<li>Real time analytics of all kinds.</li>\n<li>Storing space efficient but high performance boolean information associated with object IDs.</li>\n</ul><p>For example imagine you want to know the longest streak of daily visits of\nyour web site users. You start counting days starting from zero, that is the\nday you made your web site public, and set a bit with <code>SETBIT</code> every time\nthe user visits the web site. As a bit index you simply take the current unix\ntime, subtract the initial offset, and divide by 3600*24.</p><p>This way for each user you have a small string containing the visit\ninformation for each day. With <code>BITCOUNT</code> it is possible to easily get\nthe number of days a given user visited the web site, while with\na few <code>BITPOS</code> calls, or simply fetching and analyzing the bitmap client-side,\nit is possible to easily compute the longest streak.</p><p>Bitmaps are trivial to split into multiple keys, for example for\nthe sake of sharding the data set and because in general it is better to\navoid working with huge keys. To split a bitmap across different keys\ninstead of setting all the bits into a key, a trivial strategy is just\nto store M bits per key and obtain the key name with <code>bit-number/M</code> and\nthe Nth bit to address inside the key with <code>bit-number MOD M</code>.</p><p><a name=\"hyperloglogs\"></a></p>","link":"/alpha/topics/data-types-intro.html","spaLink":"#/alpha/topics/data-types-intro","title":"BITMAPS"},{"content":"<h2 id=\"an-introduction-to-redis-data-types-and-abstractions-hyperloglogs\">HyperLogLogs</h2><p>A HyperLogLog is a probabilistic data structure used in order to count\nunique things (technically this is referred to estimating the cardinality\nof a set). Usually counting unique items requires using an amount of memory\nproportional to the number of items you want to count, because you need\nto remember the elements you have already seen in the past in order to avoid\ncounting them multiple times. However there is a set of algorithms that trade\nmemory for precision: you end with an estimated measure with a standard error,\nin the case of the Redis implementation, which is less than 1%.  The\nmagic of this algorithm is that you no longer need to use an amount of memory\nproportional to the number of items counted, and instead can use a\nconstant amount of memory! 12k bytes in the worst case, or a lot less if your\nHyperLogLog (We’ll just call them HLL from now) has seen very few elements.</p><p>HLLs in Redis, while technically a different data structure, is encoded\nas a Redis string, so you can call <code>GET</code> to serialize a HLL, and <code>SET</code>\nto deserialize it back to the server.</p><p>Conceptually the HLL API is like using Sets to do the same task. You would\n<code>SADD</code> every observed element into a set, and would use <code>SCARD</code> to check the\nnumber of elements inside the set, which are unique since <code>SADD</code> will not\nre-add an existing element.</p><p>While you don’t really <em>add items</em> into an HLL, because the data structure\nonly contains a state that does not include actual elements, the API is the\nsame:</p><ul>\n<li>Every time you see a new element, you add it to the count with <code>PFADD</code>.</li>\n<li><p>Every time you want to retrieve the current approximation of the unique elements <em>added</em> with <code>PFADD</code> so far, you use the <code>PFCOUNT</code>.</p>\n<pre><code class=\"prettyprint prettyprinted\" style=\"\"><span class=\"pln\">  </span><span class=\"pun\">&gt;</span><span class=\"pln\"> pfadd hll a b c d\n  </span><span class=\"pun\">(</span><span class=\"pln\">integer</span><span class=\"pun\">)</span><span class=\"pln\"> </span><span class=\"lit\">1</span><span class=\"pln\">\n  </span><span class=\"pun\">&gt;</span><span class=\"pln\"> pfcount hll\n  </span><span class=\"pun\">(</span><span class=\"pln\">integer</span><span class=\"pun\">)</span><span class=\"pln\"> </span><span class=\"lit\">4</span></code></pre>\n</li>\n</ul><p>Every time you want to retrieve the current approximation of the unique elements <em>added</em> with <code>PFADD</code> so far, you use the <code>PFCOUNT</code>.</p><p>An example of use case for this data structure is counting unique queries\nperformed by users in a search form every day.</p><p>Redis is also able to perform the union of HLLs, please check the\n<a href=\"/commands#hyperloglog\">full documentation</a> for more information.</p>","link":"/alpha/topics/data-types-intro.html","spaLink":"#/alpha/topics/data-types-intro","title":"HYPERLOGLOGS"},{"content":"<h2 id=\"an-introduction-to-redis-data-types-and-abstractions-other-notable-features\">Other notable features</h2><p>There are other important things in the Redis API that can’t be explored\nin the context of this document, but are worth your attention:</p><ul>\n<li>It is possible to <a href=\"/commands/scan\">iterate the key space of a large collection incrementally</a>.</li>\n<li>It is possible to run <a href=\"/commands/eval\">Lua scripts server side</a> to win latency and bandwidth.</li>\n<li>Redis is also a <a href=\"/topics/pubsub\">Pub-Sub server</a>.</li>\n</ul>","link":"/alpha/topics/data-types-intro.html","spaLink":"#/alpha/topics/data-types-intro","title":"OTHER NOTABLE FEATURES"},{"content":"<h2 id=\"an-introduction-to-redis-data-types-and-abstractions-learn-more\">Learn more</h2><p>This tutorial is in no way complete and has covered just the basics of the API.\nRead the <a href=\"/commands\">command reference</a> to discover a lot more.</p><p>Thanks for reading, and have fun hacking with Redis!</p>","link":"/alpha/topics/data-types-intro.html","spaLink":"#/alpha/topics/data-types-intro","title":"LEARN MORE"},{"content":"<h1 id=\"data-types\">Data types</h1><p><a name=\"strings\"></a></p>","link":"/alpha/topics/data-types.html","spaLink":"#/alpha/topics/data-types","title":"DATA TYPES"},{"content":"<h2 id=\"data-types-strings\">Strings</h2><p>Strings are the most basic kind of Redis value. Redis Strings are binary safe, this means that a Redis string can contain any kind of data, for instance a\nJPEG image or a serialized Ruby object.</p><p>A String value can be at max 512 Megabytes in length.</p><p>You can do a number of interesting things using strings in Redis, for instance you can:</p><ul>\n<li>Use Strings as atomic counters using commands in the INCR family: <a href=\"/commands/incr\">INCR</a>, <a href=\"/commands/decr\">DECR</a>, <a href=\"/commands/incrby\">INCRBY</a>.</li>\n<li>Append to strings with the <a href=\"/commands/append\">APPEND</a> command.</li>\n<li>Use Strings as a random access vectors with <a href=\"/commands/getrange\">GETRANGE</a> and <a href=\"/commands/setrange\">SETRANGE</a>.</li>\n<li>Encode a lot of data in little space, or create a Redis backed Bloom Filter using <a href=\"/commands/getbit\">GETBIT</a> and <a href=\"/commands/setbit\">SETBIT</a>.</li>\n</ul><p>Check all the <a href=\"/commands/#string\">available string commands</a> for more information, or read the <a href=\"/topics/data-types-intro\">introduction to Redis data types</a>.</p><p><a name=\"lists\"></a></p>","link":"/alpha/topics/data-types.html","spaLink":"#/alpha/topics/data-types","title":"STRINGS"},{"content":"<h2 id=\"data-types-lists\">Lists</h2><p>Redis Lists are simply lists of strings, sorted by insertion order.\nIt is possible to add elements to a Redis List pushing new elements on the head  (on the left) or on the tail (on the right) of the list.</p><p>The <a href=\"/commands/lpush\">LPUSH</a> command inserts a new element on the head, while\n<a href=\"/commands/rpush\">RPUSH</a> inserts a new element on the tail. A new list is created\nwhen one of this operations is performed against an empty key.\nSimilarly the key is removed from the key space if a list operation will\nempty the list. These are very handy semantics since all the list commands will\nbehave exactly like they were called with an empty list if called with a\nnon-existing key as argument.</p><p>Some example of list operations and resulting lists:</p><p>The max length of a list is 2^32 - 1 elements (4294967295, more than 4 billion of elements per list).</p><p>The main features of Redis Lists from the point of view of time complexity are\nthe support for constant time insertion and deletion of elements near the\nhead and tail, even with many millions of inserted items.\nAccessing elements is very fast near the extremes of the list but\nis slow if you try accessing the middle of a very big list, as it is\nan O(N) operation.</p><p>You can do many interesting things with Redis Lists, for instance you can:</p><ul>\n<li>Model a timeline in a social network, using <a href=\"/commands/lpush\">LPUSH</a> in order to add new elements in the user time line, and using <a href=\"/commands/lrange\">LRANGE</a> in order to retrieve a few of recently inserted items.</li>\n<li>You can use <a href=\"/commands/lpush\">LPUSH</a> together with <a href=\"/commands/ltrim\">LTRIM</a> to create a list that never exceeds a given number of elements, but just remembers the latest N elements.</li>\n<li>Lists can be used as a message passing primitive, See for instance the well known <a href=\"https://github.com/defunkt/resque\">Resque</a> Ruby library for creating background jobs.</li>\n<li>You can do a lot more with lists, this data type supports a number of commands, including blocking commands like <a href=\"/commands/blpop\">BLPOP</a>.</li>\n</ul><p>Please check all the <a href=\"/commands#list\">available commands operating on lists</a> for more information, or read the <a href=\"/topics/data-types-intro\">introduction to Redis data types</a>.</p><p><a name=\"sets\"></a></p>","link":"/alpha/topics/data-types.html","spaLink":"#/alpha/topics/data-types","title":"LISTS"},{"content":"<h2 id=\"data-types-sets\">Sets</h2><p>Redis Sets are an unordered collection of Strings. It is possible to add,\nremove, and test for existence of members in O(1) (constant time regardless\nof the number of elements contained inside the Set).</p><p>Redis Sets have the desirable property of not allowing repeated members. Adding the same element multiple times will result in a set having a single copy of this element. Practically speaking this means that adding a member does not require a <em>check if exists then add</em> operation.</p><p>A very interesting thing about Redis Sets is that they support a number of\nserver side commands to compute sets starting from existing sets, so you\ncan do unions, intersections, differences of sets in very short time.</p><p>The max number of members in a set is 2^32 - 1 (4294967295, more than 4 billion   of members per set).</p><p>You can do many interesting things using Redis Sets, for instance you can:</p><ul>\n<li>You can track unique things using Redis Sets. Want to know all the unique IP addresses visiting a given blog post? Simply use <a href=\"/commands/sadd\">SADD</a> every time you process a page view. You are sure repeated IPs will not be inserted.</li>\n<li>Redis Sets are good to represent relations. You can create a tagging system with Redis using a Set to represent every tag. Then you can add all the IDs of all the objects having a given tag into a Set representing this particular tag, using the <a href=\"/commands/sadd\">SADD</a> command. Do you want all the IDs of all the Objects having a three different tags at the same time? Just use <a href=\"/commands/sinter\">SINTER</a>.</li>\n<li>You can use Sets to extract elements at random using the <a href=\"/commands/spop\">SPOP</a> or <a href=\"/commands/srandmember\">SRANDMEMBER</a> commands.</li>\n</ul><p>As usual, check the <a href=\"/commands#set\">full list of Set commands</a> for more information, or read the <a href=\"/topics/data-types-intro\">introduction to Redis data types</a>.</p><p><a name=\"hashes\"></a></p>","link":"/alpha/topics/data-types.html","spaLink":"#/alpha/topics/data-types","title":"SETS"},{"content":"<h2 id=\"data-types-hashes\">Hashes</h2><p>Redis Hashes are maps between string fields and string values, so they are the perfect data type to represent objects (e.g. A User with a number of fields like name, surname, age, and so forth):</p><p>A hash with a few fields (where few means up to one hundred or so) is stored in a way\nthat takes very little space, so you can store millions of objects in a small\nRedis instance.</p><p>While Hashes are used mainly to represent objects, they are capable of storing many elements, so you can use Hashes for many other tasks as well.</p><p>Every hash can store up to 2^32 - 1 field-value pairs (more than 4 billion).</p><p>Check the <a href=\"/commands#hash\">full list of Hash commands</a> for more information, or read the <a href=\"/topics/data-types-intro\">introduction to Redis data types</a>.</p><p><a name=\"sorted-sets\"></a></p>","link":"/alpha/topics/data-types.html","spaLink":"#/alpha/topics/data-types","title":"HASHES"},{"content":"<h2 id=\"data-types-sorted-sets\">Sorted sets</h2><p>Redis Sorted Sets are, similarly to Redis Sets, non repeating collections of\nStrings. The difference is that every member of a Sorted Set is associated\nwith score, that is used in order to take the sorted set ordered, from the\nsmallest to the greatest score.  While members are unique, scores may be\nrepeated.</p><p>With sorted sets you can add, remove, or update elements in a very fast way\n(in a time proportional to the logarithm of the number of elements). Since\nelements are <em>taken in order</em> and not ordered afterwards, you can also get\nranges by score or by rank (position) in a very fast way.\nAccessing the middle of a sorted set is also very fast, so you can use\nSorted Sets as a smart list of non repeating elements where you can quickly access\neverything you need: elements in order, fast existence test, fast access\nto elements in the middle!</p><p>In short with sorted sets you can do a lot of tasks with great performance\nthat are really hard to model in other kind of databases.</p><p>With Sorted Sets you can:</p><ul>\n<li>Take a leader board in a massive online game, where every time a new score\nis submitted you update it using <a href=\"/commands/zadd\">ZADD</a>. You can easily\ntake the top users using <a href=\"/commands/zrange\">ZRANGE</a>, you can also, given an\nuser name, return its rank in the listing using <a href=\"/commands/zrank\">ZRANK</a>.\nUsing ZRANK and ZRANGE together you can show users with a score similar to\na given user. All very <em>quickly</em>.</li>\n<li>Sorted Sets are often used in order to index data that is stored inside Redis.\nFor instance if you have many hashes representing users, you can use a sorted set with elements having the age of the user as the score and the ID of the user as the value. So using <a href=\"/commands/zrangebyscore\">ZRANGEBYSCORE</a> it will be trivial and fast to retrieve all the users with a given interval of ages.</li>\n</ul><p>Sorted Sets are probably the most advanced Redis data types, so take some time to check the <a href=\"/commands#sorted_set\">full list of Sorted Set commands</a> to discover what you can do with Redis! Also you may want to read the <a href=\"/topics/data-types-intro\">introduction to Redis data types</a>.</p>","link":"/alpha/topics/data-types.html","spaLink":"#/alpha/topics/data-types","title":"SORTED SETS"},{"content":"<h2 id=\"data-types-bitmaps-and-hyperloglogs\">Bitmaps and HyperLogLogs</h2><p>Redis also supports Bitmaps and HyperLogLogs which are actually data types\nbased on the String base type, but having their own semantics.</p><p>Please refer to the <a href=\"/topics/data-types-intro\">introduction to Redis data types</a> for information about those types.</p>","link":"/alpha/topics/data-types.html","spaLink":"#/alpha/topics/data-types","title":"BITMAPS AND HYPERLOGLOGS"},{"content":"<h1 id=\"redis-debugging-guide\">Redis debugging guide</h1><p>Redis is developed with a great stress on stability: we do our best with\nevery release to make sure you’ll experience a very stable product and no\ncrashes. However even with our best efforts it is impossible to avoid all\nthe critical bugs with 100% of success.</p><p>When Redis crashes it produces a detailed report of what happened, however\nsometimes looking at the crash report is not enough, nor it is possible for\nthe Redis core team to reproduce the issue independently: in this scenario we\nneed help from the user that is able to reproduce the issue.</p><p>This little guide shows how to use GDB to provide all the information the\nRedis developers will need to track the bug more easily.</p>","link":"/alpha/topics/debugging.html","spaLink":"#/alpha/topics/debugging","title":"REDIS DEBUGGING GUIDE"},{"content":"<h2 id=\"redis-debugging-guide-what-is-gdb\">What is GDB?</h2><p>GDB is the Gnu Debugger: a program that is able to inspect the internal state\nof another program. Usually tracking and fixing a bug is an exercise in\ngathering more information about the state of the program at the moment the\nbug happens, so GDB is an extremely useful tool.</p><p>GDB can be used in two ways:</p><ul>\n<li>It can attach to a running program and inspect the state of it at runtime.</li>\n<li>It can inspect the state of a program that already terminated using what is called a <em>core file</em>, that is, the image of the memory at the time the program was running.</li>\n</ul><p>From the point of view of investigating Redis bugs we need to use both this\nGDB modes: the user able to reproduce the bug attaches GDB to his running Redis instance, and when the crash happens, he creates the <code>core</code> file that the in turn the developer will use to inspect the Redis internals at the time of the crash.</p><p>This way the developer can perform all the inspections in his computer without the help of the user, and the user is free to restart Redis in the production environment.</p>","link":"/alpha/topics/debugging.html","spaLink":"#/alpha/topics/debugging","title":"WHAT IS GDB?"},{"content":"<h2 id=\"redis-debugging-guide-compiling-redis-without-optimizations\">Compiling Redis without optimizations</h2><p>By default Redis is compiled with the <code>-O2</code> switch, this means that compiler\noptimizations are enabled. This makes the Redis executable faster, but at the\nsame time it makes Redis (like any other program) harder to inspect using GDB.</p><p>It is better to attach GDB to Redis compiled without optimizations using the\n<code>make noopt</code> command to compile it (instead of just using the plain <code>make</code>\ncommand). However if you have an already running Redis in production there is\nno need to recompile and restart it if this is going to create problems on\nyour side. Even if by a lesser extent GDB still works against executables\ncompiled with optimizations.</p><p>It is great if you make sure to recompile Redis with <code>make noopt</code> after the\nfirst crash, so that the next time it will be simpler to track the issue.</p><p>You should not be concerned with the loss of performances compiling Redis\nwithout optimizations, it is very unlikely that this will cause problems in\nyour environment since it is usually just a matter of a small percentage\nbecause Redis is not very CPU-bound (it does a lot of I/O to serve queries).</p>","link":"/alpha/topics/debugging.html","spaLink":"#/alpha/topics/debugging","title":"COMPILING REDIS WITHOUT OPTIMIZATIONS"},{"content":"<h2 id=\"redis-debugging-guide-attaching-gdb-to-a-running-process\">Attaching GDB to a running process</h2><p>If you have an already running Redis server, you can attach GDB to it, so that\nif Redis will crash it will be possible to both inspect the internals and\ngenerate a <code>core dump</code> file.</p><p>After you attach GDB to the Redis process it will continue running as usually without any loss of performance, so this is not a dangerous procedure.</p><p>In order to attach GDB the first thing you need is the <em>process ID</em> of the running Redis instance (the <em>pid</em> of the process). You can easily obtain it using <code>redis-cli</code>:</p><p>In the above example the process ID is <strong>58414</strong>.</p><ul>\n<li>Login into your Redis server.</li>\n<li>(Optional but recommended) Start <strong>screen</strong> or <strong>tmux</strong> or any other program that will make sure that your GDB session will not be closed if your ssh connection will timeout. If you don’t know what screen is do yourself a favor and <a href=\"http://www.linuxjournal.com/article/6340\">Read this article</a></li>\n<li><p>Attach GDB to the running Redis server typing:</p>\n<p>  gdb <code>&lt;path-to-redis-executable&gt;</code> <code>&lt;pid&gt;</code></p>\n<p>  For example: gdb /usr/local/bin/redis-server 58414</p>\n</li>\n</ul><p>Attach GDB to the running Redis server typing:</p><p>  gdb <code>&lt;path-to-redis-executable&gt;</code> <code>&lt;pid&gt;</code></p><p>  For example: gdb /usr/local/bin/redis-server 58414</p><p>GDB will start and will attach to the running server printing something like the following:</p><ul>\n<li><p>At this point GDB is attached but <strong>your Redis instance is blocked by GDB</strong>. In order to let the Redis instance continue the execution just type <strong>continue</strong> at the GDB prompt, and press enter.</p>\n<pre><code class=\"prettyprint prettyprinted\" style=\"\"><span class=\"pln\">  </span><span class=\"pun\">(</span><span class=\"pln\">gdb</span><span class=\"pun\">)</span><span class=\"pln\"> </span><span class=\"kwd\">continue</span><span class=\"pln\">\n  </span><span class=\"typ\">Continuing</span><span class=\"pun\">.</span></code></pre>\n</li>\n<li><p>Done! Now your Redis instance has GDB attached. You can wait for… the next crash :)</p>\n</li>\n<li>Now it’s time to detach your screen / tmux session, if you are running GDB using it, pressing the usual <strong>Ctrl-a a</strong> key combination.</li>\n</ul><p>At this point GDB is attached but <strong>your Redis instance is blocked by GDB</strong>. In order to let the Redis instance continue the execution just type <strong>continue</strong> at the GDB prompt, and press enter.</p><p>Done! Now your Redis instance has GDB attached. You can wait for… the next crash :)</p>","link":"/alpha/topics/debugging.html","spaLink":"#/alpha/topics/debugging","title":"ATTACHING GDB TO A RUNNING PROCESS"},{"content":"<h2 id=\"redis-debugging-guide-after-the-crash\">After the crash</h2><p>Redis has a command to simulate a segmentation fault (in other words a bad\ncrash) using the <code>DEBUG SEGFAULT</code> command (don’t use it against a real production instance of course ;). So I’ll use this command to crash my instance to show what happens in the GDB side:</p><p>As you can see GDB detected that Redis crashed, and was able to show me\neven the file name and line number causing the crash. This is already much\nbetter than the Redis crash report back trace (containing just function\nnames and binary offsets).</p>","link":"/alpha/topics/debugging.html","spaLink":"#/alpha/topics/debugging","title":"AFTER THE CRASH"},{"content":"<h2 id=\"redis-debugging-guide-obtaining-the-stack-trace\">Obtaining the stack trace</h2><p>The first thing to do is to obtain a full stack trace with GDB. This is as\nsimple as using the <strong>bt</strong> command: (that is a short for backtrace):</p><p>This shows the backtrace, but we also want to dump the processor registers using the <strong>info registers</strong> command:</p><p>Please <strong>make sure to include</strong> both this outputs in your bug report.</p>","link":"/alpha/topics/debugging.html","spaLink":"#/alpha/topics/debugging","title":"OBTAINING THE STACK TRACE"},{"content":"<h2 id=\"redis-debugging-guide-obtaining-the-core-file\">Obtaining the core file</h2><p>The next step is to generate the core dump, that is the image of the memory of the running Redis process. This is performed using the <code>gcore</code> command:</p><p>Now you have the core dump to send to the Redis developer, but <strong>it is important to understand</strong> that this happens to contain all the data that was inside the Redis instance at the time of the crash: Redis developers will make sure to don’t share the content with any other, and will delete the file as soon as it is no longer used for debugging purposes, but you are warned that sending the core file you are sending your data.</p><p>If there are sensible stuff in the data set we suggest sending the dump directly to Salvatore Sanfilippo (that is the guy writing this doc) at the email address <strong>antirez at gmail dot com</strong>.</p>","link":"/alpha/topics/debugging.html","spaLink":"#/alpha/topics/debugging","title":"OBTAINING THE CORE FILE"},{"content":"<h2 id=\"redis-debugging-guide-what-to-send-to-developers\">What to send to developers</h2><p>Finally you can send everything to the Redis core team:</p><ul>\n<li>The Redis executable you are using.</li>\n<li>The stack trace produced by the <strong>bt</strong> command, and the registers dump.</li>\n<li>The core file you generated with gdb.</li>\n<li>Information about the operating system and GCC version, and Redis version you are using.</li>\n</ul>","link":"/alpha/topics/debugging.html","spaLink":"#/alpha/topics/debugging","title":"WHAT TO SEND TO DEVELOPERS"},{"content":"<h2 id=\"redis-debugging-guide-thank-you\">Thank you</h2><p>Your help is extremely important! Many issues can only be tracked this way, thanks! It is also possible that helping Redis debugging you’ll be among the winners of the next <a href=\"http://antirez.com/post/redis-moka-awards-2011.html\">Redis Moka Award</a>.</p>","link":"/alpha/topics/debugging.html","spaLink":"#/alpha/topics/debugging","title":"THANK YOU"},{"content":"<h1 id=\"distributed-locks-with-redis\">Distributed locks with Redis</h1><p>Distributed locks are a very useful primitive in many environments where\ndifferent processes must operate with shared resources in a mutually\nexclusive way.</p><p>There are a number of libraries and blog posts describing how to implement\na DLM (Distributed Lock Manager) with Redis, but every library uses a different\napproach, and many use a simple approach with lower guarantees compared to\nwhat can be achieved with slightly more complex designs.</p><p>This page is an attempt to provide a more canonical algorithm to implement\ndistributed locks with Redis. We propose an algorithm, called <strong>Redlock</strong>,\nwhich implements a DLM which we believe to be safer than the vanilla single\ninstance approach. We hope that the community will analyze it, provide\nfeedback, and use it as a starting point for the implementations or more\ncomplex or alternative designs.</p>","link":"/alpha/topics/distlock.html","spaLink":"#/alpha/topics/distlock","title":"DISTRIBUTED LOCKS WITH REDIS"},{"content":"<h2 id=\"distributed-locks-with-redis-implementations\">Implementations</h2><p>Before describing the algorithm, here are a few links to implementations\nalready available that can be used for reference.</p><ul>\n<li><a href=\"https://github.com/antirez/redlock-rb\">Redlock-rb</a> (Ruby implementation). There is also a <a href=\"https://github.com/leandromoreira/redlock-rb\">fork of Redlock-rb</a> that adds a gem for easy distribution and perhaps more.</li>\n<li><a href=\"https://github.com/SPSCommerce/redlock-py\">Redlock-py</a> (Python implementation).</li>\n<li><a href=\"https://github.com/ronnylt/redlock-php\">Redlock-php</a> (PHP implementation).</li>\n<li><a href=\"https://github.com/malkusch/lock#phpredismutex\">PHPRedisMutex</a> (further PHP implementation)</li>\n<li><a href=\"https://github.com/hjr265/redsync.go\">Redsync.go</a> (Go implementation).</li>\n<li><a href=\"https://github.com/mrniko/redisson\">Redisson</a> (Java implementation).</li>\n<li><a href=\"https://github.com/sbertrang/redis-distlock\">Redis::DistLock</a> (Perl implementation).</li>\n<li><a href=\"https://github.com/jacket-code/redlock-cpp\">Redlock-cpp</a> (C++ implementation).</li>\n<li><a href=\"https://github.com/kidfashion/redlock-cs\">Redlock-cs</a> (C#/.NET implementation).</li>\n<li><a href=\"https://github.com/samcook/RedLock.net\">RedLock.net</a> (C#/.NET implementation). Includes async and lock extension support.</li>\n<li><a href=\"https://github.com/psibernetic/scarletlock\">ScarletLock</a> (C# .NET implementation with configurable datastore)</li>\n<li><a href=\"https://github.com/mike-marcacci/node-redlock\">node-redlock</a> (NodeJS implementation). Includes support for lock extension.</li>\n</ul>","link":"/alpha/topics/distlock.html","spaLink":"#/alpha/topics/distlock","title":"IMPLEMENTATIONS"},{"content":"<h2 id=\"distributed-locks-with-redis-safety-and-liveness-guarantees\">Safety and Liveness guarantees</h2><p>We are going to model our design with just three properties that, from our point of view, are the minimum guarantees needed to use distributed locks in an effective way.</p>","link":"/alpha/topics/distlock.html","spaLink":"#/alpha/topics/distlock","title":"SAFETY AND LIVENESS GUARANTEES"},{"content":"<h2 id=\"distributed-locks-with-redis-why-failover-based-implementations-are-not-enough\">Why failover-based implementations are not enough</h2><p>To understand what we want to improve, let’s analyze the current state of affairs with most Redis-based distributed lock libraries.</p><p>The simplest way to use Redis to lock a resource is to create a key in an instance. The key is usually created with a limited time to live, using the Redis expires feature, so that eventually it will get released (property 2 in our list). When the client needs to release the resource, it deletes the key.</p><p>Superficially this works well, but there is a problem: this is a single point of failure in our architecture. What happens if the Redis master goes down?\nWell, let’s add a slave! And use it if the master is unavailable. This is unfortunately not viable. By doing so we can’t implement our safety property of mutual exclusion, because Redis replication is asynchronous.</p><p>There is an obvious race condition with this model:</p><p>Sometimes it is perfectly fine that under special circumstances, like during a failure, multiple clients can hold the lock at the same time.\nIf this is the case, you can use your replication based solution. Otherwise we suggest to implement the solution described in this document.</p>","link":"/alpha/topics/distlock.html","spaLink":"#/alpha/topics/distlock","title":"WHY FAILOVER-BASED IMPLEMENTATIONS ARE NOT ENOUGH"},{"content":"<h2 id=\"distributed-locks-with-redis-correct-implementation-with-a-single-instance\">Correct implementation with a single instance</h2><p>Before trying to overcome the limitation of the single instance setup described above, let’s check how to do it correctly in this simple case, since this is actually a viable solution in applications where a race condition from time to time is acceptable, and because locking into a single instance is the foundation we’ll use for the distributed algorithm described here.</p><p>To acquire the lock, the way to go is the following:</p><p>The command will set the key only if it does not already exist (NX option), with an expire of 30000 milliseconds (PX option).\nThe key is set to a value “my_random_value”. This value must be unique across all clients and all lock requests.</p><p>Basically the random value is used in order to release the lock in a safe way, with a script that tells Redis: remove the key only if it exists and the value stored at the key is exactly the one I expect to be. This is accomplished by the following Lua script:</p><p>This is important in order to avoid removing a lock that was created by another client. For example a client may acquire the lock, get blocked in some operation for longer than the lock validity time (the time at which the key will expire), and later remove the lock, that was already acquired by some other client.\nUsing just DEL is not safe as a client may remove the lock of another client. With the above script instead every lock is “signed” with a random string, so the lock will be removed only if it is still the one that was set by the client trying to remove it.</p><p>What should this random string be? I assume it’s 20 bytes from /dev/urandom, but you can find cheaper ways to make it unique enough for your tasks.\nFor example a safe pick is to seed RC4 with /dev/urandom, and generate a pseudo random stream from that.\nA simpler solution is to use a combination of unix time with microseconds resolution, concatenating it with a client ID, it is not as safe, but probably up to the task in most environments.</p><p>The time we use as the key time to live, is called the “lock validity time”. It is both the auto release time, and the time the client has in order to perform the operation required before another client may be able to acquire the lock again, without technically violating the mutual exclusion guarantee, which is only limited to a given window of time from the moment the lock is acquired.</p><p>So now we have a good way to acquire and release the lock. The system, reasoning about a non-distributed system composed of a single, always available, instance, is safe. Let’s extend the concept to a distributed system where we don’t have such guarantees.</p>","link":"/alpha/topics/distlock.html","spaLink":"#/alpha/topics/distlock","title":"CORRECT IMPLEMENTATION WITH A SINGLE INSTANCE"},{"content":"<h2 id=\"distributed-locks-with-redis-the-redlock-algorithm\">The Redlock algorithm</h2><p>In the distributed version of the algorithm we assume we have N Redis masters. Those nodes are totally independent, so we don’t use replication or any other implicit coordination system. We already described how to acquire and release the lock safely in a single instance. We take for granted that the algorithm will use this method to acquire and release the lock in a single instance. In our examples we set N=5, which is a reasonable value, so we need to run 5 Redis masters on different computers or virtual machines in order to ensure that they’ll fail in a mostly independent way.</p><p>In order to acquire the lock, the client performs the following operations:</p>","link":"/alpha/topics/distlock.html","spaLink":"#/alpha/topics/distlock","title":"THE REDLOCK ALGORITHM"},{"content":"<h2 id=\"distributed-locks-with-redis-is-the-algorithm-asynchronous\">Is the algorithm asynchronous?</h2><p>The algorithm relies on the assumption that while there is no synchronized clock across the processes, still the local time in every process flows approximately at the same rate, with an error which is small compared to the auto-release time of the lock. This assumption closely resembles a real-world computer: every computer has a local clock and we can usually rely on different computers to have a clock drift which is small.</p><p>At this point we need to better specify our mutual exclusion rule: it is guaranteed only as long as the client holding the lock will terminate its work within the lock validity time (as obtained in step 3), minus some time (just a few milliseconds in order to compensate for clock drift between processes).</p><p>For more information about similar systems requiring a bound <em>clock drift</em>, this paper is an interesting reference: <a href=\"http://dl.acm.org/citation.cfm?id=74870\">Leases: an efficient fault-tolerant mechanism for distributed file cache consistency</a>.</p>","link":"/alpha/topics/distlock.html","spaLink":"#/alpha/topics/distlock","title":"IS THE ALGORITHM ASYNCHRONOUS?"},{"content":"<h2 id=\"distributed-locks-with-redis-retry-on-failure\">Retry on failure</h2><p>When a client is unable to acquire the lock, it should try again after a random delay in order to try to desynchronize multiple clients trying to acquire the lock for the same resource at the same time (this may result in a split brain condition where nobody wins). Also the faster a client tries to acquire the lock in the majority of Redis instances, the smaller the window for a split brain condition (and the need for a retry), so ideally the client should try to send the SET commands to the N instances at the same time using multiplexing.</p><p>It is worth stressing how important it is for clients that fail to acquire the majority of locks, to release the (partially) acquired locks ASAP, so that there is no need to wait for key expiry in order for the lock to be acquired again (however if a network partition happens and the client is no longer able to communicate with the Redis instances, there is an availability penalty to pay as it waits for key expiration).</p>","link":"/alpha/topics/distlock.html","spaLink":"#/alpha/topics/distlock","title":"RETRY ON FAILURE"},{"content":"<h2 id=\"distributed-locks-with-redis-releasing-the-lock\">Releasing the lock</h2><p>Releasing the lock is simple and involves just releasing the lock in all instances, whether or not the client believes it was able to successfully lock a given instance.</p>","link":"/alpha/topics/distlock.html","spaLink":"#/alpha/topics/distlock","title":"RELEASING THE LOCK"},{"content":"<h2 id=\"distributed-locks-with-redis-safety-arguments\">Safety arguments</h2><p>Is the algorithm safe? We can try to understand what happens in different scenarios.</p><p>To start let’s assume that a client is able to acquire the lock in the majority of instances. All the instances will contain a key with the same time to live. However, the key was set at different times, so the keys will also expire at different times. But if the first key was set at worst at time T1 (the time we sample before contacting the first server) and the last key was set at worst at time T2 (the time we obtained the reply from the last server), we are sure that the first key to expire in the set will exist for at least <code>MIN_VALIDITY=TTL-(T2-T1)-CLOCK_DRIFT</code>. All the other keys will expire later, so we are sure that the keys will be simultaneously set for at least this time.</p><p>During the time that the majority of keys are set, another client will not be able to acquire the lock, since N/2+1 SET NX operations can’t succeed if N/2+1 keys already exist. So if a lock was acquired, it is not possible to re-acquire it at the same time (violating the mutual exclusion property).</p><p>However we want to also make sure that multiple clients trying to acquire the lock at the same time can’t simultaneously succeed.</p><p>If a client locked the majority of instances using a time near, or greater, than the lock maximum validity time (the TTL we use for SET basically), it will consider the lock invalid and will unlock the instances, so we only need to consider the case where a client was able to lock the majority of instances in a time which is less than the validity time. In this case for the argument already expressed above, for <code>MIN_VALIDITY</code> no client should be able to re-acquire the lock. So multiple clients will be able to lock N/2+1 instances at the same time (with “time” being the end of Step 2) only when the time to lock the majority was greater than the TTL time, making the lock invalid.</p><p>Are you able to provide a formal proof of safety, point to existing algorithms that are similar, or find a bug? That would be greatly appreciated.</p>","link":"/alpha/topics/distlock.html","spaLink":"#/alpha/topics/distlock","title":"SAFETY ARGUMENTS"},{"content":"<h2 id=\"distributed-locks-with-redis-liveness-arguments\">Liveness arguments</h2><p>The system liveness is based on three main features:</p><p>However, we pay an availability penalty equal to <code>TTL</code> time on network partitions, so if there are continuous partitions, we can pay this penalty indefinitely.\nThis happens every time a client acquires a lock and gets partitioned away before being able to remove the lock.</p><p>Basically if there are infinite continuous network partitions, the system may become not available for an infinite amount of time.</p>","link":"/alpha/topics/distlock.html","spaLink":"#/alpha/topics/distlock","title":"LIVENESS ARGUMENTS"},{"content":"<h2 id=\"distributed-locks-with-redis-performance-crash-recovery-and-fsync\">Performance, crash-recovery and fsync</h2><p>Many users using Redis as a lock server need high performance in terms of both latency to acquire and release a lock, and number of acquire / release operations that it is possible to perform per second. In order to meet this requirement, the strategy to talk with the N Redis servers to reduce latency is definitely multiplexing (or poor man’s multiplexing, which is, putting the socket in non-blocking mode, send all the commands, and read all the commands later, assuming that the RTT between the client and each instance is similar).</p><p>However there is another consideration to do about persistence if we want to target a crash-recovery system model.</p><p>Basically to see the problem here, let’s assume we configure Redis without persistence at all. A client acquires the lock in 3 of 5 instances. One of the instances where the client was able to acquire the lock is restarted, at this point there are again 3 instances that we can lock for the same resource, and another client can lock it again, violating the safety property of exclusivity of lock.</p><p>If we enable AOF persistence, things will improve quite a bit. For example we can upgrade a server by sending SHUTDOWN and restarting it. Because Redis expires are semantically implemented so that virtually the time still elapses when the server is off, all our requirements are fine.\nHowever everything is fine as long as it is a clean shutdown. What about a power outage? If Redis is configured, as by default, to fsync on disk every second, it is possible that after a restart our key is missing. In theory, if we want to guarantee the lock safety in the face of any kind of instance restart, we need to enable fsync=always in the persistence setting. This in turn will totally ruin performances to the same level of CP systems that are traditionally used to implement distributed locks in a safe way.</p><p>However things are better than what they look like at a first glance. Basically\nthe algorithm safety is retained as long as when an instance restarts after a\ncrash, it no longer participates to any <strong>currently active</strong> lock, so that the\nset of currently active locks when the instance restarts, were all obtained\nby locking instances other than the one which is rejoining the system.</p><p>To guarantee this we just need to make an instance, after a crash, unavailable\nfor at least a bit more than the max <code>TTL</code> we use, which is, the time needed\nfor all the keys about the locks that existed when the instance crashed, to\nbecome invalid and be automatically released.</p><p>Using <em>delayed restarts</em> it is basically possible to achieve safety even\nwithout any kind of Redis persistence available, however note that this may\ntranslate into an availability penalty. For example if a majority of instances\ncrash, the system will become globally unavailable for <code>TTL</code> (here globally means\nthat no resource at all will be lockable during this time).</p>","link":"/alpha/topics/distlock.html","spaLink":"#/alpha/topics/distlock","title":"PERFORMANCE, CRASH-RECOVERY AND FSYNC"},{"content":"<h2 id=\"distributed-locks-with-redis-making-the-algorithm-more-reliable-extending-the-lock\">Making the algorithm more reliable: Extending the lock</h2><p>If the work performed by clients is composed of small steps, it is possible to\nuse smaller lock validity times by default, and extend the algorithm implementing\na lock extension mechanism. Basically the client, if in the middle of the\ncomputation while the lock validity is approaching a low value, may extend the\nlock by sending a Lua script to all the instances that extends the TTL of the key\nif the key exists and its value is still the random value the client assigned\nwhen the lock was acquired.</p><p>The client should only consider the lock re-acquired if it was able to extend\nthe lock into the majority of instances, and within the validity time\n(basically the algorithm to use is very similar to the one used when acquiring\nthe lock).</p><p>However this does not technically change the algorithm, so the maximum number\nof lock reacquisition attempts should be limited, otherwise one of the liveness\nproperties is violated.</p>","link":"/alpha/topics/distlock.html","spaLink":"#/alpha/topics/distlock","title":"MAKING THE ALGORITHM MORE RELIABLE: EXTENDING THE LOCK"},{"content":"<h2 id=\"distributed-locks-with-redis-want-to-help\">Want to help?</h2><p>If you are into distributed systems, it would be great to have your opinion / analysis. Also reference implementations in other languages could be great.</p><p>Thanks in advance!</p>","link":"/alpha/topics/distlock.html","spaLink":"#/alpha/topics/distlock","title":"WANT TO HELP?"},{"content":"<h2 id=\"distributed-locks-with-redis-analysis-of-redlock\">Analysis of Redlock</h2>","link":"/alpha/topics/distlock.html","spaLink":"#/alpha/topics/distlock","title":"ANALYSIS OF REDLOCK"},{"content":"<h1 id=\"redis-encryption\">Redis Encryption</h1><p>The idea of adding SSL support to Redis was proposed many times, however\ncurrently we believe that given the small percentage of users requiring\nSSL support, and the fact that each scenario tends to be different, to use\na different “tunneling” strategy can be better. We may change the idea in the\nfuture, but currently a good solution that may be suitable for many use cases\nis to use the following project:</p><ul>\n<li><a href=\"http://www.tarsnap.com/spiped.html\">Spiped</a> is a utility for creating symmetrically encrypted and authenticated pipes between socket addresses, so that one may connect to one address (e.g., a UNIX socket on localhost) and transparently have a connection established to another address (e.g., a UNIX socket on a different system).</li>\n</ul><p>The software is written in a similar spirit to Redis itself, it is a self-contained 4000 lines of C code utility that does a single thing well.</p>","link":"/alpha/topics/encryption.html","spaLink":"#/alpha/topics/encryption","title":"REDIS ENCRYPTION"},{"content":"<h1 id=\"faq\">FAQ</h1>","link":"/alpha/topics/faq.html","spaLink":"#/alpha/topics/faq","title":"FAQ"},{"content":"<h2 id=\"faq-why-redis-is-different-compared-to-other-key-value-stores\">Why Redis is different compared to other key-value stores?</h2><p>There are two main reasons.</p><ul>\n<li>Redis is a different evolution path in the key-value DBs where values can contain more complex data types, with atomic operations defined on those data types. Redis data types are closely related to fundamental data structures and are exposed to the programmer as such, without additional abstraction layers.</li>\n<li>Redis is an in-memory but persistent on disk database, so it represents a different trade off where very high write and read speed is achieved with the limitation of data sets that can’t be larger than memory. Another advantage of\nin memory databases is that the memory representation of complex data structures\nis much simpler to manipulate compared to the same data structure on disk, so\nRedis can do a lot, with little internal complexity. At the same time the\ntwo on-disk storage formats (RDB and AOF) don’t need to be suitable for random\naccess, so they are compact and always generated in an append-only fashion\n(Even the AOF log rotation is an append-only operation, since the new version\nis generated from the copy of data in memory).</li>\n</ul>","link":"/alpha/topics/faq.html","spaLink":"#/alpha/topics/faq","title":"WHY REDIS IS DIFFERENT COMPARED TO OTHER KEY-VALUE STORES?"},{"content":"<h2 id=\"faq-whats-the-redis-memory-footprint\">What’s the Redis memory footprint?</h2><p>To give you a few examples (all obtained using 64-bit instances):</p><ul>\n<li>An empty instance uses ~ 1MB of memory.</li>\n<li>1 Million small Keys -&gt; String Value pairs use ~ 100MB of memory.</li>\n<li>1 Million Keys -&gt; Hash value, representing an object with 5 fields, use ~ 200 MB of memory.</li>\n</ul><p>To test your use case is trivial using the <code>redis-benchmark</code> utility to generate random data sets and check with the <code>INFO memory</code> command the space used.</p><p>64-bit systems will use considerably more memory than 32-bit systems to store the same keys, especially if the keys and values are small. This is because pointers take 8 bytes in 64-bit systems. But of course the advantage is that you can\nhave a lot of memory in 64-bit systems, so in order to run large Redis servers a 64-bit system is more or less required. The alternative is sharding.</p>","link":"/alpha/topics/faq.html","spaLink":"#/alpha/topics/faq","title":"WHAT’S THE REDIS MEMORY FOOTPRINT?"},{"content":"<h2 id=\"faq-i-like-rediss-high-level-operations-and-features-but-i-dont-like-that-it-takes-everything-in-memory-and-i-cant-have-a-dataset-larger-the-memory-plans-to-change-this\">I like Redis’s high level operations and features, but I don’t like that it takes everything in memory and I can’t have a dataset larger the memory. Plans to change this?</h2><p>In the past the Redis developers experimented with Virtual Memory and other systems in order to allow larger than RAM datasets, but after all we are very happy if we can do one thing well: data served from memory, disk used for storage. So for now there are no plans to create an on disk backend for Redis. Most of what\nRedis is, after all, is a direct result of its current design.</p><p>If your real problem is not the total RAM needed, but the fact that you need\nto split your data set into multiple Redis instances, please read the\n<a href=\"/topics/partitioning\">Partitioning page</a> in this documentation for more info.</p>","link":"/alpha/topics/faq.html","spaLink":"#/alpha/topics/faq","title":"I LIKE REDIS’S HIGH LEVEL OPERATIONS AND FEATURES, BUT I DON’T LIKE THAT IT TAKES EVERYTHING IN MEMORY AND I CAN’T HAVE A DATASET LARGER THE MEMORY. PLANS TO CHANGE THIS?"},{"content":"<h2 id=\"faq-is-using-redis-together-with-an-on-disk-database-a-good-idea\">Is using Redis together with an on-disk database a good idea?</h2><p>Yes, a common design pattern involves taking very write-heavy small data\nin Redis (and data you need the Redis data structures to model your problem\nin an efficient way), and big <em>blobs</em> of data into an SQL or eventually\nconsistent on-disk database.</p>","link":"/alpha/topics/faq.html","spaLink":"#/alpha/topics/faq","title":"IS USING REDIS TOGETHER WITH AN ON-DISK DATABASE A GOOD IDEA?"},{"content":"<h2 id=\"faq-is-there-something-i-can-do-to-lower-the-redis-memory-usage\">Is there something I can do to lower the Redis memory usage?</h2><p>If you can, use Redis 32 bit instances. Also make good use of small hashes,\nlists, sorted sets, and sets of integers, since Redis is able to represent\nthose data types in the special case of a few elements in a much more compact\nway. There is more info in the <a href=\"/topics/memory-optimization\">Memory Optimization page</a>.</p>","link":"/alpha/topics/faq.html","spaLink":"#/alpha/topics/faq","title":"IS THERE SOMETHING I CAN DO TO LOWER THE REDIS MEMORY USAGE?"},{"content":"<h2 id=\"faq-what-happens-if-redis-runs-out-of-memory\">What happens if Redis runs out of memory?</h2><p>Redis will either be killed by the Linux kernel OOM killer,\ncrash with an error, or will start to slow down.\nWith modern operating systems malloc() returning NULL is not common, usually\nthe server will start swapping, and Redis performance will degrade, so\nyou’ll probably notice there is something wrong.</p><p>The INFO command will report the amount of memory Redis is using so you can\nwrite scripts that monitor your Redis servers checking for critical conditions.</p><p>Redis has built-in protections allowing the user to set a max limit to memory\nusage, using the <code>maxmemory</code> option in the config file to put a limit\nto the memory Redis can use. If this limit is reached Redis will start to reply\nwith an error to write commands (but will continue to accept read-only\ncommands), or you can configure it to evict keys when the max memory limit\nis reached in the case you are using Redis for caching.</p><p>We have documentation if you plan to use <a href=\"/topics/lru-cache\">Redis as an LRU cache</a>.</p>","link":"/alpha/topics/faq.html","spaLink":"#/alpha/topics/faq","title":"WHAT HAPPENS IF REDIS RUNS OUT OF MEMORY?"},{"content":"<h2 id=\"faq-background-saving-is-failing-with-a-fork-error-under-linux-even-if-ive-a-lot-of-free-ram\">Background saving is failing with a fork() error under Linux even if I’ve a lot of free RAM!</h2><p>Short answer: <code>echo 1 &gt; /proc/sys/vm/overcommit_memory</code> :)</p><p>And now the long one:</p><p>Redis background saving schema relies on the copy-on-write semantic of fork in\nmodern operating systems: Redis forks (creates a child process) that is an\nexact copy of the parent. The child process dumps the DB on disk and finally\nexits. In theory the child should use as much memory as the parent being a\ncopy, but actually thanks to the copy-on-write semantic implemented by most\nmodern operating systems the parent and child process will <em>share</em> the common\nmemory pages. A page will be duplicated only when it changes in the child or in\nthe parent. Since in theory all the pages may change while the child process is\nsaving, Linux can’t tell in advance how much memory the child will take, so if\nthe <code>overcommit_memory</code> setting is set to zero fork will fail unless there is\nas much free RAM as required to really duplicate all the parent memory pages,\nwith the result that if you have a Redis dataset of 3 GB and just 2 GB of free\nmemory it will fail.</p><p>Setting <code>overcommit_memory</code> to 1 says Linux to relax and perform the fork in a\nmore optimistic allocation fashion, and this is indeed what you want for Redis.</p><p>A good source to understand how Linux Virtual Memory work and other\nalternatives for <code>overcommit_memory</code> and <code>overcommit_ratio</code> is this classic\nfrom Red Hat Magazine, <a href=\"http://www.redhat.com/magazine/001nov04/features/vm/\">“Understanding Virtual Memory”</a>.\nBeware, this article had <code>1</code> and <code>2</code> configuration values for <code>overcommit_memory</code>\nreversed: refer to the <a href=\"http://man7.org/linux/man-pages/man5/proc.5.html\">proc(5)</a> man page for the right meaning of the\navailable values.</p>","link":"/alpha/topics/faq.html","spaLink":"#/alpha/topics/faq","title":"BACKGROUND SAVING IS FAILING WITH A FORK() ERROR UNDER LINUX EVEN IF I’VE A LOT OF FREE RAM!"},{"content":"<h2 id=\"faq-are-redis-on-disk-snapshots-atomic\">Are Redis on-disk-snapshots atomic?</h2><p>Yes, redis background saving process is always forked when the server is\noutside of the execution of a command, so every command reported to be atomic\nin RAM is also atomic from the point of view of the disk snapshot.</p>","link":"/alpha/topics/faq.html","spaLink":"#/alpha/topics/faq","title":"ARE REDIS ON-DISK-SNAPSHOTS ATOMIC?"},{"content":"<h2 id=\"faq-redis-is-single-threaded-how-can-i-exploit-multiple-cpu-cores\">Redis is single threaded. How can I exploit multiple CPU / cores?</h2><p>It’s very unlikely that CPU becomes your bottleneck with Redis, as usually Redis is either memory or network bound. For instance, using pipelining Redis running\non an average Linux system can deliver even 500k requests per second, so\nif your application mainly uses O(N) or O(log(N)) commands, it is hardly\ngoing to use too much CPU.</p><p>However, to maximize CPU usage you can start multiple instances of Redis in\nthe same box and treat them as different servers. At some point a single\nbox may not be enough anyway, so if you want to use multiple CPUs you can\nstart thinking of some way to shard earlier.</p><p>You can find more information about using multiple Redis instances in the <a href=\"/topics/partitioning\">Partitioning page</a>.</p>","link":"/alpha/topics/faq.html","spaLink":"#/alpha/topics/faq","title":"REDIS IS SINGLE THREADED. HOW CAN I EXPLOIT MULTIPLE CPU / CORES?"},{"content":"<h2 id=\"faq-what-is-the-maximum-number-of-keys-a-single-redis-instance-can-hold-and-what-the-max-number-of-elements-in-a-hash-list-set-sorted-set\">What is the maximum number of keys a single Redis instance can hold? and what the max number of elements in a Hash, List, Set, Sorted Set?</h2><p>Redis can handle up to 2^32 keys, and was tested in practice to\nhandle at least 250 million of keys per instance.</p><p>Every hash, list, set, and sorted set, can hold 2^32 elements.</p><p>In other words your limit is likely the available memory in your system.</p>","link":"/alpha/topics/faq.html","spaLink":"#/alpha/topics/faq","title":"WHAT IS THE MAXIMUM NUMBER OF KEYS A SINGLE REDIS INSTANCE CAN HOLD? AND WHAT THE MAX NUMBER OF ELEMENTS IN A HASH, LIST, SET, SORTED SET?"},{"content":"<h2 id=\"faq-my-slave-claims-to-have-a-different-number-of-keys-compared-to-its-master-why\">My slave claims to have a different number of keys compared to its master, why?</h2><p>If you use keys with limited time to live (Redis expires) this is normal behavior. This is what happens:</p><ul>\n<li>The master generates an RDB file on the first synchronization with the slave.</li>\n<li>The RDB file will not include keys already expired in the master, but that are still in memory.</li>\n<li>However these keys are still in the memory of the Redis master, even if logically expired. They’ll not be considered as existing, but the memory will be reclaimed later, both incrementally and explicitly on access. However while these keys are not logical part of the dataset, they are advertised in <code>INFO</code> output and by the <code>DBSIZE</code> command.</li>\n<li>When the slave reads the RDB file generated by the master, this set of keys will not be loaded.</li>\n</ul><p>As a result of this, it is common for users with many keys with an expire set to see less keys in the slaves, because of this artifact, but there is no actual logical difference in the instances content.</p>","link":"/alpha/topics/faq.html","spaLink":"#/alpha/topics/faq","title":"MY SLAVE CLAIMS TO HAVE A DIFFERENT NUMBER OF KEYS COMPARED TO ITS MASTER, WHY?"},{"content":"<h2 id=\"faq-what-redis-means-actually\">What Redis means actually?</h2><p>It means REmote DIctionary Server.</p>","link":"/alpha/topics/faq.html","spaLink":"#/alpha/topics/faq","title":"WHAT REDIS MEANS ACTUALLY?"},{"content":"<h2 id=\"faq-why-did-you-start-the-redis-project\">Why did you start the Redis project?</h2><p>Originally Redis was started in order to scale <a href=\"http://lloogg.com\">LLOOGG</a>. But after I got the basic server working I liked the idea to share the work with other people, and Redis was turned into an open source project.</p>","link":"/alpha/topics/faq.html","spaLink":"#/alpha/topics/faq","title":"WHY DID YOU START THE REDIS PROJECT?"},{"content":"<h1 id=\"event-library\">Event Library</h1>","link":"/alpha/topics/internals-eventlib.html","spaLink":"#/alpha/topics/internals-eventlib","title":"EVENT LIBRARY"},{"content":"<h2 id=\"event-library-why-is-an-event-library-needed-at-all\">Why is an Event Library needed at all?</h2><p>Let us figure it out through a series of Q&amp;As.</p><p>Q: What do you expect a network server to be doing all the time? <br>\nA: Watch for inbound connections on the port its listening and accept them.</p><p>Q: Calling [accept](<a href=\"http://man.cx/accept%282%29\">http://man.cx/accept%282%29</a> accept) yields a descriptor. What do I do with it?<br>\nA: Save the descriptor and do a non-blocking read/write operation on it.</p><p>Q: Why does the read/write have to be non-blocking?<br>\nA: If the file operation ( even a socket in Unix is a file ) is blocking how could the server for example accept other connection requests when its blocked in a file I/O operation.</p><p>Q: I guess I have to do many such non-blocking operations on the socket to see when it’s ready. Am I right?<br>\nA: Yes. That is what an event library does for you. Now you get it.</p><p>Q: How do Event Libraries do what they do?<br>\nA: They use the operating system’s <a href=\"http://www.devshed.com/c/a/BrainDump/Linux-Files-and-the-Event-Poll-Interface/\">polling</a> facility along with timers.</p><p>Q: So are there any open source event libraries that do what you just described? <br>\nA: Yes. <code>libevent</code> and <code>libev</code> are two such event libraries that I can recall off the top of my head.</p><p>Q: Does Redis use such open source event libraries for handling socket I/O?<br>\nA: No. For various <a href=\"http://groups.google.com/group/redis-db/browse_thread/thread/b52814e9ef15b8d0/\">reasons</a> Redis uses its own event library.</p>","link":"/alpha/topics/internals-eventlib.html","spaLink":"#/alpha/topics/internals-eventlib","title":"WHY IS AN EVENT LIBRARY NEEDED AT ALL?"},{"content":"<h1 id=\"redis-event-library\">Redis Event Library</h1><p>Redis implements its own event library. The event library is implemented in <code>ae.c</code>.</p><p>The best way to understand how the Redis event library works is to understand how Redis uses it.</p>","link":"/alpha/topics/internals-rediseventlib.html","spaLink":"#/alpha/topics/internals-rediseventlib","title":"REDIS EVENT LIBRARY"},{"content":"<h2 id=\"redis-event-library-event-loop-initialization\">Event Loop Initialization</h2><p><code>initServer</code> function defined in <code>redis.c</code> initializes the numerous fields of the <code>redisServer</code> structure variable. One such field is the Redis event loop <code>el</code>:</p><p><code>initServer</code> initializes <code>server.el</code> field by calling <code>aeCreateEventLoop</code> defined in <code>ae.c</code>. The definition of <code>aeEventLoop</code> is below:</p>","link":"/alpha/topics/internals-rediseventlib.html","spaLink":"#/alpha/topics/internals-rediseventlib","title":"EVENT LOOP INITIALIZATION"},{"content":"<h2 id=\"redis-event-library-aecreateeventloop\"><code>aeCreateEventLoop</code></h2><p><code>aeCreateEventLoop</code> first <code>malloc</code>s <code>aeEventLoop</code> structure then calls <code>ae_epoll.c:aeApiCreate</code>.</p><p><code>aeApiCreate</code> <code>malloc</code>s <code>aeApiState</code> that has two fields - <code>epfd</code> that holds the <code>epoll</code> file descriptor returned by a call from <a href=\"http://man.cx/epoll_create%282%29\"><code>epoll_create</code></a> and <code>events</code> that is of type <code>struct epoll_event</code> define by the Linux <code>epoll</code> library. The use of the <code>events</code> field will be  described later.</p><p>Next is <code>ae.c:aeCreateTimeEvent</code>. But before that <code>initServer</code> call <code>anet.c:anetTcpServer</code> that creates and returns a <em>listening descriptor</em>. The descriptor listens on <em>port 6379</em> by default. The returned  <em>listening descriptor</em> is stored in <code>server.fd</code> field.</p>","link":"/alpha/topics/internals-rediseventlib.html","spaLink":"#/alpha/topics/internals-rediseventlib","title":"AECREATEEVENTLOOP"},{"content":"<h2 id=\"redis-event-library-aecreatetimeevent\"><code>aeCreateTimeEvent</code></h2><p><code>aeCreateTimeEvent</code> accepts the following as parameters:</p><ul>\n<li><code>eventLoop</code>: This is <code>server.el</code> in <code>redis.c</code></li>\n<li>milliseconds: The number of milliseconds from the current time after which the timer expires.</li>\n<li><code>proc</code>: Function pointer. Stores the address of the function that has to be called after the timer expires.</li>\n<li><code>clientData</code>: Mostly <code>NULL</code>.</li>\n<li><code>finalizerProc</code>: Pointer to the function that has to be called before the timed event is removed from the list of timed events.</li>\n</ul><p><code>initServer</code> calls <code>aeCreateTimeEvent</code> to add a timed event to <code>timeEventHead</code> field of <code>server.el</code>. <code>timeEventHead</code> is a pointer to a list of such timed events. The call to <code>aeCreateTimeEvent</code> from <code>redis.c:initServer</code> function is given below:</p><p><code>redis.c:serverCron</code> performs many operations that helps keep Redis running properly.</p>","link":"/alpha/topics/internals-rediseventlib.html","spaLink":"#/alpha/topics/internals-rediseventlib","title":"AECREATETIMEEVENT"},{"content":"<h2 id=\"redis-event-library-aecreatefileevent\"><code>aeCreateFileEvent</code></h2><p>The essence of <code>aeCreateFileEvent</code> function is to execute <a href=\"http://man.cx/epoll_ctl\"><code>epoll_ctl</code></a> system call which adds a watch for <code>EPOLLIN</code> event on the <em>listening descriptor</em> create by <code>anetTcpServer</code> and associate it with the <code>epoll</code> descriptor created by a call to <code>aeCreateEventLoop</code>.</p><p>Following is an explanation of what precisely <code>aeCreateFileEvent</code> does when called from <code>redis.c:initServer</code>.</p><p><code>initServer</code> passes the following arguments to <code>aeCreateFileEvent</code>:</p><ul>\n<li><code>server.el</code>: The event loop created by <code>aeCreateEventLoop</code>. The <code>epoll</code> descriptor is got from <code>server.el</code>.</li>\n<li><code>server.fd</code>: The <em>listening descriptor</em> that also serves as an index to access the relevant file event structure from the <code>eventLoop-&gt;events</code> table and store extra information like the callback function.</li>\n<li><code>AE_READABLE</code>: Signifies that <code>server.fd</code> has to be watched for <code>EPOLLIN</code> event.</li>\n<li><code>acceptHandler</code>: The function that has to be executed when the event being watched for is ready. This function pointer is stored in <code>eventLoop-&gt;events[server.fd]-&gt;rfileProc</code>.</li>\n</ul><p>This completes the initialization of Redis event loop.</p>","link":"/alpha/topics/internals-rediseventlib.html","spaLink":"#/alpha/topics/internals-rediseventlib","title":"AECREATEFILEEVENT"},{"content":"<h2 id=\"redis-event-library-event-loop-processing\">Event Loop Processing</h2><p><code>ae.c:aeMain</code> called from <code>redis.c:main</code> does the job of processing the event loop that is initialized in the previous phase.</p><p><code>ae.c:aeMain</code> calls <code>ae.c:aeProcessEvents</code> in a while loop that processes pending time and file events.</p>","link":"/alpha/topics/internals-rediseventlib.html","spaLink":"#/alpha/topics/internals-rediseventlib","title":"EVENT LOOP PROCESSING"},{"content":"<h2 id=\"redis-event-library-aeprocessevents\"><code>aeProcessEvents</code></h2><p><code>ae.c:aeProcessEvents</code> looks for the time event that will be pending in the smallest amount of time by calling <code>ae.c:aeSearchNearestTimer</code> on the event loop. In our case there is only one timer event in the event loop that was created by <code>ae.c:aeCreateTimeEvent</code>.</p><p>Remember, that timer event created by <code>aeCreateTimeEvent</code> has by now probably elapsed because it had a expiry time of one millisecond. Since, the timer has already expired the seconds and microseconds fields of the <code>tvp</code> <code>timeval</code> structure variable is initialized to zero.</p><p>The <code>tvp</code> structure variable along with the event loop variable is passed to <code>ae_epoll.c:aeApiPoll</code>.</p><p><code>aeApiPoll</code> functions does a <a href=\"http://man.cx/epoll_wait\"><code>epoll_wait</code></a> on the <code>epoll</code> descriptor and populates the <code>eventLoop-&gt;fired</code> table with the details:</p><ul>\n<li><code>fd</code>: The descriptor that is now ready to do a read/write operation depending on the mask value.</li>\n<li><code>mask</code>: The read/write event that can now be performed on the corresponding descriptor.</li>\n</ul><p><code>aeApiPoll</code> returns the number of such file events ready for operation. Now to put things in context, if any client has requested for a connection then <code>aeApiPoll</code> would have noticed it and populated the <code>eventLoop-&gt;fired</code> table with an entry of the descriptor being the <em>listening descriptor</em> and mask being <code>AE_READABLE</code>.</p><p>Now, <code>aeProcessEvents</code> calls the <code>redis.c:acceptHandler</code> registered as the callback. <code>acceptHandler</code> executes <a href=\"http://man.cx/accept\">accept</a> on the <em>listening descriptor</em> returning a <em>connected descriptor</em> with the client. <code>redis.c:createClient</code> adds a file event on the <em>connected descriptor</em> through a call to <code>ae.c:aeCreateFileEvent</code> like below:</p><p><code>c</code> is the <code>redisClient</code> structure variable and <code>c-&gt;fd</code> is the connected descriptor.</p><p>Next the <code>ae.c:aeProcessEvent</code> calls <code>ae.c:processTimeEvents</code></p>","link":"/alpha/topics/internals-rediseventlib.html","spaLink":"#/alpha/topics/internals-rediseventlib","title":"AEPROCESSEVENTS"},{"content":"<h2 id=\"redis-event-library-processtimeevents\"><code>processTimeEvents</code></h2><p><code>ae.processTimeEvents</code> iterates over list of time events starting at <code>eventLoop-&gt;timeEventHead</code>.</p><p>For every timed event that has elapsed <code>processTimeEvents</code> calls the registered callback. In this case it calls the only timed event callback registered, that is, <code>redis.c:serverCron</code>. The callback returns the time in milliseconds after which the callback must be called again. This change is recorded via a call to <code>ae.c:aeAddMilliSeconds</code> and will be handled on the next iteration of <code>ae.c:aeMain</code> while loop.</p><p>That’s all.</p>","link":"/alpha/topics/internals-rediseventlib.html","spaLink":"#/alpha/topics/internals-rediseventlib","title":"PROCESSTIMEEVENTS"},{"content":"<h1 id=\"hacking-strings\">Hacking Strings</h1><p>The implementation of Redis strings is contained in <code>sds.c</code> (<code>sds</code> stands for Simple Dynamic Strings).</p><p>The C structure <code>sdshdr</code> declared in <code>sds.h</code> represents a Redis string:</p><p>The <code>buf</code> character array stores the actual string.</p><p>The <code>len</code> field stores the length of <code>buf</code>. This makes obtaining the length\nof a Redis string an O(1) operation.</p><p>The <code>free</code> field stores the number of additional bytes available for use.</p><p>Together the <code>len</code> and <code>free</code> field can be thought of as holding the metadata of the <code>buf</code> character array.</p>","link":"/alpha/topics/internals-sds.html","spaLink":"#/alpha/topics/internals-sds","title":"HACKING STRINGS"},{"content":"<h2 id=\"hacking-strings-creating-redis-strings\">Creating Redis Strings</h2><p>A new data type named <code>sds</code> is defined in <code>sds.h</code> to be a synonym for a character pointer:</p><p><code>sdsnewlen</code> function defined in <code>sds.c</code> creates a new Redis String:</p><p>Remember a Redis string is a variable of type <code>struct sdshdr</code>. But <code>sdsnewlen</code> returns a character pointer!!</p><p>That’s a trick and needs some explanation.</p><p>Suppose I create a Redis string using <code>sdsnewlen</code> like below:</p><p>This creates a new variable of type <code>struct sdshdr</code> allocating memory for <code>len</code> and <code>free</code>\nfields as well as for the <code>buf</code> character array.</p><p>After <code>sdsnewlen</code> successfully creates a Redis string the result is something like:</p><p><code>sdsnewlen</code> returns <code>sh-&gt;buf</code> to the caller.</p><p>What do you do if you need to free the Redis string pointed by <code>sh</code>?</p><p>You want the pointer <code>sh</code> but you only have the pointer <code>sh-&gt;buf</code>.</p><p>Can you get the pointer <code>sh</code> from <code>sh-&gt;buf</code>?</p><p>Yes. Pointer arithmetic. Notice from the above ASCII art that if you subtract\nthe size of two longs from <code>sh-&gt;buf</code> you get the pointer <code>sh</code>.</p><p>The <code>sizeof</code> two longs happens to be the size of <code>struct sdshdr</code>.</p><p>Look at <code>sdslen</code> function and see this trick at work:</p><p>Knowing this trick you could easily go through the rest of the functions in <code>sds.c</code>.</p><p>The Redis string implementation is hidden behind an interface that accepts only character pointers. The users of Redis strings need not care about how its implemented and treat Redis strings as a character pointer.</p>","link":"/alpha/topics/internals-sds.html","spaLink":"#/alpha/topics/internals-sds","title":"CREATING REDIS STRINGS"},{"content":"<h1 id=\"virtual-memory-technical-specification\">Virtual Memory technical specification</h1><p>This document details the internals of the Redis Virtual Memory subsystem. The intended audience is not the final user but programmers willing to understand or modify the Virtual Memory implementation.</p>","link":"/alpha/topics/internals-vm.html","spaLink":"#/alpha/topics/internals-vm","title":"VIRTUAL MEMORY TECHNICAL SPECIFICATION"},{"content":"<h2 id=\"virtual-memory-technical-specification-keys-vs-values-what-is-swapped-out\">Keys vs Values: what is swapped out?</h2><p>The goal of the VM subsystem is to free memory transferring Redis Objects from memory to disk. This is a very generic command, but specifically, Redis transfers only objects associated with <em>values</em>. In order to understand better this concept we’ll show, using the DEBUG command, how a key holding a value looks from the point of view of the Redis internals:</p><p>As you can see from the above output, the Redis top level hash table maps Redis Objects (keys) to other Redis Objects (values). The Virtual Memory is only able to swap <em>values</em> on disk, the objects associated to <em>keys</em> are always taken in memory: this trade off guarantees very good lookup performances, as one of the main design goals of the Redis VM is to have performances similar to Redis with VM disabled when the part of the dataset frequently used fits in RAM.</p>","link":"/alpha/topics/internals-vm.html","spaLink":"#/alpha/topics/internals-vm","title":"KEYS VS VALUES: WHAT IS SWAPPED OUT?"},{"content":"<h2 id=\"virtual-memory-technical-specification-how-does-a-swapped-value-looks-like-internally\">How does a swapped value looks like internally</h2><p>When an object is swapped out, this is what happens in the hash table entry:</p><ul>\n<li>The key continues to hold a Redis Object representing the key.</li>\n<li>The value is set to NULL</li>\n</ul><p>So you may wonder where we store the information that a given value (associated to a given key) was swapped out. Just in the key object!</p><p>This is how the Redis Object structure <em>robj</em> looks like:</p><p>As you can see there are a few fields about VM. The most important one is <em>storage</em>, that can be one of this values:</p><ul>\n<li>REDIS_VM_MEMORY: the associated value is in memory.</li>\n<li>REDIS_VM_SWAPPED: the associated values is swapped, and the value entry of the hash table is just set to NULL.</li>\n<li>REDIS_VM_LOADING: the value is swapped on disk, the entry is NULL, but there is a job to load the object from the swap to the memory (this field is only used when threaded VM is active).</li>\n<li>REDIS_VM_SWAPPING: the value is in memory, the entry is a pointer to the actual Redis Object, but there is an I/O job in order to transfer this value to the swap file.</li>\n</ul><p>If an object is swapped on disk (REDIS<em>VM_SWAPPED or REDIS_VM_LOADING), how do we know where it is stored, what type it is, and so forth? That’s simple: the _vtype</em> field is set to the original type of the Redis object swapped, while the <em>vm</em> field (that is a <em>redisObjectVM</em> structure) holds information about the location of the object. This is the definition of this additional structure:</p><p>As you can see the structure contains the page at which the object is located in the swap file, the number of pages used, and the last access time of the object (this is very useful for the algorithm that select what object is a good candidate for swapping, as we want to transfer on disk objects that are rarely accessed).</p><p>As you can see, while all the other fields are using unused bytes in the old Redis Object structure (we had some free bit due to natural memory alignment concerns), the <em>vm</em> field is new, and indeed uses additional memory. Should we pay such a memory cost even when VM is disabled? No! This is the code to create a new Redis Object:</p><p>As you can see if the VM system is not enabled we allocate just <code>sizeof(*o)-sizeof(struct redisObjectVM)</code> of memory. Given that the <em>vm</em> field is the last in the object structure, and that this fields are never accessed if VM is disabled, we are safe and Redis without VM does not pay the memory overhead.</p>","link":"/alpha/topics/internals-vm.html","spaLink":"#/alpha/topics/internals-vm","title":"HOW DOES A SWAPPED VALUE LOOKS LIKE INTERNALLY"},{"content":"<h2 id=\"virtual-memory-technical-specification-the-swap-file\">The Swap File</h2><p>The next step in order to understand how the VM subsystem works is understanding how objects are stored inside the swap file. The good news is that’s not some kind of special format, we just use the same format used to store the objects in .rdb files, that are the usual dump files produced by Redis using the SAVE command.</p><p>The swap file is composed of a given number of pages, where every page size is a given number of bytes. This parameters can be changed in redis.conf, since different Redis instances may work better with different values: it depends on the actual data you store inside it. The following are the default values:</p><p>Redis takes a “bitmap” (an contiguous array of bits set to zero or one) in memory, every bit represent a page of the swap file on disk: if a given bit is set to 1, it represents a page that is already used (there is some Redis Object stored there), while if the corresponding bit is zero, the page is free.</p><p>Taking this bitmap (that will call the page table) in memory is a huge win in terms of performances, and the memory used is small: we just need 1 bit for every page on disk. For instance in the example below 134217728 pages of 32 bytes each (4GB swap file) is using just 16 MB of RAM for the page table.</p>","link":"/alpha/topics/internals-vm.html","spaLink":"#/alpha/topics/internals-vm","title":"THE SWAP FILE"},{"content":"<h2 id=\"virtual-memory-technical-specification-transferring-objects-from-memory-to-swap\">Transferring objects from memory to swap</h2><p>In order to transfer an object from memory to disk we need to perform the following steps (assuming non threaded VM, just a simple blocking approach):</p><ul>\n<li>Find how many pages are needed in order to store this object on the swap file. This is trivially accomplished just calling the function <code>rdbSavedObjectPages</code> that returns the number of pages used by an object on disk. Note that this function does not duplicate the .rdb saving code just to understand what will be the length <em>after</em> an object will be saved on disk, we use the trick of opening /dev/null and writing the object there, finally calling <code>ftello</code> in order check the amount of bytes required. What we do basically is to save the object on a virtual very fast file, that is, /dev/null.</li>\n<li>Now that we know how many pages are required in the swap file, we need to find this number of contiguous free pages inside the swap file. This task is accomplished by the <code>vmFindContiguousPages</code> function. As you can guess this function may fail if the swap is full, or so fragmented that we can’t easily find the required number of contiguous free pages. When this happens we just abort the swapping of the object, that will continue to live in memory.</li>\n<li>Finally we can write the object on disk, at the specified position, just calling the function <code>vmWriteObjectOnSwap</code>.</li>\n</ul><p>As you can guess once the object was correctly written in the swap file, it is freed from memory, the storage field in the associated key is set to REDIS_VM_SWAPPED, and the used pages are marked as used in the page table.</p>","link":"/alpha/topics/internals-vm.html","spaLink":"#/alpha/topics/internals-vm","title":"TRANSFERRING OBJECTS FROM MEMORY TO SWAP"},{"content":"<h2 id=\"virtual-memory-technical-specification-loading-objects-back-in-memory\">Loading objects back in memory</h2><p>Loading an object from swap to memory is simpler, as we already know where the object is located and how many pages it is using. We also know the type of the object (the loading functions are required to know this information, as there is no header or any other information about the object type on disk), but this is stored in the <em>vtype</em> field of the associated key as already seen above.</p><p>Calling the function <code>vmLoadObject</code> passing the key object associated to the value object we want to load back is enough. The function will also take care of fixing the storage type of the key (that will be REDIS_VM_MEMORY), marking the pages as freed in the page table, and so forth.</p><p>The return value of the function is the loaded Redis Object itself, that we’ll have to set again as value in the main hash table (instead of the NULL value we put in place of the object pointer when the value was originally swapped out).</p>","link":"/alpha/topics/internals-vm.html","spaLink":"#/alpha/topics/internals-vm","title":"LOADING OBJECTS BACK IN MEMORY"},{"content":"<h2 id=\"virtual-memory-technical-specification-how-blocking-vm-works\">How blocking VM works</h2><p>Now we have all the building blocks in order to describe how the blocking VM works. First of all, an important detail about configuration. In order to enable blocking VM in Redis <code>server.vm_max_threads</code> must be set to zero.\nWe’ll see later how this max number of threads info is used in the threaded VM, for now all it’s needed to now is that Redis reverts to fully blocking VM when this is set to zero.</p><p>We also need to introduce another important VM parameter, that is, <code>server.vm_max_memory</code>. This parameter is very important as it is used in order to trigger swapping: Redis will try to swap objects only if it is using more memory than the max memory setting, otherwise there is no need to swap as we are matching the user requested memory usage.</p>","link":"/alpha/topics/internals-vm.html","spaLink":"#/alpha/topics/internals-vm","title":"HOW BLOCKING VM WORKS"},{"content":"<h2 id=\"virtual-memory-technical-specification-blocking-vm-swapping\">Blocking VM swapping</h2><p>Swapping of object from memory to disk happens in the cron function. This function used to be called every second, while in the recent Redis versions on git it is called every 100 milliseconds (that is, 10 times per second).\nIf this function detects we are out of memory, that is, the memory used is greater than the vm-max-memory setting, it starts transferring objects from memory to disk in a loop calling the function <code>vmSwapOneObect</code>. This function takes just one argument, if 0 it will swap objects in a blocking way, otherwise if it is 1, I/O threads are used. In the blocking scenario we just call it with zero as argument.</p><p>vmSwapOneObject acts performing the following steps:</p><ul>\n<li>The key space in inspected in order to find a good candidate for swapping (we’ll see later what a good candidate for swapping is).</li>\n<li>The associated value is transferred to disk, in a blocking way.</li>\n<li>The key storage field is set to REDIS<em>VM_SWAPPED, while the _vm</em> fields of the object are set to the right values (the page index where the object was swapped, and the number of pages used to swap it).</li>\n<li>Finally the value object is freed and the value entry of the hash table is set to NULL.</li>\n</ul><p>The function is called again and again until one of the following happens: there is no way to swap more objects because either the swap file is full or nearly all the objects are already transferred on disk, or simply the memory usage is already under the vm-max-memory parameter.</p>","link":"/alpha/topics/internals-vm.html","spaLink":"#/alpha/topics/internals-vm","title":"BLOCKING VM SWAPPING"},{"content":"<h2 id=\"virtual-memory-technical-specification-what-values-to-swap-when-we-are-out-of-memory\">What values to swap when we are out of memory?</h2><p>Understanding what’s a good candidate for swapping is not too hard. A few objects at random are sampled, and for each their <em>swappability</em> is commuted as:</p><p>The age is the number of seconds the key was not requested, while size_in_memory is a fast estimation of the amount of memory (in bytes) used by the object in memory. So we try to swap out objects that are rarely accessed, and we try to swap bigger objects over smaller one, but the latter is a less important factor (because of the logarithmic function used). This is because we don’t want bigger objects to be swapped out and in too often as the bigger the object the more I/O and CPU is required in order to transfer it.</p>","link":"/alpha/topics/internals-vm.html","spaLink":"#/alpha/topics/internals-vm","title":"WHAT VALUES TO SWAP WHEN WE ARE OUT OF MEMORY?"},{"content":"<h2 id=\"virtual-memory-technical-specification-blocking-vm-loading\">Blocking VM loading</h2><p>What happens if an operation against a key associated with a swapped out object is requested? For instance Redis may just happen to process the following command:</p><p>If the value object of the <code>foo</code> key is swapped we need to load it back in memory before processing the operation. In Redis the key lookup process is centralized in the <code>lookupKeyRead</code> and <code>lookupKeyWrite</code> functions, this two functions are used in the implementation of all the Redis commands accessing the keyspace, so we have a single point in the code where to handle the loading of the key from the swap file to memory.</p><p>So this is what happens:</p><ul>\n<li>The user calls some command having as argument a swapped key</li>\n<li>The command implementation calls the lookup function</li>\n<li>The lookup function search for the key in the top level hash table. If the value associated with the requested key is swapped (we can see that checking the <em>storage</em> field of the key object), we load it back in memory in a blocking way before to return to the user.</li>\n</ul><p>This is pretty straightforward, but things will get more <em>interesting</em> with the threads. From the point of view of the blocking VM the only real problem is the saving of the dataset using another process, that is, handling BGSAVE and BGREWRITEAOF commands.</p>","link":"/alpha/topics/internals-vm.html","spaLink":"#/alpha/topics/internals-vm","title":"BLOCKING VM LOADING"},{"content":"<h2 id=\"virtual-memory-technical-specification-background-saving-when-vm-is-active\">Background saving when VM is active</h2><p>The default Redis way to persist on disk is to create .rdb files using a child process. Redis calls the fork() system call in order to create a child, that has the exact copy of the in memory dataset, since fork duplicates the whole program memory space (actually thanks to a technique called Copy on Write memory pages are shared between the parent and child process, so the fork() call will not require too much memory).</p><p>In the child process we have a copy of the dataset in a given point in the time. Other commands issued by clients will just be served by the parent process and will not modify the child data.</p><p>The child process will just store the whole dataset into the dump.rdb file and finally will exit. But what happens when the VM is active? Values can be swapped out so we don’t have all the data in memory, and we need to access the swap file in order to retrieve the swapped values. While child process is saving the swap file is shared between the parent and child process, since:</p><ul>\n<li>The parent process needs to access the swap file in order to load values back into memory if an operation against swapped out values are performed.</li>\n<li>The child process needs to access the swap file in order to retrieve the full dataset while saving the data set on disk.</li>\n</ul><p>In order to avoid problems while both the processes are accessing the same swap file we do a simple thing, that is, not allowing values to be swapped out in the parent process while a background saving is in progress. This way both the processes will access the swap file in read only. This approach has the problem that while the child process is saving no new values can be transferred on the swap file even if Redis is using more memory than the max memory parameters dictates. This is usually not a problem as the background saving will terminate in a short amount of time and if still needed a percentage of values will be swapped on disk ASAP.</p><p>An alternative to this scenario is to enable the Append Only File that will have this problem only when a log rewrite is performed using the BGREWRITEAOF command.</p>","link":"/alpha/topics/internals-vm.html","spaLink":"#/alpha/topics/internals-vm","title":"BACKGROUND SAVING WHEN VM IS ACTIVE"},{"content":"<h2 id=\"virtual-memory-technical-specification-the-problem-with-the-blocking-vm\">The problem with the blocking VM</h2><p>The problem of blocking VM is that… it’s blocking :)\nThis is not a problem when Redis is used in batch processing activities, but for real-time usage one of the good points of Redis is the low latency. The blocking VM will have bad latency behaviors as when a client is accessing a swapped out value, or when Redis needs to swap out values, no other clients will be served in the meantime.</p><p>Swapping out keys should happen in background. Similarly when a client is accessing a swapped out value other clients accessing in memory values should be served mostly as fast as when VM is disabled. Only the clients dealing with swapped out keys should be delayed.</p><p>All this limitations called for a non-blocking VM implementation.</p>","link":"/alpha/topics/internals-vm.html","spaLink":"#/alpha/topics/internals-vm","title":"THE PROBLEM WITH THE BLOCKING VM"},{"content":"<h2 id=\"virtual-memory-technical-specification-threaded-vm\">Threaded VM</h2><p>There are basically three main ways to turn the blocking VM into a non blocking one.\n<em> 1: One way is obvious, and in my opinion, not a good idea at all, that is, turning Redis itself into a threaded server: if every request is served by a different thread automatically other clients don’t need to wait for blocked ones. Redis is fast, exports atomic operations, has no locks, and is just 10k lines of code, </em>because<em> it is single threaded, so this was not an option for me.\n</em> 2: Using non-blocking I/O against the swap file. After all you can think Redis already event-loop based, why don’t just handle disk I/O in a non-blocking fashion? I also discarded this possibility because of two main reasons. One is that non blocking file operations, unlike sockets, are an incompatibility nightmare. It’s not just like calling select, you need to use OS-specific things. The other problem is that the I/O is just one part of the time consumed to handle VM, another big part is the CPU used in order to encode/decode data to/from the swap file. This is I picked option three, that is…\n* 3: Using I/O threads, that is, a pool of threads handling the swap I/O operations. This is what the Redis VM is using, so let’s detail how this works.</p>","link":"/alpha/topics/internals-vm.html","spaLink":"#/alpha/topics/internals-vm","title":"THREADED VM"},{"content":"<h2 id=\"virtual-memory-technical-specification-io-threads\">I/O Threads</h2><p>The threaded VM design goals where the following, in order of importance:</p><ul>\n<li>Simple implementation, little room for race conditions, simple locking, VM system more or less completely decoupled from the rest of Redis code.</li>\n<li>Good performances, no locks for clients accessing values in memory.</li>\n<li>Ability to decode/encode objects in the I/O threads.</li>\n</ul><p>The above goals resulted in an implementation where the Redis main thread (the one serving actual clients) and the I/O threads communicate using a queue of jobs, with a single mutex.\nBasically when main thread requires some work done in the background by some I/O thread, it pushes an I/O job structure in the <code>server.io_newjobs</code> queue (that is, just a linked list). If there are no active I/O threads, one is started. At this point some I/O thread will process the I/O job, and the result of the processing is pushed in the <code>server.io_processed</code> queue. The I/O thread will send a byte using an UNIX pipe to the main thread in order to signal that a new job was processed and the result is ready to be processed.</p><p>This is how the <code>iojob</code> structure looks like:</p><p>There are just three type of jobs that an I/O thread can perform (the type is specified by the <code>type</code> field of the structure):</p><ul>\n<li>REDIS_IOJOB_LOAD: load the value associated to a given key from swap to memory. The object offset inside the swap file is <code>page</code>, the object type is <code>key-&gt;vtype</code>. The result of this operation will populate the <code>val</code> field of the structure.</li>\n<li>REDIS_IOJOB_PREPARE_SWAP: compute the number of pages needed in order to save the object pointed by <code>val</code> into the swap. The result of this operation will populate the <code>pages</code> field.</li>\n<li>REDIS_IOJOB_DO_SWAP: Transfer the object pointed by <code>val</code> to the swap file, at page offset <code>page</code>.</li>\n</ul><p>The main thread delegates just the above three tasks. All the rest is handled by the main thread itself, for instance finding a suitable range of free pages in the swap file page table (that is a fast operation), deciding what object to swap, altering the storage field of a Redis object to reflect the current state of a value.</p>","link":"/alpha/topics/internals-vm.html","spaLink":"#/alpha/topics/internals-vm","title":"I/O THREADS"},{"content":"<h2 id=\"virtual-memory-technical-specification-non-blocking-vm-as-probabilistic-enhancement-of-blocking-vm\">Non blocking VM as probabilistic enhancement of blocking VM</h2><p>So now we have a way to request background jobs dealing with slow VM operations. How to add this to the mix of the rest of the work done by the main thread? While blocking VM was aware that an object was swapped out just when the object was looked up, this is too late for us: in C it is not trivial to start a background job in the middle of the command, leave the function, and re-enter in the same point the computation when the I/O thread finished what we requested (that is, no co-routines or continuations or alike).</p><p>Fortunately there was a much, much simpler way to do this. And we love simple things: basically consider the VM implementation a blocking one, but add an optimization (using non the no blocking VM operations we are able to perform) to make the blocking <em>very</em> unlikely.</p><p>This is what we do:</p><ul>\n<li>Every time a client sends us a command, <em>before</em> the command is executed, we examine the argument vector of the command in search for swapped keys. After all we know for every command what arguments are keys, as the Redis command format is pretty simple.</li>\n<li>If we detect that at least a key in the requested command is swapped on disk, we block the client instead of really issuing the command. For every swapped value associated to a requested key, an I/O job is created, in order to bring the values back in memory. The main thread continues the execution of the event loop, without caring about the blocked client.</li>\n<li>In the meanwhile, I/O threads are loading values in memory. Every time an I/O thread finished loading a value, it sends a byte to the main thread using an UNIX pipe. The pipe file descriptor has a readable event associated in the main thread event loop, that is the function <code>vmThreadedIOCompletedJob</code>. If this function detects that all the values needed for a blocked client were loaded, the client is restarted and the original command called.</li>\n</ul><p>So you can think at this as a blocked VM that almost always happen to have the right keys in memory, since we pause clients that are going to issue commands about swapped out values until this values are loaded.</p><p>If the function checking what argument is a key fails in some way, there is no problem: the lookup function will see that a given key is associated to a swapped out value and will block loading it. So our non blocking VM reverts to a blocking one when it is not possible to anticipate what keys are touched.</p><p>For instance in the case of the SORT command used together with the GET or BY options, it is not trivial to know beforehand what keys will be requested, so at least in the first implementation, SORT BY/GET resorts to the blocking VM implementation.</p>","link":"/alpha/topics/internals-vm.html","spaLink":"#/alpha/topics/internals-vm","title":"NON BLOCKING VM AS PROBABILISTIC ENHANCEMENT OF BLOCKING VM"},{"content":"<h2 id=\"virtual-memory-technical-specification-blocking-clients-on-swapped-keys\">Blocking clients on swapped keys</h2><p>How to block clients? To suspend a client in an event-loop based server is pretty trivial. All we do is canceling its read handler. Sometimes we do something different (for instance for BLPOP) that is just marking the client as blocked, but not processing new data (just accumulating the new data into input buffers).</p>","link":"/alpha/topics/internals-vm.html","spaLink":"#/alpha/topics/internals-vm","title":"BLOCKING CLIENTS ON SWAPPED KEYS"},{"content":"<h2 id=\"virtual-memory-technical-specification-aborting-io-jobs\">Aborting I/O jobs</h2><p>There is something hard to solve about the interactions between our blocking and non blocking VM, that is, what happens if a blocking operation starts about a key that is also “interested” by a non blocking operation at the same time?</p><p>For instance while SORT BY is executed, a few keys are being loaded in a blocking manner by the sort command. At the same time, another client may request the same keys with a simple <em>GET key</em> command, that will trigger the creation of an I/O job to load the key in background.</p><p>The only simple way to deal with this problem is to be able to kill I/O jobs in the main thread, so that if a key that we want to load or swap in a blocking way is in the REDIS_VM_LOADING or REDIS_VM_SWAPPING state (that is, there is an I/O job about this key), we can just kill the I/O job about this key, and go ahead with the blocking operation we want to perform.</p><p>This is not as trivial as it is. In a given moment an I/O job can be in one of the following three queues:</p><ul>\n<li>server.io_newjobs: the job was already queued but no thread is handling it.</li>\n<li>server.io_processing: the job is being processed by an I/O thread.</li>\n<li>server.io_processed: the job was already processed.\nThe function able to kill an I/O job is <code>vmCancelThreadedIOJob</code>, and this is what it does:</li>\n<li>If the job is in the newjobs queue, that’s simple, removing the iojob structure from the queue is enough as no thread is still executing any operation.</li>\n<li>If the job is in the processing queue, a thread is messing with our job (and possibly with the associated object!). The only thing we can do is waiting for the item to move to the next queue in a <em>blocking way</em>. Fortunately this condition happens very rarely so it’s not a performance problem.</li>\n<li>If the job is in the processed queue, we just mark it as <em>canceled</em> marking setting the <code>canceled</code> field to 1 in the iojob structure. The function processing completed jobs will just ignored and free the job instead of really processing it.</li>\n</ul>","link":"/alpha/topics/internals-vm.html","spaLink":"#/alpha/topics/internals-vm","title":"ABORTING I/O JOBS"},{"content":"<h2 id=\"virtual-memory-technical-specification-questions\">Questions?</h2><p>This document is in no way complete, the only way to get the whole picture is reading the source code, but it should be a good introduction in order to make the code review / understanding a lot simpler.</p><p>Something is not clear about this page? Please leave a comment and I’ll try to address the issue possibly integrating the answer in this document.</p>","link":"/alpha/topics/internals-vm.html","spaLink":"#/alpha/topics/internals-vm","title":"QUESTIONS?"},{"content":"<h1 id=\"redis-internals-documentation\">Redis Internals documentation</h1><p>Redis source code is not very big (just 20k lines of code for the 2.2 release) and we try hard to make it simple and easy to understand. However we have some documentation explaining selected parts of the Redis internals.</p>","link":"/alpha/topics/internals.html","spaLink":"#/alpha/topics/internals","title":"REDIS INTERNALS DOCUMENTATION"},{"content":"<h2 id=\"redis-internals-documentation-redis-dynamic-strings\">Redis dynamic strings</h2><p>String is the basic building block of Redis types. </p><p>Redis is a key-value store.\nAll Redis keys are strings and its also the simplest value type.</p><p>Lists, sets, sorted sets and hashes are other more complex value types and even\nthese are composed of strings.</p><p><a href=\"/topics/internals-sds\">Hacking Strings</a> documents the Redis String implementation details.</p>","link":"/alpha/topics/internals.html","spaLink":"#/alpha/topics/internals","title":"REDIS DYNAMIC STRINGS"},{"content":"<h2 id=\"redis-internals-documentation-redis-virtual-memory\">Redis Virtual Memory</h2><p>We have a document explaining <a href=\"/topics/internals-vm\">virtual memory implementation details</a>, but warning: this document refers to the 2.0 VM implementation. 2.2 is different… and better.</p>","link":"/alpha/topics/internals.html","spaLink":"#/alpha/topics/internals","title":"REDIS VIRTUAL MEMORY"},{"content":"<h2 id=\"redis-internals-documentation-redis-event-library\">Redis Event Library</h2><p>Read <a href=\"/topics/internals-eventlib\">event library</a> to understand what an event library does and why its needed.</p><p><a href=\"/topics/internals-rediseventlib\">Redis event library</a> documents the implementation details of the event library used by Redis.</p>","link":"/alpha/topics/internals.html","spaLink":"#/alpha/topics/internals","title":"REDIS EVENT LIBRARY"},{"content":"<h1 id=\"introduction-to-redis\">Introduction to Redis</h1><p>Redis is an open source (BSD licensed), in-memory <strong>data structure store</strong>, used as database, cache and message broker. It supports data structures such as\n<a href=\"/topics/data-types-intro#strings\">strings</a>, <a href=\"/topics/data-types-intro#hashes\">hashes</a>, <a href=\"/topics/data-types-intro#lists\">lists</a>, <a href=\"/topics/data-types-intro#sets\">sets</a>, <a href=\"/topics/data-types-intro#sorted-sets\">sorted sets</a> with range queries, <a href=\"/topics/data-types-intro#bitmaps\">bitmaps</a>, <a href=\"/topics/data-types-intro#hyperloglogs\">hyperloglogs</a> and <a href=\"/commands/geoadd\">geospatial indexes</a> with radius queries. Redis has built-in <a href=\"/topics/replication\">replication</a>, <a href=\"/commands/eval\">Lua scripting</a>, <a href=\"/topics/lru-cache\">LRU eviction</a>, <a href=\"/topics/transactions\">transactions</a> and different levels of <a href=\"/topics/persistence\">on-disk persistence</a>, and provides high availability via <a href=\"/topics/sentinel\">Redis Sentinel</a> and automatic partitioning with <a href=\"/topics/cluster-tutorial\">Redis Cluster</a>.</p><p>You can run <strong>atomic operations</strong>\non these types, like <a href=\"/commands/append\">appending to a string</a>;\n<a href=\"/commands/hincrby\">incrementing the value in a hash</a>; <a href=\"/commands/lpush\">pushing an element to a\nlist</a>; <a href=\"/commands/sinter\">computing set intersection</a>,\n<a href=\"/commands/sunion\">union</a> and <a href=\"/commands/sdiff\">difference</a>;\nor <a href=\"/commands/zrangebyscore\">getting the member with highest ranking in a sorted\nset</a>.</p><p>In order to achieve its outstanding performance, Redis works with an\n<strong>in-memory dataset</strong>. Depending on your use case, you can persist it either\nby <a href=\"/topics/persistence#snapshotting\">dumping the dataset to disk</a>\nevery once in a while, or by <a href=\"/topics/persistence#append-only-file\">appending each command to a\nlog</a>. Persistence can be optionally\ndisabled, if you just need a feature-rich, networked, in-memory cache.</p><p>Redis also supports trivial-to-setup <a href=\"/topics/replication\">master-slave asynchronous replication</a>, with very fast non-blocking first synchronization, auto-reconnection with partial resynchronization on net split.</p><p>Other features include:</p><ul>\n<li><a href=\"/topics/transactions\">Transactions</a></li>\n<li><a href=\"/topics/pubsub\">Pub/Sub</a></li>\n<li><a href=\"/commands/eval\">Lua scripting</a></li>\n<li><a href=\"/commands/expire\">Keys with a limited time-to-live</a></li>\n<li><a href=\"/topics/lru-cache\">LRU eviction of keys</a></li>\n<li><a href=\"/topics/sentinel\">Automatic failover</a></li>\n</ul><p>You can use Redis from <a href=\"/clients\">most programming languages</a> out there. </p><p>Redis is written in <strong>ANSI C</strong> and works in most POSIX systems like Linux,\n*BSD, OS X without external dependencies. Linux and OS X are the two operating systems where Redis is developed and more tested, and we <strong>recommend using Linux for deploying</strong>. Redis may work in Solaris-derived systems like SmartOS, but the support is <em>best effort</em>. There\nis no official support for Windows builds, but Microsoft develops and\nmaintains a <a href=\"https://github.com/MSOpenTech/redis\">Win-64 port of Redis</a>.</p>","link":"/alpha/topics/introduction.html","spaLink":"#/alpha/topics/introduction","title":"INTRODUCTION TO REDIS"},{"content":"<h1 id=\"redis-latency-monitoring-framework\">Redis latency monitoring framework</h1><p>Redis is often used in the context of demanding use cases, where it\nserves a big amount of queries per second per instance, and at the same\ntime, there are very strict latency requirements both for the average response\ntime and for the worst case latency.</p><p>While Redis is an in memory system, it deals with the operating system in\ndifferent ways, for example, in the context of persisting to disk.\nMoreover Redis implements a rich set of commands. Certain commands\nare fast and run in constant or logarithmic time, other commands are slower\nO(N) commands, that can cause latency spikes.</p><p>Finally Redis is single threaded: this is usually an advantage\nfrom the point of view of the amount of work it can perform per core, and in\nthe latency figures it is able to provide, but at the same time it poses\na challenge from the point of view of latency, since the single\nthread must be able to perform certain tasks incrementally, like for\nexample keys expiration, in a way that does not impact the other clients\nthat are served.</p><p>For all these reasons, Redis 2.8.13 introduced a new feature called\n<strong>Latency Monitoring</strong>, that helps the user to check and troubleshoot possible\nlatency problems. Latency monitoring is composed of the following conceptual\nparts:</p><ul>\n<li>Latency hooks that sample different latency sensitive code paths.</li>\n<li>Time series recording of latency spikes split by different event.</li>\n<li>Reporting engine to fetch raw data from the time series.</li>\n<li>Analysis engine to provide human readable reports and hints according to the measurements.</li>\n</ul><p>The remaining part of this document covers the latency monitoring subsystem\ndetails, however for more information about the general topic of Redis\nand latency, please read the <a href=\"/topics/latency\">Redis latency problems troubleshooting</a> page in this documentation.</p>","link":"/alpha/topics/latency-monitor.html","spaLink":"#/alpha/topics/latency-monitor","title":"REDIS LATENCY MONITORING FRAMEWORK"},{"content":"<h2 id=\"redis-latency-monitoring-framework-events-and-time-series\">Events and time series</h2><p>Different monitored code paths have different names, and are called <em>events</em>.\nFor example <code>command</code> is an event measuring latency spikes of possibly slow\ncommands executions, while <code>fast-command</code> is the event name for the monitoring\nof the O(1) and O(log N) commands. Other events are less generic, and monitor\na very specific operation performed by Redis. For example the <code>fork</code> event\nonly monitors the time taken by Redis to execute the <code>fork(2)</code> system call.</p><p>A latency spike is an event that runs in more time than the configured latency\nthreshold. There is a separated time series associated with every monitored\nevent. This is how the time series work:</p><ul>\n<li>Every time a latency spike happens, it is logged in the appropriate time series.</li>\n<li>Every time series is composed of 160 elements.</li>\n<li>Each element is a pair: an unix timestamp of the time the latency spike was measured, and the number of milliseconds the event took to executed.</li>\n<li>Latency spikes for the same event happening in the same second are merged (by taking the maximum latency), so even if continuous latency spikes are measured for a given event, for example because the user set a very low threshold, at least 180 seconds of history are available.</li>\n<li>For every element the all-time maximum latency is recorded.</li>\n</ul>","link":"/alpha/topics/latency-monitor.html","spaLink":"#/alpha/topics/latency-monitor","title":"EVENTS AND TIME SERIES"},{"content":"<h2 id=\"redis-latency-monitoring-framework-how-to-enable-latency-monitoring\">How to enable latency monitoring</h2><p>What is high latency for an use case, is not high latency for another. There are applications where all the queries must be served in less than 1 millisecond and applications where from time to time a small percentage of clients experiencing a 2 seconds latency is acceptable.</p><p>So the first step to enable the latency monitor is to set a <strong>latency threshold</strong> in milliseconds. Only events that will take more than the specified threshold will be logged as latency spikes. The user should set the threshold according to its needs. For example if for the requirements of the application based on Redis the maximum acceptable latency is 100 milliseconds, the threshold should be set to such a value in order to log all the events blocking the server for a time equal or greater to 100 milliseconds.</p><p>The latency monitor can easily be enabled at runtime in a production server\nwith the following command:</p><p>By default monitoring is disabled (threshold set to 0), even if the actual cost of latency monitoring is near zero. However while the memory requirements of latency monitoring are very small, there is no good reason to raise the baseline memory usage of a Redis instance that is working well.</p>","link":"/alpha/topics/latency-monitor.html","spaLink":"#/alpha/topics/latency-monitor","title":"HOW TO ENABLE LATENCY MONITORING"},{"content":"<h2 id=\"redis-latency-monitoring-framework-information-reporting-with-the-latency-command\">Information reporting with the LATENCY command</h2><p>The user interface to the latency monitoring subsystem is the <code>LATENCY</code> command.\nLike many other Redis commands, <code>LATENCY</code> accept subcommands that modify the\nbehavior of the command. The next sections document each subcommand.</p>","link":"/alpha/topics/latency-monitor.html","spaLink":"#/alpha/topics/latency-monitor","title":"INFORMATION REPORTING WITH THE LATENCY COMMAND"},{"content":"<h2 id=\"redis-latency-monitoring-framework-latency-latest\">LATENCY LATEST</h2><p>The <code>LATENCY LATEST</code> command reports the latest latency events logged. Each event has the following fields:</p><ul>\n<li>Event name.</li>\n<li>Unix timestamp of the latest latency spike for the event.</li>\n<li>Latest event latency in millisecond.</li>\n<li>All time maximum latency for this event.</li>\n</ul><p>All time does not really mean the maximum latency since the Redis instance was\nstarted, because it is possible to reset events data using <code>LATENCY RESET</code> as we’ll see later.</p><p>The following is an example output:</p>","link":"/alpha/topics/latency-monitor.html","spaLink":"#/alpha/topics/latency-monitor","title":"LATENCY LATEST"},{"content":"<h2 id=\"redis-latency-monitoring-framework-latency-history-event-name\">LATENCY HISTORY <code>event-name</code></h2><p>The <code>LATENCY HISTORY</code> command is useful in order to fetch raw data from the\nevent time series, as timestamp-latency pairs. The command will return up\nto 160 elements for a given event. An application may want to fetch raw data\nin order to perform monitoring, display graphs, and so forth.</p><p>Example output:</p>","link":"/alpha/topics/latency-monitor.html","spaLink":"#/alpha/topics/latency-monitor","title":"LATENCY HISTORY EVENT-NAME"},{"content":"<h2 id=\"redis-latency-monitoring-framework-latency-reset-event-name-event-name\">LATENCY RESET [<code>event-name</code> … <code>event-name</code>]</h2><p>The <code>LATENCY RESET</code> command, if called without arguments, resets all the\nevents, discarding the currently logged latency spike events, and resetting\nthe maximum event time register.</p><p>It is possible to reset only specific events by providing the event names\nas arguments. The command returns the number of events time series that were\nreset during the command execution.</p>","link":"/alpha/topics/latency-monitor.html","spaLink":"#/alpha/topics/latency-monitor","title":"LATENCY RESET [EVENT-NAME … EVENT-NAME]"},{"content":"<h2 id=\"redis-latency-monitoring-framework-latency-graph-event-name\">LATENCY GRAPH <code>event-name</code></h2><p>Produces an ASCII-art style graph for the specified event:</p><p>The vertical labels under each graph column represent the amount of seconds,\nminutes, hours or days ago the event happened. For example “15s” means that the\nfirst graphed event happened 15 seconds ago.</p><p>The graph is normalized in the min-max scale so that the zero (the underscore\nin the lower row) is the minimum, and a # in the higher row is the maximum.</p><p>The graph subcommand is useful in order to get a quick idea about the trend\nof a given latency event without using additional tooling, and without the\nneed to interpret raw data as provided by <code>LATENCY HISTORY</code>.</p>","link":"/alpha/topics/latency-monitor.html","spaLink":"#/alpha/topics/latency-monitor","title":"LATENCY GRAPH EVENT-NAME"},{"content":"<h2 id=\"redis-latency-monitoring-framework-latency-doctor\">LATENCY DOCTOR</h2><p>The <code>LATENCY DOCTOR</code> command is the most powerful analysis tool in the latency\nmonitoring, and is able to provide additional statistical data like the average\nperiod between latency spikes, the median deviation, and an human readable\nanalysis of the event. For certain events, like <code>fork</code>, additional information\nis provided, like the rate at which the system forks processes.</p><p>This is the output you should post in the Redis mailing list if you are\nlooking for help about Latency related issues.</p><p>Example output:</p><p>The doctor has erratic psychological behaviors, so we recommend interacting with\nit carefully.</p>","link":"/alpha/topics/latency-monitor.html","spaLink":"#/alpha/topics/latency-monitor","title":"LATENCY DOCTOR"},{"content":"<h1 id=\"redis-latency-problems-troubleshooting\">Redis latency problems troubleshooting</h1><p>This document will help you understand what the problem could be if you\nare experiencing latency problems with Redis.</p><p>In this context <em>latency</em> is the maximum delay between the time a client\nissues a command and the time the reply to the command is received by the\nclient. Usually Redis processing time is extremely low, in the sub microsecond\nrange, but there are certain conditions leading to higher latency figures.</p>","link":"/alpha/topics/latency.html","spaLink":"#/alpha/topics/latency","title":"REDIS LATENCY PROBLEMS TROUBLESHOOTING"},{"content":"<h2 id=\"redis-latency-problems-troubleshooting-ive-little-time-give-me-the-checklist\">I’ve little time, give me the checklist</h2><p>The following documentation is very important in order to run Redis in\na low latency fashion. However I understand that we are busy people, so\nlet’s start with a quick checklist. If you fail following these steps, please\nreturn here to read the full documentation.</p><p>In general, use the following table for durability VS latency/performance tradeoffs, ordered from stronger safety to better latency.</p><p>And now for people with 15 minutes to spend, the details…</p>","link":"/alpha/topics/latency.html","spaLink":"#/alpha/topics/latency","title":"I’VE LITTLE TIME, GIVE ME THE CHECKLIST"},{"content":"<h2 id=\"redis-latency-problems-troubleshooting-measuring-latency\">Measuring latency</h2><p>If you are experiencing latency problems, probably you know how to measure\nit in the context of your application, or maybe your latency problem is very\nevident even macroscopically. However redis-cli can be used to measure the\nlatency of a Redis server in milliseconds, just try:</p>","link":"/alpha/topics/latency.html","spaLink":"#/alpha/topics/latency","title":"MEASURING LATENCY"},{"content":"<h2 id=\"redis-latency-problems-troubleshooting-using-the-internal-redis-latency-monitoring-subsystem\">Using the internal Redis latency monitoring subsystem</h2><p>Since Redis 2.8.13, Redis provides latency monitoring capabilities that\nare able to sample different execution paths to understand where the\nserver is blocking. This makes debugging of the problems illustrated in\nthis documentation much simpler, so we suggest to enable latency monitoring\nASAP. Please refer to the <a href=\"/topics/latency-monitor\">Latency monitor documentation</a>.</p><p>While the latency monitoring sampling and reporting capabilities will make\nsimpler to understand the source of latency in your Redis system, it is still\nadvised that you read this documentation extensively to better understand\nthe topic of Redis and latency spikes.</p>","link":"/alpha/topics/latency.html","spaLink":"#/alpha/topics/latency","title":"USING THE INTERNAL REDIS LATENCY MONITORING SUBSYSTEM"},{"content":"<h2 id=\"redis-latency-problems-troubleshooting-latency-baseline\">Latency baseline</h2><p>There is a kind of latency that is inherently part of the environment where\nyou run Redis, that is the latency provided by your operating system kernel\nand, if you are using virtualization, by the hypervisor you are using.</p><p>While this latency can’t be removed it is important to study it because\nit is the baseline, or in other words, you’ll not be able to achieve a Redis\nlatency that is better than the latency that every process running in your\nenvironment will experience because of the kernel or hypervisor implementation\nor setup.</p><p>We call this kind of latency <strong>intrinsic latency</strong>, and <code>redis-cli</code> starting\nfrom Redis version 2.8.7 is able to measure it. This is an example run\nunder Linux 3.11.0 running on an entry level server.</p><p>Note: the argument <code>100</code> is the number of seconds the test will be executed.\nThe more time we run the test, the more likely we’ll be able to spot\nlatency spikes. 100 seconds is usually appropriate, however you may want\nto perform a few runs at different times. Please note that the test is CPU\nintensive and will likely saturate a single core in your system.</p><p>Note: redis-cli in this special case needs to <strong>run in the server</strong> where you run or plan to run Redis, not in the client. In this special mode redis-cli does no connect to a Redis server at all: it will just try to measure the largest time the kernel does not provide CPU time to run to the redis-cli process itself.</p><p>In the above example, the intrinsic latency of the system is just 0.115\nmilliseconds (or 115 microseconds), which is a good news, however keep in mind\nthat the intrinsic latency may change over time depending on the load of the\nsystem.</p><p>Virtualized environments will not show so good numbers, especially with high\nload or if there are noisy neighbors. The following is a run on a Linode 4096\ninstance running Redis and Apache:</p><p>Here we have an intrinsic latency of 9.7 milliseconds: this means that we can’t ask better than that to Redis. However other runs at different times in different virtualization environments with higher load or with noisy neighbors can easily show even worse values. We were able to measured up to 40 milliseconds in\nsystems otherwise apparently running normally.</p>","link":"/alpha/topics/latency.html","spaLink":"#/alpha/topics/latency","title":"LATENCY BASELINE"},{"content":"<h2 id=\"redis-latency-problems-troubleshooting-latency-induced-by-network-and-communication\">Latency induced by network and communication</h2><p>Clients connect to Redis using a TCP/IP connection or a Unix domain connection.\nThe typical latency of a 1 Gbit/s network is about 200 us, while the latency\nwith a Unix domain socket can be as low as 30 us. It actually depends on your\nnetwork and system hardware. On top of the communication itself, the system\nadds some more latency (due to thread scheduling, CPU caches, NUMA placement,\netc …). System induced latencies are significantly higher on a virtualized\nenvironment than on a physical machine.</p><p>The consequence is even if Redis processes most commands in sub microsecond\nrange, a client performing many roundtrips to the server will have to pay\nfor these network and system related latencies.</p><p>An efficient client will therefore try to limit the number of roundtrips by\npipelining several commands together. This is fully supported by the servers\nand most clients. Aggregated commands like MSET/MGET can be also used for\nthat purpose. Starting with Redis 2.4, a number of commands also support\nvariadic parameters for all data types.</p><p>Here are some guidelines:</p><ul>\n<li>If you can afford it, prefer a physical machine over a VM to host the server.</li>\n<li>Do not systematically connect/disconnect to the server (especially true\nfor web based applications). Keep your connections as long lived as possible.</li>\n<li>If your client is on the same host than the server, use Unix domain sockets.</li>\n<li>Prefer to use aggregated commands (MSET/MGET), or commands with variadic\nparameters (if possible) over pipelining.</li>\n<li>Prefer to use pipelining (if possible) over sequence of roundtrips.</li>\n<li>Redis supports Lua server-side scripting to cover cases that are not suitable\nfor raw pipelining (for instance when the result of a command is an input for\nthe following commands).</li>\n</ul><p>On Linux, some people can achieve better latencies by playing with process\nplacement (taskset), cgroups, real-time priorities (chrt), NUMA\nconfiguration (numactl), or by using a low-latency kernel. Please note\nvanilla Redis is not really suitable to be bound on a <strong>single</strong> CPU core.\nRedis can fork background tasks that can be extremely CPU consuming\nlike bgsave or AOF rewrite. These tasks must <strong>never</strong> run on the same core\nas the main event loop.</p><p>In most situations, these kind of system level optimizations are not needed.\nOnly do them if you require them, and if you are familiar with them.</p>","link":"/alpha/topics/latency.html","spaLink":"#/alpha/topics/latency","title":"LATENCY INDUCED BY NETWORK AND COMMUNICATION"},{"content":"<h2 id=\"redis-latency-problems-troubleshooting-single-threaded-nature-of-redis\">Single threaded nature of Redis</h2><p>Redis uses a <em>mostly</em> single threaded design. This means that a single process\nserves all the client requests, using a technique called <strong>multiplexing</strong>.\nThis means that Redis can serve a single request in every given moment, so\nall the requests are served sequentially. This is very similar to how Node.js\nworks as well. However, both products are often not perceived as being slow.\nThis is caused in part by the small amount of time to complete a single request,\nbut primarily because these products are designed to not block on system calls,\nsuch as reading data from or writing data to a socket.</p><p>I said that Redis is <em>mostly</em> single threaded since actually from Redis 2.4\nwe use threads in Redis in order to perform some slow I/O operations in the\nbackground, mainly related to disk I/O, but this does not change the fact\nthat Redis serves all the requests using a single thread.</p>","link":"/alpha/topics/latency.html","spaLink":"#/alpha/topics/latency","title":"SINGLE THREADED NATURE OF REDIS"},{"content":"<h2 id=\"redis-latency-problems-troubleshooting-latency-generated-by-slow-commands\">Latency generated by slow commands</h2><p>A consequence of being single thread is that when a request is slow to serve\nall the other clients will wait for this request to be served. When executing\nnormal commands, like <code>GET</code> or <code>SET</code> or <code>LPUSH</code> this is not a problem\nat all since this commands are executed in constant (and very small) time.\nHowever there are commands operating on many elements, like <code>SORT</code>, <code>LREM</code>,\n<code>SUNION</code> and others. For instance taking the intersection of two big sets\ncan take a considerable amount of time.</p><p>The algorithmic complexity of all commands is documented. A good practice\nis to systematically check it when using commands you are not familiar with.</p><p>If you have latency concerns you should either not use slow commands against\nvalues composed of many elements, or you should run a replica using Redis\nreplication where to run all your slow queries.</p><p>It is possible to monitor slow commands using the Redis\n<a href=\"/commands/slowlog\">Slow Log feature</a>.</p><p>Additionally, you can use your favorite per-process monitoring program\n(top, htop, prstat, etc …) to quickly check the CPU consumption of the\nmain Redis process. If it is high while the traffic is not, it is usually\na sign that slow commands are used.</p><p><strong>IMPORTANT NOTE</strong>: a VERY common source of latency generated by the execution\nof slow commands is the use of the <code>KEYS</code> command in production environments.\n<code>KEYS</code>, as documented in the Redis documentation, should only be used for\ndebugging purposes. Since Redis 2.8 a new commands were introduced in order to\niterate the key space and other large collections incrementally, please check\nthe <code>SCAN</code>, <code>SSCAN</code>, <code>HSCAN</code> and <code>ZSCAN</code> commands for more information.</p>","link":"/alpha/topics/latency.html","spaLink":"#/alpha/topics/latency","title":"LATENCY GENERATED BY SLOW COMMANDS"},{"content":"<h2 id=\"redis-latency-problems-troubleshooting-latency-generated-by-fork\">Latency generated by fork</h2><p>In order to generate the RDB file in background, or to rewrite the Append Only File if AOF persistence is enabled, Redis has to fork background processes.\nThe fork operation (running in the main thread) can induce latency by itself.</p><p>Forking is an expensive operation on most Unix-like systems, since it involves\ncopying a good number of objects linked to the process. This is especially\ntrue for the page table associated to the virtual memory mechanism.</p><p>For instance on a Linux/AMD64 system, the memory is divided in 4 kB pages.\nTo convert virtual addresses to physical addresses, each process stores\na page table (actually represented as a tree) containing at least a pointer\nper page of the address space of the process. So a large 24 GB Redis instance\nrequires a page table of 24 GB / 4 kB * 8 = 48 MB.</p><p>When a background save is performed, this instance will have to be forked,\nwhich will involve allocating and copying 48 MB of memory. It takes time\nand CPU, especially on virtual machines where allocation and initialization\nof a large memory chunk can be expensive.</p>","link":"/alpha/topics/latency.html","spaLink":"#/alpha/topics/latency","title":"LATENCY GENERATED BY FORK"},{"content":"<h2 id=\"redis-latency-problems-troubleshooting-fork-time-in-different-systems\">Fork time in different systems</h2><p>Modern hardware is pretty fast to copy the page table, but Xen is not.\nThe problem with Xen is not virtualization-specific, but Xen-specific. For instance using VMware or Virtual Box does not result into slow fork time.\nThe following is a table that compares fork time for different Redis instance\nsize. Data is obtained performing a BGSAVE and looking at the <code>latest_fork_usec</code> filed in the <code>INFO</code> command output.</p><p>However the good news is that <strong>new types of EC2 HVM based instances are much\nbetter with fork times</strong>, almost on pair with physical servers, so for example\nusing m3.medium (or better) instances will provide good results.</p><ul>\n<li><strong>Linux beefy VM on VMware</strong> 6.0GB RSS forked in 77 milliseconds (12.8 milliseconds per GB).</li>\n<li><strong>Linux running on physical machine (Unknown HW)</strong> 6.1GB RSS forked in 80 milliseconds (13.1 milliseconds per GB)</li>\n<li><strong>Linux running on physical machine (Xeon @ 2.27Ghz)</strong> 6.9GB RSS forked into 62 milliseconds (9 milliseconds per GB).</li>\n<li><strong>Linux VM on 6sync (KVM)</strong> 360 MB RSS forked in 8.2 milliseconds (23.3 milliseconds per GB).</li>\n<li><strong>Linux VM on EC2, old instance types (Xen)</strong> 6.1GB RSS forked in 1460 milliseconds (239.3 milliseconds per GB).</li>\n<li><strong>Linux VM on EC2, new instance types (Xen)</strong> 1GB RSS forked in 10 milliseconds (10 milliseconds per GB).</li>\n<li><strong>Linux VM on Linode (Xen)</strong> 0.9GBRSS forked into 382 milliseconds (424 milliseconds per GB).</li>\n</ul><p>As you can see certain VM running on Xen have a performance hit that is between one order to two orders of magnitude. For EC2 users the suggestion is simple: use modern HVM based instances.</p>","link":"/alpha/topics/latency.html","spaLink":"#/alpha/topics/latency","title":"FORK TIME IN DIFFERENT SYSTEMS"},{"content":"<h2 id=\"redis-latency-problems-troubleshooting-latency-induced-by-transparent-huge-pages\">Latency induced by transparent huge pages</h2><p>Unfortunately when a Linux kernel has transparent huge pages enabled, Redis\nincurs to a big latency penalty after the <code>fork</code> call is used in order to\npersist on disk. Huge pages are the cause of the following issue:</p><p>Make sure to <strong>disable transparent huge pages</strong> using the following command:</p>","link":"/alpha/topics/latency.html","spaLink":"#/alpha/topics/latency","title":"LATENCY INDUCED BY TRANSPARENT HUGE PAGES"},{"content":"<h2 id=\"redis-latency-problems-troubleshooting-latency-induced-by-swapping-operating-system-paging\">Latency induced by swapping (operating system paging)</h2><p>Linux (and many other modern operating systems) is able to relocate memory\npages from the memory to the disk, and vice versa, in order to use the\nsystem memory efficiently.</p><p>If a Redis page is moved by the kernel from the memory to the swap file, when\nthe data stored in this memory page is used by Redis (for example accessing\na key stored into this memory page) the kernel will stop the Redis process\nin order to move the page back into the main memory. This is a slow operation\ninvolving random I/Os (compared to accessing a page that is already in memory)\nand will result into anomalous latency experienced by Redis clients.</p><p>The kernel relocates Redis memory pages on disk mainly because of three reasons:</p><ul>\n<li>The system is under memory pressure since the running processes are demanding\nmore physical memory than the amount that is available. The simplest instance of\nthis problem is simply Redis using more memory than the one available.</li>\n<li>The Redis instance data set, or part of the data set, is mostly completely idle\n(never accessed by clients), so the kernel could swap idle memory pages on disk.\nThis problem is very rare since even a moderately slow instance will touch all\nthe memory pages often, forcing the kernel to retain all the pages in memory.</li>\n<li>Some processes are generating massive read or write I/Os on the system. Because\nfiles are generally cached, it tends to put pressure on the kernel to increase\nthe filesystem cache, and therefore generate swapping activity. Please note it\nincludes Redis RDB and/or AOF background threads which can produce large files.</li>\n</ul><p>Fortunately Linux offers good tools to investigate the problem, so the simplest\nthing to do is when latency due to swapping is suspected is just to check if\nthis is the case.</p><p>The first thing to do is to checking the amount of Redis memory that is swapped\non disk. In order to do so you need to obtain the Redis instance pid:</p><p>Now enter the /proc file system directory for this process:</p><p>Here you’ll find a file called <strong>smaps</strong> that describes the memory layout of\nthe Redis process (assuming you are using Linux 2.6.16 or newer).\nThis file contains very detailed information about our process memory maps,\nand one field called <strong>Swap</strong> is exactly what we are looking for. However\nthere is not just a single swap field since the smaps file contains the\ndifferent memory maps of our Redis process (The memory layout of a process\nis more complex than a simple linear array of pages).</p><p>Since we are interested in all the memory swapped by our process the first thing\nto do is to grep for the Swap field across all the file:</p><p>If everything is 0 kB, or if there are sporadic 4k entries, everything is\nperfectly normal. Actually in our example instance (the one of a real web\nsite running Redis and serving hundreds of users every second) there are a\nfew entries that show more swapped pages. To investigate if this is a serious\nproblem or not we change our command in order to also print the size of the\nmemory map:</p><p>As you can see from the output, there is a map of 720896 kB\n(with just 12 kB swapped) and 156 kB more swapped in another map:\nbasically a very small amount of our memory is swapped so this is not\ngoing to create any problem at all.</p><p>If instead a non trivial amount of the process memory is swapped on disk your\nlatency problems are likely related to swapping. If this is the case with your\nRedis instance you can further verify it using the <strong>vmstat</strong> command:</p><p>The interesting part of the output for our needs are the two columns <strong>si</strong>\nand <strong>so</strong>, that counts the amount of memory swapped from/to the swap file. If\nyou see non zero counts in those two columns then there is swapping activity\nin your system.</p><p>Finally, the <strong>iostat</strong> command can be used to check the global I/O activity of\nthe system.</p><p>If your latency problem is due to Redis memory being swapped on disk you need\nto lower the memory pressure in your system, either adding more RAM if Redis\nis using more memory than the available, or avoiding running other memory\nhungry processes in the same system.</p>","link":"/alpha/topics/latency.html","spaLink":"#/alpha/topics/latency","title":"LATENCY INDUCED BY SWAPPING (OPERATING SYSTEM PAGING)"},{"content":"<h2 id=\"redis-latency-problems-troubleshooting-latency-due-to-aof-and-disk-io\">Latency due to AOF and disk I/O</h2><p>Another source of latency is due to the Append Only File support on Redis.\nThe AOF basically uses two system calls to accomplish its work. One is\nwrite(2) that is used in order to write data to the append only file, and\nthe other one is fdatasync(2) that is used in order to flush the kernel\nfile buffer on disk in order to ensure the durability level specified by\nthe user.</p><p>Both the write(2) and fdatasync(2) calls can be source of latency.\nFor instance write(2) can block both when there is a system wide sync\nin progress, or when the output buffers are full and the kernel requires\nto flush on disk in order to accept new writes.</p><p>The fdatasync(2) call is a worse source of latency as with many combinations\nof kernels and file systems used it can take from a few milliseconds to\na few seconds to complete, especially in the case of some other process\ndoing I/O. For this reason when possible Redis does the fdatasync(2) call\nin a different thread since Redis 2.4.</p><p>We’ll see how configuration can affect the amount and source of latency\nwhen using the AOF file.</p><p>The AOF can be configured to perform an fsync on disk in three different\nways using the <strong>appendfsync</strong> configuration option (this setting can be\nmodified at runtime using the <strong>CONFIG SET</strong> command).</p><ul>\n<li><p>When appendfsync is set to the value of <strong>no</strong> Redis performs no fsync.\nIn this configuration the only source of latency can be write(2).\nWhen this happens usually there is no solution since simply the disk can’t\ncope with the speed at which Redis is receiving data, however this is\nuncommon if the disk is not seriously slowed down by other processes doing\nI/O.</p>\n</li>\n<li><p>When appendfsync is set to the value of <strong>everysec</strong> Redis performs an\nfsync every second. It uses a different thread, and if the fsync is still\nin progress Redis uses a buffer to delay the write(2) call up to two seconds\n(since write would block on Linux if an fsync is in progress against the\nsame file). However if the fsync is taking too long Redis will eventually\nperform the write(2) call even if the fsync is still in progress, and this\ncan be a source of latency.</p>\n</li>\n<li><p>When appendfsync is set to the value of <strong>always</strong> an fsync is performed\nat every write operation before replying back to the client with an OK code\n(actually Redis will try to cluster many commands executed at the same time\ninto a single fsync). In this mode performances are very low in general and\nit is strongly recommended to use a fast disk and a file system implementation\nthat can perform the fsync in short time.</p>\n</li>\n</ul><p>When appendfsync is set to the value of <strong>no</strong> Redis performs no fsync.\nIn this configuration the only source of latency can be write(2).\nWhen this happens usually there is no solution since simply the disk can’t\ncope with the speed at which Redis is receiving data, however this is\nuncommon if the disk is not seriously slowed down by other processes doing\nI/O.</p><p>When appendfsync is set to the value of <strong>everysec</strong> Redis performs an\nfsync every second. It uses a different thread, and if the fsync is still\nin progress Redis uses a buffer to delay the write(2) call up to two seconds\n(since write would block on Linux if an fsync is in progress against the\nsame file). However if the fsync is taking too long Redis will eventually\nperform the write(2) call even if the fsync is still in progress, and this\ncan be a source of latency.</p><p>When appendfsync is set to the value of <strong>always</strong> an fsync is performed\nat every write operation before replying back to the client with an OK code\n(actually Redis will try to cluster many commands executed at the same time\ninto a single fsync). In this mode performances are very low in general and\nit is strongly recommended to use a fast disk and a file system implementation\nthat can perform the fsync in short time.</p><p>Most Redis users will use either the <strong>no</strong> or <strong>everysec</strong> setting for the\nappendfsync configuration directive. The suggestion for minimum latency is\nto avoid other processes doing I/O in the same system.\nUsing an SSD disk can help as well, but usually even non SSD disks perform\nwell with the append only file if the disk is spare as Redis writes\nto the append only file without performing any seek.</p><p>If you want to investigate your latency issues related to the append only\nfile you can use the strace command under Linux:</p><p>The above command will show all the fdatasync(2) system calls performed by\nRedis in the main thread. With the above command you’ll not see the\nfdatasync system calls performed by the background thread when the\nappendfsync config option is set to <strong>everysec</strong>. In order to do so\njust add the -f switch to strace.</p><p>If you wish you can also see both fdatasync and write system calls with the\nfollowing command:</p><p>However since write(2) is also used in order to write data to the client\nsockets this will likely show too many things unrelated to disk I/O.\nApparently there is no way to tell strace to just show slow system calls so\nI use the following command:</p>","link":"/alpha/topics/latency.html","spaLink":"#/alpha/topics/latency","title":"LATENCY DUE TO AOF AND DISK I/O"},{"content":"<h2 id=\"redis-latency-problems-troubleshooting-latency-generated-by-expires\">Latency generated by expires</h2><p>Redis evict expired keys in two ways:</p><ul>\n<li>One <em>lazy</em> way expires a key when it is requested by a command, but it is found to be already expired.</li>\n<li>One <em>active</em> way expires a few keys every 100 milliseconds.</li>\n</ul><p>The active expiring is designed to be adaptive. An expire cycle is started every 100 milliseconds (10 times per second), and will do the following:</p><ul>\n<li>Sample <code>ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP</code> keys, evicting all the keys already expired.</li>\n<li>If the more than 25% of the keys were found expired, repeat.</li>\n</ul><p>Given that <code>ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP</code> is set to 20 by default, and the process is performed ten times per second, usually just 200 keys per second are actively expired. This is enough to clean the DB fast enough even when already expired keys are not accessed for a long time, so that the <em>lazy</em> algorithm does not help. At the same time expiring just 200 keys per second has no effects in the latency a Redis instance.</p><p>However the algorithm is adaptive and will loop if it founds more than 25% of keys already expired in the set of sampled keys. But given that we run the algorithm ten times per second, this means that the unlucky event of more than 25% of the keys in our random sample are expiring at least <em>in the same second</em>.</p><p>Basically this means that <strong>if the database has many many keys expiring in the same second, and these make up at least 25% of the current population of keys with an expire set</strong>, Redis can block in order to get the percentage of keys already expired below 25%.</p><p>This approach is needed in order to avoid using too much memory for keys that area already expired, and usually is absolutely harmless since it’s strange that a big number of keys are going to expire in the same exact second, but it is not impossible that the user used <code>EXPIREAT</code> extensively with the same Unix time.</p><p>In short: be aware that many keys expiring at the same moment can be a source of latency.</p>","link":"/alpha/topics/latency.html","spaLink":"#/alpha/topics/latency","title":"LATENCY GENERATED BY EXPIRES"},{"content":"<h2 id=\"redis-latency-problems-troubleshooting-redis-software-watchdog\">Redis software watchdog</h2><p>Redis 2.6 introduces the <em>Redis Software Watchdog</em> that is a debugging tool\ndesigned to track those latency problems that for one reason or the other\nescaped an analysis using normal tools.</p><p>The software watchdog is an experimental feature. While it is designed to\nbe used in production environments care should be taken to backup the database\nbefore proceeding as it could possibly have unexpected interactions with the\nnormal execution of the Redis server.</p><p>It is important to use it only as <em>last resort</em> when there is no way to track the issue by other means.</p><p>This is how this feature works:</p><ul>\n<li>The user enables the software watchdog using the <code>CONFIG SET</code> command.</li>\n<li>Redis starts monitoring itself constantly.</li>\n<li>If Redis detects that the server is blocked into some operation that is not returning fast enough, and that may be the source of the latency issue, a low level report about where the server is blocked is dumped on the log file.</li>\n<li>The user contacts the developers writing a message in the Redis Google Group, including the watchdog report in the message.</li>\n</ul><p>Note that this feature can not be enabled using the redis.conf file, because it is designed to be enabled only in already running instances and only for debugging purposes.</p><p>To enable the feature just use the following:</p><p>The period is specified in milliseconds. In the above example I specified to log latency issues only if the server detects a delay of 500 milliseconds or greater. The minimum configurable period is 200 milliseconds.</p><p>When you are done with the software watchdog you can turn it off setting the <code>watchdog-period</code> parameter to 0. <strong>Important:</strong> remember to do this because keeping the instance with the watchdog turned on for a longer time than needed is generally not a good idea.</p><p>The following is an example of what you’ll see printed in the log file once the software watchdog detects a delay longer than the configured one:</p><p>Note: in the example the <strong>DEBUG SLEEP</strong> command was used in order to block the server. The stack trace is different if the server blocks in a different context.</p><p>If you happen to collect multiple watchdog stack traces you are encouraged to send everything to the Redis Google Group: the more traces we obtain, the simpler it will be to understand what the problem with your instance is.</p>","link":"/alpha/topics/latency.html","spaLink":"#/alpha/topics/latency","title":"REDIS SOFTWARE WATCHDOG"},{"content":"<h1 id=\"redis-lua-scripts-debugger\">Redis Lua scripts debugger</h1><p>Starting with version 3.2 Redis includes a complete Lua debugger, that can be\nused in order to make the task of writing complex Redis scripts much simpler.</p><p>Because Redis 3.2 is still in beta, please download the <code>unstable</code> branch of Redis from Github and compile it in order to test the debugger. You can use Redis unstable in order to debug your scripts that you’ll later run in a stable version of Redis, so the debugger is already usable in practical terms.</p><p>The Redis Lua debugger, codename LDB, has the following important features:</p><ul>\n<li>It uses a server-client model, so it’s a remote debugger. The Redis server acts as the debugging server, while the default client is <code>redis-cli</code>. However other clients can be developed by following the simple protocol implemented by the server.</li>\n<li>By default every new debugging session is a forked session. It means that while the Redis Lua script is being debugged, the server does not block and is usable for development or in order to execute multiple debugging sessions in parallel. This also means that changes are <strong>rolled back</strong> after the script debugging session finished, so that’s possible to restart a new debugging session again, using exactly the same Redis data set as the previous debugging session.</li>\n<li>An alternative synchronous (non forked) debugging model is available on demand, so that changes to the dataset can be retained. In this mode the server blocks for the time the debugging session is active.</li>\n<li>Support for step by step execution.</li>\n<li>Support for static and dynamic breakpoints.</li>\n<li>Support from logging the debugged script into the debugger console.</li>\n<li>Inspection of Lua variables.</li>\n<li>Tracing of Redis commands executed by the script.</li>\n<li>Pretty printing of Redis and Lua values.</li>\n<li>Infinite loops and long execution detection, which simulates a breakpoint.</li>\n</ul>","link":"/alpha/topics/ldb.html","spaLink":"#/alpha/topics/ldb","title":"REDIS LUA SCRIPTS DEBUGGER"},{"content":"<h2 id=\"redis-lua-scripts-debugger-quick-start\">Quick start</h2><p>A simple way to get started with the Lua debugger is to watch this video\nintroduction:</p><p><strong>Important note:</strong> please make sure to avoid debugging Lua scripts using your Redis production server. Use a development server instead. Also note that using the synchronous debugging mode (which is NOT the default) results into the Redis server blocking for all the time the debugging session lasts.</p><p>To start a new debugging session using <code>redis-cli</code> do the following steps:</p><p>Start a debugging session with:</p><p> ./redis-cli —ldb —eval /tmp/script.lua</p><p>Note that with the <code>--eval</code> option of <code>redis-cli</code> you can pass key names and arguments to the script, separated by a comma, like in the following example:</p><p>You’ll enter a special mode where <code>redis-cli</code> no longer accepts its normal\ncommands, but instead prints an help screen and passes the unmodified debugging\ncommands directly to Redis.</p><p>The only commands which are not passed to the Redis debugger are:</p><ul>\n<li><code>quit</code> — this will terminate the debugging session. It’s like removing all the breakpoints and using the <code>continue</code> debugging command. Moreover the command will exit from <code>redis-cli</code>.</li>\n<li><code>restart</code> — the debugging session will restart from scratch, <strong>reloading the new version of the script from the file</strong>. So a normal debugging cycle involves modifying the script after some debugging, and calling <code>restart</code> in order to start debugging again with the new script changes.</li>\n<li><code>help</code> — this command is passed to the Redis Lua debugger, that will print a list of commands like the following:</li>\n</ul><p>Note that when you start the debugger it will start in <strong>stepping mode</strong>. It will stop at the first line of the script that actually does something before executing it.</p><p>From this point you usually call <code>step</code> in order to execute the line and go to the next line. While you step Redis will show all the commands executed by the server like in the following example:</p><p>The <code>&lt;redis&gt;</code> and <code>&lt;reply&gt;</code> lines show the command executed by the line just\nexecuted, and the reply from the server. Note that this happens only in stepping mode. If you use <code>continue</code> in order to execute the script till the next breakpoint, commands will not be dumped on the screen to prevent too much output.</p>","link":"/alpha/topics/ldb.html","spaLink":"#/alpha/topics/ldb","title":"QUICK START"},{"content":"<h2 id=\"redis-lua-scripts-debugger-termination-of-the-debugging-session\">Termination of the debugging session</h2><p>When the scripts terminates naturally, the debugging session ends and\n<code>redis-cli</code> returns in its normal non-debugging mode. You can restart the\nsession using the <code>restart</code> command as usually.</p><p>Another way to stop a debugging session is just interrupting <code>redis-cli</code>\nmanually by pressing <code>Ctrl+C</code>. Note that also any event breaking the\nconnection between <code>redis-cli</code> and the <code>redis-server</code> will interrupt the\ndebugging session.</p><p>All the forked debugging sessions are terminated when the server is shut\ndown.</p>","link":"/alpha/topics/ldb.html","spaLink":"#/alpha/topics/ldb","title":"TERMINATION OF THE DEBUGGING SESSION"},{"content":"<h2 id=\"redis-lua-scripts-debugger-abbreviating-debugging-commands\">Abbreviating debugging commands</h2><p>Debugging can be a very repetitive task. For this reason every Redis\ndebugger command starts with a different character, and you can use the single\ninitial character in order to refer to the command.</p><p>So for example instead of typing <code>step</code> you can just type <code>s</code>.</p>","link":"/alpha/topics/ldb.html","spaLink":"#/alpha/topics/ldb","title":"ABBREVIATING DEBUGGING COMMANDS"},{"content":"<h2 id=\"redis-lua-scripts-debugger-breakpoints\">Breakpoints</h2><p>Adding and removing breakpoints is trivial as described in the online help.\nJust use <code>b 1 2 3 4</code> to add a breakpoint in line 1, 2, 3, 4.\nThe command <code>b 0</code> removes all the breakpoints. Selected breakpoints can be\nremoved using as argument the line where the breakpoint we want to remove is, but prefixed by a minus sign. So for example <code>b -3</code> removes the breakpoint from line 3.</p><p>Note that adding breakpoints to lines that Lua never executes, like declaration of local variables or comments, will not work. The breakpoint will be added but since this part of the script will never be executed, the program will never stop.</p>","link":"/alpha/topics/ldb.html","spaLink":"#/alpha/topics/ldb","title":"BREAKPOINTS"},{"content":"<h2 id=\"redis-lua-scripts-debugger-dynamic-breakpoints\">Dynamic breakpoints</h2><p>Using the <code>breakpoint</code> command it is possible to add breakpoints into specific\nlines. However sometimes we want to stop the execution of the program only\nwhen something special happens. In order to do so, you can use the\n<code>redis.breakpoint()</code> function inside your Lua script. When called it simulates\na breakpoint in the next line that will be executed.</p><p>This feature is extremely useful when debugging, so that we can avoid to\ncontinue the script execution manually multiple times until a given condition\nis encountered.</p>","link":"/alpha/topics/ldb.html","spaLink":"#/alpha/topics/ldb","title":"DYNAMIC BREAKPOINTS"},{"content":"<h2 id=\"redis-lua-scripts-debugger-synchronous-mode\">Synchronous mode</h2><p>As explained previously, but default LDB uses forked sessions with rollback\nof all the data changes operated by the script while it has being debugged.\nDeterminism is usually a good thing to have during debugging, so that successive\ndebugging sessions can be started without having to reset the database content\nto its original state.</p><p>However for tracking certain bugs, you may want to retain the changes performed\nto the key space by each debugging session. When this is a good idea you\nshould start the debugger using a special option, <code>ldb-sync-mode</code>, in <code>redis-cli</code>.</p><p><strong>Note that the Redis server will be unreachable during the debugging session in this mode</strong>, so use with care.</p><p>In this special mode, the <code>abort</code> command can stop the script half-way taking the changes operated to the dataset. Note that this is different compared to ending the debugging session normally. If you just interrupt <code>redis-cli</code> the script will be fully executed and then the session terminated. Instead with <code>abort</code> you can interrupt the script execution in the middle and start a new debugging session if needed.</p>","link":"/alpha/topics/ldb.html","spaLink":"#/alpha/topics/ldb","title":"SYNCHRONOUS MODE"},{"content":"<h2 id=\"redis-lua-scripts-debugger-logging-from-scripts\">Logging from scripts</h2><p>The <code>redis.debug()</code> command is a powerful debugging facility that can be\ncalled inside the Redis Lua script in order to log things into the debug\nconsole:</p><p>If the script is executed outside of a debugging session, <code>redis.debug()</code> has no effects at all. Note that the function accepts multiple arguments, that are separated by a comma and a space in the output.</p><p>Tables and nested tables are displayed correctly in order to make values simple to observe for the programmer debugging the script.</p>","link":"/alpha/topics/ldb.html","spaLink":"#/alpha/topics/ldb","title":"LOGGING FROM SCRIPTS"},{"content":"<h2 id=\"redis-lua-scripts-debugger-inspecting-the-program-state-with-print-and-eval\">Inspecting the program state with <code>print</code> and <code>eval</code></h2><p>While the <code>redis.debug()</code> function can be used in order to print values\ndirectly from within the Lua script, often it is useful to observe the local\nvariables of a program while stepping or when stopped into a breakpoint.</p><p>The <code>print</code> command does just that, and performs lookup in the call frames\nstarting from the current one back to the previous ones, up to top-level.\nThis means that even if we are into a nested function inside a Lua script,\nwe can still use <code>print foo</code> to look at the value of <code>foo</code> in the context\nof the calling function. When called without a variable name, <code>print</code> will\nprint all variables and their respective values.</p><p>The <code>eval</code> command executes small pieces of Lua scripts <strong>outside the context of the current call frame</strong> (evaluating inside the context of the current call frame is not possible with the current Lua internals). However you can use this command in order to test Lua functions.</p>","link":"/alpha/topics/ldb.html","spaLink":"#/alpha/topics/ldb","title":"INSPECTING THE PROGRAM STATE WITH PRINT AND EVAL"},{"content":"<h2 id=\"redis-lua-scripts-debugger-debugging-clients\">Debugging clients</h2><p>LDB uses the client-server model where the Redis servers acts as a debugging server that communicates using <a href=\"/topics/protocol\">RESP</a>. While <code>redis-cli</code> is the default debug client, any <a href=\"/clients\">client</a> can be used for debugging as long as it meets one of the following conditions:</p><p>For example, the <a href=\"https://redislabs.com/blog/zerobrane-studio-plugin-for-redis-lua-scripts\">Redis plugin</a> for <a href=\"http://studio.zerobrane.com/\">ZeroBrane Studio</a> integrates with LDB using <a href=\"https://github.com/nrk/redis-lua\">redis-lua</a>. The following Lua code is a simplified example of how the plugin achieves that:</p>","link":"/alpha/topics/ldb.html","spaLink":"#/alpha/topics/ldb","title":"DEBUGGING CLIENTS"},{"content":"<h1 id=\"redis-license-and-trademark-information\">Redis license and trademark information</h1><p>Redis is <strong>open source software</strong> released under the terms of the <strong>three clause BSD license</strong>. Most of the Redis source code was written and is copyrighted by Salvatore Sanfilippo and Pieter Noordhuis. A list of other contributors can be found in the git history.</p><p>The Redis trademark and logo are owned by Salvatore Sanfilippo and can be\nused in accordance with the <a href=\"/topics/trademark\">Redis Trademark Guidelines</a>.</p>","link":"/alpha/topics/license.html","spaLink":"#/alpha/topics/license","title":"REDIS LICENSE AND TRADEMARK INFORMATION"},{"content":"<h1 id=\"three-clause-bsd-license\">Three clause BSD license</h1><p>Every file in the Redis distribution, with the exceptions of third party files specified in the list below, contain the following license:</p><p>Redistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:</p><ul>\n<li><p>Redistributions of source code must retain the above copyright notice,\nthis list of conditions and the following disclaimer.</p>\n</li>\n<li><p>Redistributions in binary form must reproduce the above copyright\nnotice, this list of conditions and the following disclaimer in the\ndocumentation and/or other materials provided with the distribution.</p>\n</li>\n<li><p>Neither the name of Redis nor the names of its contributors may be used\nto endorse or promote products derived from this software without\nspecific prior written permission.</p>\n</li>\n</ul><p>Redistributions of source code must retain the above copyright notice,\nthis list of conditions and the following disclaimer.</p><p>Redistributions in binary form must reproduce the above copyright\nnotice, this list of conditions and the following disclaimer in the\ndocumentation and/or other materials provided with the distribution.</p><p>Neither the name of Redis nor the names of its contributors may be used\nto endorse or promote products derived from this software without\nspecific prior written permission.</p><p>THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS”\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\nARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\nLIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\nCONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\nSUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\nINTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\nCONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\nARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\nPOSSIBILITY OF SUCH DAMAGE.</p>","link":"/alpha/topics/license.html","spaLink":"#/alpha/topics/license","title":"THREE CLAUSE BSD LICENSE"},{"content":"<h1 id=\"third-party-files-and-licenses\">Third party files and licenses</h1><p>Redis uses source code from third parties. All this code contains a BSD or BSD-compatible license. The following is a list of third party files and information about their copyright.</p><ul>\n<li><p>Redis uses the <a href=\"http://oldhome.schmorp.de/marc/liblzf.html\">LHF compression library</a>. LibLZF is copyright Marc Alexander Lehmann and is released under the terms of the <strong>two clause BSD license</strong>.</p>\n</li>\n<li><p>Redis uses the <code>sha1.c</code> file that is copyright by Steve Reid and released under the <strong>public domain</strong>. This file is extremely popular and used among open source and proprietary code.</p>\n</li>\n<li><p>When compiled on Linux Redis uses the <a href=\"http://www.canonware.com/jemalloc/\">Jemalloc allocator</a>, that is copyright by Jason Evans, Mozilla Foundation and Facebook, Inc and is released under the <strong>two clause BSD license</strong>.</p>\n</li>\n<li><p>Inside Jemalloc the file <code>pprof</code> is copyright Google Inc and released under the <strong>three clause BSD license</strong>.</p>\n</li>\n<li><p>Inside Jemalloc the files <code>inttypes.h</code>, <code>stdbool.h</code>, <code>stdint.h</code>, <code>strings.h</code> under the <code>msvc_compat</code> directory are copyright Alexander Chemeris and released under the <strong>three clause BSD license</strong>.</p>\n</li>\n<li><p>The libraries <strong>hiredis</strong> and <strong>linenoise</strong> also included inside the Redis distribution are copyright Salvatore Sanfilippo and Pieter Noordhuis and released under the terms respectively of the <strong>three clause BSD license</strong> and <strong>two clause BSD license</strong>.</p>\n</li>\n</ul><p>Redis uses the <a href=\"http://oldhome.schmorp.de/marc/liblzf.html\">LHF compression library</a>. LibLZF is copyright Marc Alexander Lehmann and is released under the terms of the <strong>two clause BSD license</strong>.</p><p>Redis uses the <code>sha1.c</code> file that is copyright by Steve Reid and released under the <strong>public domain</strong>. This file is extremely popular and used among open source and proprietary code.</p><p>When compiled on Linux Redis uses the <a href=\"http://www.canonware.com/jemalloc/\">Jemalloc allocator</a>, that is copyright by Jason Evans, Mozilla Foundation and Facebook, Inc and is released under the <strong>two clause BSD license</strong>.</p><p>Inside Jemalloc the file <code>pprof</code> is copyright Google Inc and released under the <strong>three clause BSD license</strong>.</p><p>Inside Jemalloc the files <code>inttypes.h</code>, <code>stdbool.h</code>, <code>stdint.h</code>, <code>strings.h</code> under the <code>msvc_compat</code> directory are copyright Alexander Chemeris and released under the <strong>three clause BSD license</strong>.</p><p>The libraries <strong>hiredis</strong> and <strong>linenoise</strong> also included inside the Redis distribution are copyright Salvatore Sanfilippo and Pieter Noordhuis and released under the terms respectively of the <strong>three clause BSD license</strong> and <strong>two clause BSD license</strong>.</p>","link":"/alpha/topics/license.html","spaLink":"#/alpha/topics/license","title":"THIRD PARTY FILES AND LICENSES"},{"content":"<h1 id=\"using-redis-as-an-lru-cache\">Using Redis as an LRU cache</h1><p>When Redis is used as a cache, sometimes it is handy to let it automatically\nevict old data as you add new one. This behavior is very well known in the\ncommunity of developers, since it is the default behavior of the popular\n<em>memcached</em> system.</p><p>LRU is actually only one of the supported eviction methods. This page covers\nthe more general topic of the Redis <code>maxmemory</code> directive that is used in\norder to limit the memory usage to a fixed amount, and it also covers in\ndepth the LRU algorithm used by Redis, that is actually an approximation of\nthe exact LRU.</p>","link":"/alpha/topics/lru-cache.html","spaLink":"#/alpha/topics/lru-cache","title":"USING REDIS AS AN LRU CACHE"},{"content":"<h2 id=\"using-redis-as-an-lru-cache-maxmemory-configuration-directive\">Maxmemory configuration directive</h2><p>The <code>maxmemory</code> configuration directive is used in order to configure Redis\nto use a specified amount of memory for the data set. It is possible to\nset the configuration directive using the <code>redis.conf</code> file, or later using\nthe <code>CONFIG SET</code> command at runtime.</p><p>For example in order to configure a memory limit of 100 megabytes, the\nfollowing directive can be used inside the <code>redis.conf</code> file.</p><p>Setting <code>maxmemory</code> to zero results into no memory limits. This is the\ndefault behavior for 64 bit systems, while 32 bit systems use an implicit\nmemory limit of 3GB.</p><p>When the specified amount of memory is reached, it is possible to select\namong different behaviors, called <strong>policies</strong>.\nRedis can just return errors for commands that could result in more memory\nbeing used, or it can evict some old data in order to return back to the\nspecified limit every time new data is added.</p>","link":"/alpha/topics/lru-cache.html","spaLink":"#/alpha/topics/lru-cache","title":"MAXMEMORY CONFIGURATION DIRECTIVE"},{"content":"<h2 id=\"using-redis-as-an-lru-cache-eviction-policies\">Eviction policies</h2><p>The exact behavior Redis follows when the <code>maxmemory</code> limit is reached is\nconfigured using the <code>maxmemory-policy</code> configuration directive.</p><p>The following policies are available:</p><ul>\n<li><strong>noeviction</strong>: return errors when the memory limit was reached and the client is trying to execute commands that could result in more memory to be used (most write commands, but <code>DEL</code> and a few more exceptions).</li>\n<li><strong>allkeys-lru</strong>: evict keys trying to remove the less recently used (LRU) keys first, in order to make space for the new data added.</li>\n<li><strong>volatile-lru</strong>: evict keys trying to remove the less recently used (LRU) keys first, but only among keys that have an <strong>expire set</strong>, in order to make space for the new data added.</li>\n<li><strong>allkeys-random</strong>: evict random keys in order to make space for the new data added.</li>\n<li><strong>volatile-random</strong>: evict random keys in order to make space for the new data added, but only evict keys with an <strong>expire set</strong>.</li>\n<li><strong>volatile-ttl</strong>: In order to make space for the new data, evict only keys with an <strong>expire set</strong>, and try to evict keys with a shorter time to live (TTL) first.</li>\n</ul><p>The policies <strong>volatile-lru</strong>, <strong>volatile-random</strong> and <strong>volatile-ttl</strong> behave like <strong>noeviction</strong> if there are no keys to evict matching the prerequisites.</p><p>To pick the right eviction policy is important depending on the access pattern \nof your application, however you can reconfigure the policy at runtime while \nthe application is running, and monitor the number of cache misses and hits \nusing the Redis <code>INFO</code> output in order to tune your setup.</p><p>In general as a rule of thumb:</p><ul>\n<li>Use the <strong>allkeys-lru</strong> policy when you expect a power-law distribution in the popularity of your requests, that is, you expect that a subset of elements will be accessed far more often than the rest. <strong>This is a good pick if you are unsure</strong>.</li>\n<li>Use the <strong>allkeys-random</strong> if you have a cyclic access where all the keys are scanned continuously, or when you expect the distribution to be uniform (all elements likely accessed with the same probability).</li>\n<li>Use the <strong>volatile-ttl</strong> if you want to be able to provide hints to Redis about what are good candidate for expiration by using different TTL values when you create your cache objects.</li>\n</ul><p>The <strong>allkeys-lru</strong> and <strong>volatile-random</strong> policies are mainly useful when you want to use a single instance for both caching and to have a set of persistent keys. However it is usually a better idea to run two Redis instances to solve such a problem.</p><p>It is also worth to note that setting an expire to a key costs memory, so using a policy like <strong>allkeys-lru</strong> is more memory efficient since there is no need to set an expire for the key to be evicted under memory pressure.</p>","link":"/alpha/topics/lru-cache.html","spaLink":"#/alpha/topics/lru-cache","title":"EVICTION POLICIES"},{"content":"<h2 id=\"using-redis-as-an-lru-cache-how-the-eviction-process-works\">How the eviction process works</h2><p>It is important to understand that the eviction process works like this:</p><ul>\n<li>A client runs a new command, resulting in more data added.</li>\n<li>Redis checks the memory usage, and if it is greater than the <code>maxmemory</code> limit , it evicts keys according to the policy.</li>\n<li>A new command is executed, and so forth.</li>\n</ul><p>So we continuously cross the boundaries of the memory limit, by going over it, and then by evicting keys to return back under the limits.</p><p>If a command results in a lot of memory being used (like a big set intersection stored into a new key) for some time the memory limit can be surpassed by a noticeable amount.</p>","link":"/alpha/topics/lru-cache.html","spaLink":"#/alpha/topics/lru-cache","title":"HOW THE EVICTION PROCESS WORKS"},{"content":"<h2 id=\"using-redis-as-an-lru-cache-approximated-lru-algorithm\">Approximated LRU algorithm</h2><p>Redis LRU algorithm is not an exact implementation. This means that Redis is\nnot able to pick the <em>best candidate</em> for eviction, that is, the access that\nwas accessed the most in the past. Instead it will try to run an approximation\nof the LRU algorithm, by sampling a small number of keys, and evicting the\none that is the best (with the oldest access time) among the sampled keys.</p><p>However since Redis 3.0 the algorithm was improved to also take a pool of good\ncandidates for eviction. This improved the performance of the algorithm, making\nit able to approximate more closely the behavior of a real LRU algorithm.</p><p>What is important about the Redis LRU algorithm is that you <strong>are able to tune</strong> the precision of the algorithm by changing the number of samples to check for every eviction. This parameter is controlled by the following configuration directive:</p><p>The reason why Redis does not use a true LRU implementation is because it\ncosts more memory. However the approximation is virtually equivalent for the\napplication using Redis. The following is a graphical comparison of how\nthe LRU approximation used by Redis compares with true LRU.</p><p><img src=\"http://redis.io/images/redisdoc/lru_comparison.png\" alt=\"LRU comparison\"></p><p>The test to generate the above graphs filled a Redis server with a given number of keys. The keys were accessed from the first to the last, so that the first keys are the best candidates for eviction using an LRU algorithm. Later more 50% of keys are added, in order to force half of the old keys to be evicted.</p><p>You can see three kind of dots in the graphs, forming three distinct bands.</p><ul>\n<li>The light gray band are objects that were evicted.</li>\n<li>The gray band are objects that were not evicted.</li>\n<li>The green band are objects that were added.</li>\n</ul><p>In a theoretical LRU implementation we expect that, among the old keys, the first half will be expired. The Redis LRU algorithm will instead only <em>probabilistically</em> expire the older keys.</p><p>As you can see Redis 3.0 does a better job with 5 samples compared to Redis 2.8, however most objects that are among the latest accessed are still retained by Redis 2.8. Using a sample size of 10 in Redis 3.0 the approximation is very close to the theoretical performance of Redis 3.0.</p><p>Note that LRU is just a model to predict how likely a given key will be accessed in the future. Moreover, if your data access pattern closely\nresembles the power law, most of the accesses will be in the set of keys that\nthe LRU approximated algorithm will be able to handle well.</p><p>In simulations we found that using a power law access pattern, the difference between true LRU and Redis approximation were minimal or non-existent.</p><p>However you can raise the sample size to 10 at the cost of some additional CPU\nusage in order to closely approximate true LRU, and check if this makes a\ndifference in your cache misses rate.</p><p>To experiment in production with different values for the sample size by using\nthe <code>CONFIG SET maxmemory-samples &lt;count&gt;</code> command, is very simple.</p>","link":"/alpha/topics/lru-cache.html","spaLink":"#/alpha/topics/lru-cache","title":"APPROXIMATED LRU ALGORITHM"},{"content":"<h1 id=\"redis-mass-insertion\">Redis Mass Insertion</h1><p>Sometimes Redis instances needs to be loaded with big amount of preexisting\nor user generated data in a short amount of time, so that millions of keys\nwill be created as fast as possible.</p><p>This is called a <em>mass insertion</em>, and the goal of this document is to\nprovide information about how to feed Redis with data as fast as possible.</p>","link":"/alpha/topics/mass-insert.html","spaLink":"#/alpha/topics/mass-insert","title":"REDIS MASS INSERTION"},{"content":"<h2 id=\"redis-mass-insertion-use-the-protocol-luke\">Use the protocol, Luke</h2><p>Using a normal Redis client to perform mass insertion is not a good idea\nfor a few reasons: the naive approach of sending one command after the other\nis slow because you have to pay for the round trip time for every command.\nIt is possible to use pipelining, but for mass insertion of many records\nyou need to write new commands while you read replies at the same time to\nmake sure you are inserting as fast as possible.</p><p>Only a small percentage of clients support non-blocking I/O, and not all the\nclients are able to parse the replies in an efficient way in order to maximize\nthroughput. For all this reasons the preferred way to mass import data into\nRedis is to generate a text file containing the Redis protocol, in raw format,\nin order to call the commands needed to insert the required data.</p><p>For instance if I need to generate a large data set where there are billions\nof keys in the form: `keyN -&gt; ValueN’ I will create a file containing the\nfollowing commands in the Redis protocol format:</p><p>Once this file is created, the remaining action is to feed it to Redis\nas fast as possible. In the past the way to do this was to use the\n<code>netcat</code> with the following command:</p><p>However this is not a very reliable way to perform mass import because netcat\ndoes not really know when all the data was transferred and can’t check for\nerrors. In 2.6 or later versions of Redis the <code>redis-cli</code> utility\nsupports a new mode called <strong>pipe mode</strong> that was designed in order to perform\nmass insertion.</p><p>Using the pipe mode the command to run looks like the following:</p><p>That will produce an output similar to this:</p><p>The redis-cli utility will also make sure to only redirect errors received\nfrom the Redis instance to the standard output.</p>","link":"/alpha/topics/mass-insert.html","spaLink":"#/alpha/topics/mass-insert","title":"USE THE PROTOCOL, LUKE"},{"content":"<h2 id=\"redis-mass-insertion-generating-redis-protocol\">Generating Redis Protocol</h2><p>The Redis protocol is extremely simple to generate and parse, and is\n<a href=\"/topics/protocol\">Documented here</a>. However in order to generate protocol for\nthe goal of mass insertion you don’t need to understand every detail of the\nprotocol, but just that every command is represented in the following way:</p><p>Where <code>&lt;cr&gt;</code> means “\\r” (or ASCII character 13) and <code>&lt;lf&gt;</code> means “\\n” (or ASCII character 10).</p><p>For instance the command <strong>SET key value</strong> is represented by the following protocol:</p><p>Or represented as a quoted string:</p><p>The file you need to generate for mass insertion is just composed of commands\nrepresented in the above way, one after the other.</p><p>The following Ruby function generates valid protocol:</p><p>Using the above function it is possible to easily generate the key value pairs\nin the above example, with this program:</p><p>We can run the program directly in pipe to redis-cli in order to perform our\nfirst mass import session.</p>","link":"/alpha/topics/mass-insert.html","spaLink":"#/alpha/topics/mass-insert","title":"GENERATING REDIS PROTOCOL"},{"content":"<h2 id=\"redis-mass-insertion-how-the-pipe-mode-works-under-the-hoods\">How the pipe mode works under the hoods</h2><p>The magic needed inside the pipe mode of redis-cli is to be as fast as netcat\nand still be able to understand when the last reply was sent by the server\nat the same time.</p><p>This is obtained in the following way:</p><ul>\n<li>redis-cli —pipe tries to send data as fast as possible to the server.</li>\n<li>At the same time it reads data when available, trying to parse it.</li>\n<li>Once there is no more data to read from stdin, it sends a special <strong>ECHO</strong> command with a random 20 bytes string: we are sure this is the latest command sent, and we are sure we can match the reply checking if we receive the same 20 bytes as a bulk reply.</li>\n<li>Once this special final command is sent, the code receiving replies starts to match replies with this 20 bytes. When the matching reply is reached it can exit with success.</li>\n</ul><p>Using this trick we don’t need to parse the protocol we send to the server in order to understand how many commands we are sending, but just the replies.</p><p>However while parsing the replies we take a counter of all the replies parsed so that at the end we are able to tell the user the amount of commands transferred to the server by the mass insert session.</p>","link":"/alpha/topics/mass-insert.html","spaLink":"#/alpha/topics/mass-insert","title":"HOW THE PIPE MODE WORKS UNDER THE HOODS"},{"content":"<h2 id=\"-special-encoding-of-small-aggregate-data-types\">Special encoding of small aggregate data types</h2><p>Since Redis 2.2 many data types are optimized to use less space up to a certain size. Hashes, Lists, Sets composed of just integers, and Sorted Sets, when smaller than a given number of elements, and up to a maximum element size, are encoded in a very memory efficient way that uses <em>up to 10 times less memory</em> (with 5 time less memory used being the average saving).</p><p>This is completely transparent from the point of view of the user and API.\nSince this is a CPU / memory trade off it is possible to tune the maximum number of elements and maximum element size for special encoded types using the following redis.conf directives.</p><p>If a specially encoded value will overflow the configured max size, Redis will automatically convert it into normal encoding. This operation is very fast for small values, but if you change the setting in order to use specially encoded values for much larger aggregate types the suggestion is to run some benchmark and test to check the conversion time.</p>","link":"/alpha/topics/memory-optimization.html","spaLink":"#/alpha/topics/memory-optimization","title":"SPECIAL ENCODING OF SMALL AGGREGATE DATA TYPES"},{"content":"<h2 id=\"-using-32-bit-instances\">Using 32 bit instances</h2><p>Redis compiled with 32 bit target uses a lot less memory per key, since pointers are small, but such an instance will be limited to 4 GB of maximum memory usage. To compile Redis as 32 bit binary use <em>make 32bit</em>. RDB and AOF files are compatible between 32 bit and 64 bit instances (and between little and big endian of course) so you can switch from 32 to 64 bit, or the contrary, without problems.</p>","link":"/alpha/topics/memory-optimization.html","spaLink":"#/alpha/topics/memory-optimization","title":"USING 32 BIT INSTANCES"},{"content":"<h2 id=\"-bit-and-byte-level-operations\">Bit and byte level operations</h2><p>Redis 2.2 introduced new bit and byte level operations: <code>GETRANGE</code>, <code>SETRANGE</code>, <code>GETBIT</code> and <code>SETBIT</code>. Using these commands you can treat the Redis string type as a random access array. For instance if you have an application where users are identified by a unique progressive integer number, you can use a bitmap in order to save information about the sex of users, setting the bit for females and clearing it for males, or the other way around. With 100 million users this data will take just 12 megabytes of RAM in a Redis instance. You can do the same using <code>GETRANGE</code> and <code>SETRANGE</code> in order to store one byte of information for each user. This is just an example but it is actually possible to model a number of problems in very little space with these new primitives.</p>","link":"/alpha/topics/memory-optimization.html","spaLink":"#/alpha/topics/memory-optimization","title":"BIT AND BYTE LEVEL OPERATIONS"},{"content":"<h2 id=\"-use-hashes-when-possible\">Use hashes when possible</h2><p>Small hashes are encoded in a very small space, so you should try representing your data using hashes every time it is possible. For instance if you have objects representing users in a web application, instead of using different keys for name, surname, email, password, use a single hash with all the required fields.</p><p>If you want to know more about this, read the next section.</p>","link":"/alpha/topics/memory-optimization.html","spaLink":"#/alpha/topics/memory-optimization","title":"USE HASHES WHEN POSSIBLE"},{"content":"<h2 id=\"-using-hashes-to-abstract-a-very-memory-efficient-plain-key-value-store-on-top-of-redis\">Using hashes to abstract a very memory efficient plain key-value store on top of Redis</h2><p>I understand the title of this section is a bit scaring, but I’m going to explain in details what this is about.</p><p>Basically it is possible to model a plain key-value store using Redis\nwhere values can just be just strings, that is not just more memory efficient\nthan Redis plain keys but also much more memory efficient than memcached.</p><p>Let’s start with some fact: a few keys use a lot more memory than a single key\ncontaining a hash with a few fields. How is this possible? We use a trick.\nIn theory in order to guarantee that we perform lookups in constant time\n(also known as O(1) in big O notation) there is the need to use a data structure\nwith a constant time complexity in the average case, like a hash table.</p><p>But many times hashes contain just a few fields. When hashes are small we can\ninstead just encode them in an O(N) data structure, like a linear\narray with length-prefixed key value pairs. Since we do this only when N\nis small, the amortized time for HGET and HSET commands is still O(1): the\nhash will be converted into a real hash table as soon as the number of elements\nit contains will grow too much (you can configure the limit in redis.conf).</p><p>This does not work well just from the point of view of time complexity, but\nalso from the point of view of constant times, since a linear array of key\nvalue pairs happens to play very well with the CPU cache (it has a better\ncache locality than a hash table).</p><p>However since hash fields and values are not (always) represented as full\nfeatured Redis objects, hash fields can’t have an associated time to live\n(expire) like a real key, and can only contain a string. But we are okay with\nthis, this was anyway the intention when the hash data type API was\ndesigned (we trust simplicity more than features, so nested data structures\nare not allowed, as expires of single fields are not allowed).</p><p>So hashes are memory efficient. This is very useful when using hashes\nto represent objects or to model other problems when there are group of\nrelated fields. But what about if we have a plain key value business?</p><p>Imagine we want to use Redis as a cache for many small objects, that can be\nJSON encoded objects, small HTML fragments, simple key -&gt; boolean values\nand so forth. Basically anything is a string -&gt; string map with small keys\nand values.</p><p>Now let’s assume the objects we want to cache are numbered, like:</p><ul>\n<li>object:102393</li>\n<li>object:1234</li>\n<li>object:5</li>\n</ul><p>This is what we can do. Every time there is to perform a\nSET operation to set a new value, we actually split the key into two parts,\none used as a key, and used as field name for the hash. For instance the\nobject named “object:1234” is actually split into:</p><ul>\n<li>a Key named object:12</li>\n<li>a Field named 34</li>\n</ul><p>So we use all the characters but the latest two for the key, and the final\ntwo characters for the hash field name. To set our key we use the following\ncommand:</p><p>As you can see every hash will end containing 100 fields, that\nis an optimal compromise between CPU and memory saved.</p><p>There is another very important thing to note, with this schema\nevery hash will have more or \nless 100 fields regardless of the number of objects we cached. This is since\nour objects will always end with a number, and not a random string. In some\nway the final number can be considered as a form of implicit pre-sharding.</p><p>What about small numbers? Like object:2? We handle this case using just\n“object:” as a key name, and the whole number as the hash field name.\nSo object:2 and object:10 will both end inside the key “object:”, but one\nas field name “2” and one as “10”.</p><p>How much memory we save this way?</p><p>I used the following Ruby program to test how this works:</p><p>This is the result against a 64 bit instance of Redis 2.2:</p><ul>\n<li>UseOptimization set to true: 1.7 MB of used memory</li>\n<li>UseOptimization set to false; 11 MB of used memory</li>\n</ul><p>This is an order of magnitude, I think this makes Redis more or less the most\nmemory efficient plain key value store out there.</p><p><em>WARNING</em>: for this to work, make sure that in your redis.conf you have\nsomething like this:</p><p>Also remember to set the following field accordingly to the maximum size\nof your keys and values:</p><p>Every time a hash will exceed the number of elements or element size specified\nit will be converted into a real hash table, and the memory saving will be lost.</p><p>You may ask, why don’t you do this implicitly in the normal key space so that\nI don’t have to care? There are two reasons: one is that we tend to make\ntrade offs explicit, and this is a clear tradeoff between many things: CPU,\nmemory, max element size. The second is that the top level key space must\nsupport a lot of interesting things like expires, LRU data, and so\nforth so it is not practical to do this in a general way.</p><p>But the Redis Way is that the user must understand how things work so that\nhe is able to pick the best compromise, and to understand how the system will\nbehave exactly.</p>","link":"/alpha/topics/memory-optimization.html","spaLink":"#/alpha/topics/memory-optimization","title":"USING HASHES TO ABSTRACT A VERY MEMORY EFFICIENT PLAIN KEY-VALUE STORE ON TOP OF REDIS"},{"content":"<h2 id=\"-memory-allocation\">Memory allocation</h2><p>To store user keys, Redis allocates at most as much memory as the <code>maxmemory</code>\nsetting enables (however there are small extra allocations possible).</p><p>The exact value can be set in the configuration file or set later via\n<code>CONFIG SET</code> (see <a href=\"http://redis.io/topics/lru-cache\">Using memory as an LRU cache for more info</a>). There are a few things that should be noted about how\nRedis manages memory:</p><ul>\n<li>Redis will not always free up (return) memory to the OS when keys are removed.\nThis is not something special about Redis, but it is how most malloc() implementations work. For example if you fill an instance with 5GB worth of data, and then\nremove the equivalent of 2GB of data, the Resident Set Size (also known as\nthe RSS, which is the number of memory pages consumed by the process)\nwill probably still be around 5GB, even if Redis will claim that the user\nmemory is around 3GB.  This happens because the underlying allocator can’t easily release the memory. For example often most of the removed keys were allocated in the same pages as the other keys that still exist.</li>\n<li>The previous point means that you need to provision memory based on your\n<strong>peak memory usage</strong>. If your workload from time to time requires 10GB, even if\nmost of the times 5GB could do, you need to provision for 10GB.</li>\n<li>However allocators are smart and are able to reuse free chunks of memory,\nso after you freed 2GB of your 5GB data set, when you start adding more keys\nagain, you’ll see the RSS (Resident Set Size) to stay steady and don’t grow\nmore, as you add up to 2GB of additional keys. The allocator is basically\ntrying to reuse the 2GB of memory previously (logically) freed.</li>\n<li>Because of all this, the fragmentation ratio is not reliable when you\nhad a memory usage that at peak is much larger than the currently used memory.\nThe fragmentation is calculated as the amount of memory currently in use\n(as the sum of all the allocations performed by Redis) divided by the physical\nmemory actually used (the RSS value). Because the RSS reflects the peak memory,\nwhen the (virtually) used memory is low since a lot of keys / values were\nfreed, but the RSS is high, the ratio <code>mem_used / RSS</code> will be very high.</li>\n</ul><p>If <code>maxmemory</code> is not set Redis will keep allocating memory as it finds\nfit and thus it can (gradually) eat up all your free memory.\nTherefore it is generally advisable to configure some limit. You may also\nwant to set <code>maxmemory-policy</code> to <code>noeviction</code> (which is <em>not</em> the default\nvalue in some older versions of Redis).</p><p>It makes Redis return an out of memory error for write commands if and when it reaches the limit - which in turn may result in errors in the application but will not render the whole machine dead because of memory starvation.</p>","link":"/alpha/topics/memory-optimization.html","spaLink":"#/alpha/topics/memory-optimization","title":"MEMORY ALLOCATION"},{"content":"<h2 id=\"-work-in-progress\">Work in progress</h2><p>Work in progress… more tips will be added soon.</p>","link":"/alpha/topics/memory-optimization.html","spaLink":"#/alpha/topics/memory-optimization","title":"WORK IN PROGRESS"},{"content":"<h1 id=\"redis-keyspace-notifications\">Redis Keyspace Notifications</h1><p><strong>IMPORTANT</strong> Keyspace notifications is a feature available since 2.8.0</p>","link":"/alpha/topics/notifications.html","spaLink":"#/alpha/topics/notifications","title":"REDIS KEYSPACE NOTIFICATIONS"},{"content":"<h2 id=\"redis-keyspace-notifications-feature-overview\">Feature overview</h2><p>Keyspace notifications allows clients to subscribe to Pub/Sub channels in order\nto receive events affecting the Redis data set in some way.</p><p>Examples of the events that is possible to receive are the following:</p><ul>\n<li>All the commands affecting a given key.</li>\n<li>All the keys receiving an LPUSH operation.</li>\n<li>All the keys expiring in the database 0.</li>\n</ul><p>Events are delivered using the normal Pub/Sub layer of Redis, so clients\nimplementing Pub/Sub are able to use this feature without modifications.</p><p>Because Redis Pub/Sub is <em>fire and forget</em> currently there is no way to use this\nfeature if you application demands <strong>reliable notification</strong> of events, that is,\nif your Pub/Sub client disconnects, and reconnects later, all the events\ndelivered during the time the client was disconnected are lost.</p><p>In the future there are plans to allow for more reliable delivering of\nevents, but probably this will be addressed at a more general level either\nbringing reliability to Pub/Sub itself, or allowing Lua scripts to intercept\nPub/Sub messages to perform operations like pushing the events into a list.</p>","link":"/alpha/topics/notifications.html","spaLink":"#/alpha/topics/notifications","title":"FEATURE OVERVIEW"},{"content":"<h2 id=\"redis-keyspace-notifications-type-of-events\">Type of events</h2><p>Keyspace notifications are implemented sending two distinct type of events\nfor every operation affecting the Redis data space. For instance a <code>DEL</code>\noperation targeting the key named <code>mykey</code> in database <code>0</code> will trigger\nthe delivering of two messages, exactly equivalent to the following two\n<code>PUBLISH</code> commands:</p><p>It is easy to see how one channel allows to listen to all the events targeting\nthe key <code>mykey</code> and the other channel allows to obtain information about\nall the keys that are target of a <code>del</code> operation.</p><p>The first kind of event, with <code>keyspace</code> prefix in the channel is called\na <strong>Key-space notification</strong>, while the second, with the <code>keyevent</code> prefix,\nis called a <strong>Key-event notification</strong>.</p><p>In the above example a <code>del</code> event was generated for the key <code>mykey</code>.\nWhat happens is that:</p><ul>\n<li>The Key-space channel receives as message the name of the event.</li>\n<li>The Key-event channel receives as message the name of the key.</li>\n</ul><p>It is possible to enable only one kind of notification in order to deliver\njust the subset of events we are interested in.</p>","link":"/alpha/topics/notifications.html","spaLink":"#/alpha/topics/notifications","title":"TYPE OF EVENTS"},{"content":"<h2 id=\"redis-keyspace-notifications-configuration\">Configuration</h2><p>By default keyspace events notifications are disabled because while not\nvery sensible the feature uses some CPU power. Notifications are enabled\nusing the <code>notify-keyspace-events</code> of redis.conf or via the <strong>CONFIG SET</strong>.</p><p>Setting the parameter to the empty string disables notifications.\nIn order to enable the feature a non-empty string is used, composed of multiple\ncharacters, where every character has a special meaning according to the\nfollowing table:</p><p>At least <code>K</code> or <code>E</code> should be present in the string, otherwise no event\nwill be delivered regardless of the rest of the string.</p><p>For instance to enable just Key-space events for lists, the configuration\nparameter must be set to <code>Kl</code>, and so forth.</p><p>The string <code>KEA</code> can be used to enable every possible event.</p>","link":"/alpha/topics/notifications.html","spaLink":"#/alpha/topics/notifications","title":"CONFIGURATION"},{"content":"<h2 id=\"redis-keyspace-notifications-events-generated-by-different-commands\">Events generated by different commands</h2><p>Different commands generate different kind of events according to the following list.</p><ul>\n<li><code>DEL</code> generates a <code>del</code> event for every deleted key.</li>\n<li><code>RENAME</code> generates two events, a <code>rename_from</code> event for the source key, and a <code>rename_to</code> event for the destination key.</li>\n<li><code>EXPIRE</code> generates an <code>expire</code> event when an expire is set to the key, or a <code>expired</code> event every time setting an expire results into the key being deleted (see <code>EXPIRE</code> documentation for more info).</li>\n<li><code>SORT</code> generates a <code>sortstore</code> event when <code>STORE</code> is used to set a new key. If the resulting list is empty, and the <code>STORE</code> option is used, and there was already an existing key with that name, the result is that the key is deleted, so a <code>del</code> event is generated in this condition.</li>\n<li><code>SET</code> and all its variants (<code>SETEX</code>, <code>SETNX</code>,<code>GETSET</code>) generate <code>set</code> events. However <code>SETEX</code> will also generate an <code>expire</code> events.</li>\n<li><code>MSET</code> generates a separated <code>set</code> event for every key.</li>\n<li><code>SETRANGE</code> generates a <code>setrange</code> event.</li>\n<li><code>INCR</code>, <code>DECR</code>, <code>INCRBY</code>, <code>DECRBY</code> commands all generate <code>incrby</code> events.</li>\n<li><code>INCRBYFLOAT</code> generates an <code>incrbyfloat</code> events.</li>\n<li><code>APPEND</code> generates an <code>append</code> event.</li>\n<li><code>LPUSH</code> and <code>LPUSHX</code> generates a single <code>lpush</code> event, even in the variadic case.</li>\n<li><code>RPUSH</code> and <code>RPUSHX</code> generates a single <code>rpush</code> event, even in the variadic case.</li>\n<li><code>RPOP</code> generates an <code>rpop</code> event. Additionally a <code>del</code> event is generated if the key is removed because the last element from the list was popped.</li>\n<li><code>LPOP</code> generates an <code>lpop</code> event. Additionally a <code>del</code> event is generated if the key is removed because the last element from the list was popped.</li>\n<li><code>LINSERT</code> generates an <code>linsert</code> event.</li>\n<li><code>LSET</code> generates an <code>lset</code> event.</li>\n<li><code>LREM</code> generates an <code>lrem</code> event, and additionally a <code>del</code> event if the resulting list is empty and the key is removed.</li>\n<li><code>LTRIM</code> generates an <code>ltrim</code> event, and additionally a <code>del</code> event if the resulting list is empty and the key is removed.</li>\n<li><code>RPOPLPUSH</code> and <code>BRPOPLPUSH</code> generate an <code>rpop</code> event and an <code>lpush</code> event. In both cases the order is guaranteed (the <code>lpush</code> event will always be delivered after the <code>rpop</code> event). Additionally a <code>del</code> event will be generated if the resulting list is zero length and the key is removed.</li>\n<li><code>HSET</code>, <code>HSETNX</code> and <code>HMSET</code> all generate a single <code>hset</code> event.</li>\n<li><code>HINCRBY</code> generates an <code>hincrby</code> event.</li>\n<li><code>HINCRBYFLOAT</code> generates an <code>hincrbyfloat</code> event.</li>\n<li><code>HDEL</code> generates a single <code>hdel</code> event, and an additional <code>del</code> event if the resulting hash is empty and the key is removed.</li>\n<li><code>SADD</code> generates a single <code>sadd</code> event, even in the variadic case.</li>\n<li><code>SREM</code> generates a single <code>srem</code> event, and an additional <code>del</code> event if the resulting set is empty and the key is removed.</li>\n<li><code>SMOVE</code> generates an <code>srem</code> event for the source key, and an <code>sadd</code> event for the destination key.</li>\n<li><code>SPOP</code> generates an <code>spop</code> event, and an additional <code>del</code> event if the resulting set is empty and the key is removed.</li>\n<li><code>SINTERSTORE</code>, <code>SUNIONSTORE</code>, <code>SDIFFSTORE</code> generate <code>sinterstore</code>, <code>sunionostore</code>, <code>sdiffstore</code> events respectively. In the special case the resulting set is empty, and the key where the result is stored already exists, a <code>del</code> event is generated since the key is removed.</li>\n<li><code>ZINCR</code> generates a <code>zincr</code> event.</li>\n<li><code>ZADD</code> generates a single <code>zadd</code> event even when multiple elements are added.</li>\n<li><code>ZREM</code> generates a single <code>zrem</code> event even when multiple elements are deleted. When the resulting sorted set is empty and the key is generated, an additional <code>del</code> event is generated.</li>\n<li><code>ZREMBYSCORE</code> generates a single <code>zrembyscore</code> event. When the resulting sorted set is empty and the key is generated, an additional <code>del</code> event is generated.</li>\n<li><code>ZREMBYRANK</code> generates a single <code>zrembyrank</code> event. When the resulting sorted set is empty and the key is generated, an additional <code>del</code> event is generated.</li>\n<li><code>ZINTERSTORE</code> and <code>ZUNIONSTORE</code> respectively generate <code>zinterstore</code> and <code>zunionstore</code> events. In the special case the resulting sorted set is empty, and the key where the result is stored already exists, a <code>del</code> event is generated since the key is removed.</li>\n<li>Every time a key with a time to live associated is removed from the data set because it expired, an <code>expired</code> event is generated.</li>\n<li>Every time a key is evicted from the data set in order to free memory as a result of the <code>maxmemory</code> policy, an <code>evicted</code> event is generated.</li>\n</ul><p><strong>IMPORTANT</strong> all the commands generate events only if the target key is really modified. For instance an <code>SREM</code> deleting a non-existing element from a Set will not actually change the value of the key, so no event will be generated.</p><p>If in doubt about how events are generated for a given command, the simplest\nthing to do is to watch yourself:</p><p>At this point use <code>redis-cli</code> in another terminal to send commands to the\nRedis server and watch the events generated:</p>","link":"/alpha/topics/notifications.html","spaLink":"#/alpha/topics/notifications","title":"EVENTS GENERATED BY DIFFERENT COMMANDS"},{"content":"<h2 id=\"redis-keyspace-notifications-timing-of-expired-events\">Timing of expired events</h2><p>Keys with a time to live associated are expired by Redis in two ways:</p><ul>\n<li>When the key is accessed by a command and is found to be expired.</li>\n<li>Via a background system that looks for expired keys in background, incrementally, in order to be able to also collect keys that are never accessed.</li>\n</ul><p>The <code>expired</code> events are generated when a key is accessed and is found to be expired by one of the above systems, as a result there are no guarantees that the Redis server will be able to generate the <code>expired</code> event at the time the key time to live reaches the value of zero.</p><p>If no command targets the key constantly, and there are many keys with a TTL associated, there can be a significant delay between the time the key time to live drops to zero, and the time the <code>expired</code> event is generated.</p><p>Basically <code>expired</code> events <strong>are generated when the Redis server deletes the key</strong> and not when the time to live theoretically reaches the value of zero.</p>","link":"/alpha/topics/notifications.html","spaLink":"#/alpha/topics/notifications","title":"TIMING OF EXPIRED EVENTS"}]